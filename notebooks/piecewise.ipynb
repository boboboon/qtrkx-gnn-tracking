{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains code for interacting with hit graphs.\n",
    "A Graph is a namedtuple of matrices X, Ri, Ro, y.\n",
    "\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# A Graph is a namedtuple of matrices (X, Ri, Ro, y)\n",
    "Graph = namedtuple('Graph', ['X', 'Ri', 'Ro', 'y'])\n",
    "\n",
    "def graph_to_sparse(graph):\n",
    "    Ri_rows, Ri_cols = graph.Ri.nonzero()\n",
    "    Ro_rows, Ro_cols = graph.Ro.nonzero()\n",
    "    return dict(X=graph.X, y=graph.y,\n",
    "                Ri_rows=Ri_rows, Ri_cols=Ri_cols,\n",
    "                Ro_rows=Ro_rows, Ro_cols=Ro_cols)\n",
    "\n",
    "def sparse_to_graph(X, Ri_rows, Ri_cols, Ro_rows, Ro_cols, y, dtype=np.uint8):\n",
    "    n_nodes, n_edges = X.shape[0], Ri_rows.shape[0]\n",
    "    Ri = np.zeros((n_nodes, n_edges), dtype=dtype)\n",
    "    Ro = np.zeros((n_nodes, n_edges), dtype=dtype)\n",
    "    Ri[Ri_rows, Ri_cols] = 1\n",
    "    Ro[Ro_rows, Ro_cols] = 1\n",
    "    return Graph(X, Ri, Ro, y)\n",
    "\n",
    "def save_graph(graph, filename):\n",
    "    \"\"\"Write a single graph to an NPZ file archive\"\"\"\n",
    "    np.savez(filename, **graph_to_sparse(graph))\n",
    "\n",
    "def save_graphs(graphs, filenames):\n",
    "    for graph, filename in zip(graphs, filenames):\n",
    "        save_graph(graph, filename)\n",
    "\n",
    "def load_graph(filename):\n",
    "    \"\"\"Reade a single graph NPZ\"\"\"\n",
    "    with np.load(filename) as f:\n",
    "        return sparse_to_graph(**dict(f.items()))\n",
    "\n",
    "def load_graphs(filenames, graph_type=Graph):\n",
    "    return [load_graph(f, graph_type) for f in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preparation script for GNN tracking.\n",
    "This script processes the TrackML dataset and produces graph data on disk.\n",
    "\"\"\"\n",
    "\n",
    "# System\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "# Externals\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import trackml.dataset\n",
    "\n",
    "# Locals\n",
    "#from datasets.graph import Graph, save_graphs\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser('prepare.py')\n",
    "    add_arg = parser.add_argument\n",
    "    add_arg('config', nargs='?', default='configs/prepare_trackml.yaml')\n",
    "    add_arg('--n-workers', type=int, default=1)\n",
    "    add_arg('--task', type=int, default=0)\n",
    "    add_arg('--n-tasks', type=int, default=1)\n",
    "    add_arg('-v', '--verbose', action='store_true')\n",
    "    add_arg('--show-config', action='store_true')\n",
    "    add_arg('--interactive', action='store_true')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def calc_dphi(phi1, phi2):\n",
    "    \"\"\"Computes phi2-phi1 given in range [-pi,pi]\"\"\"\n",
    "    dphi = phi2 - phi1\n",
    "    dphi[dphi > np.pi] -= 2*np.pi\n",
    "    dphi[dphi < -np.pi] += 2*np.pi\n",
    "    return dphi\n",
    "\n",
    "def calc_eta(r, z):\n",
    "    theta = np.arctan2(r, z)\n",
    "    return -1. * np.log(np.tan(theta / 2.))\n",
    "\n",
    "def select_segments(hits1, hits2, phi_slope_max, z0_max):\n",
    "    \"\"\"\n",
    "    Construct a list of selected segments from the pairings\n",
    "    between hits1 and hits2, filtered with the specified\n",
    "    phi slope and z0 criteria.\n",
    "    Returns: pd DataFrame of (index_1, index_2), corresponding to the\n",
    "    DataFrame hit label-indices in hits1 and hits2, respectively.\n",
    "    \"\"\"\n",
    "    # Start with all possible pairs of hits\n",
    "    keys = ['evtid', 'r', 'phi', 'z']\n",
    "    hit_pairs = hits1[keys].reset_index().merge(\n",
    "        hits2[keys].reset_index(), on='evtid', suffixes=('_1', '_2'))\n",
    "    # Compute line through the points\n",
    "    dphi = calc_dphi(hit_pairs.phi_1, hit_pairs.phi_2)\n",
    "    dz = hit_pairs.z_2 - hit_pairs.z_1\n",
    "    dr = hit_pairs.r_2 - hit_pairs.r_1\n",
    "    phi_slope = dphi / dr\n",
    "    z0 = hit_pairs.z_1 - hit_pairs.r_1 * dz / dr\n",
    "    # Filter segments according to criteria\n",
    "    good_seg_mask = (phi_slope.abs() < phi_slope_max) & (z0.abs() < z0_max)\n",
    "    return hit_pairs[['index_1', 'index_2']][good_seg_mask]\n",
    "\n",
    "def construct_graph(hits, layer_pairs,\n",
    "                    phi_slope_max, z0_max,\n",
    "                    feature_names, feature_scale):\n",
    "    \"\"\"Construct one graph (e.g. from one event)\"\"\"\n",
    "\n",
    "    # Loop over layer pairs and construct segments\n",
    "    layer_groups = hits.groupby('layer')\n",
    "    segments = []\n",
    "    for (layer1, layer2) in layer_pairs:\n",
    "        # Find and join all hit pairs\n",
    "        try:\n",
    "            hits1 = layer_groups.get_group(layer1)\n",
    "            hits2 = layer_groups.get_group(layer2)\n",
    "        # If an event has no hits on a layer, we get a KeyError.\n",
    "        # In that case we just skip to the next layer pair\n",
    "        except KeyError as e:\n",
    "            logging.info('skipping empty layer: %s' % e)\n",
    "            continue\n",
    "        # Construct the segments\n",
    "        segments.append(select_segments(hits1, hits2, phi_slope_max, z0_max))\n",
    "    # Combine segments from all layer pairs\n",
    "    segments = pd.concat(segments)\n",
    "\n",
    "    # Prepare the graph matrices\n",
    "    n_hits = hits.shape[0]\n",
    "    n_edges = segments.shape[0]\n",
    "    X = (hits[feature_names].values / feature_scale).astype(np.float32)\n",
    "    Ri = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    Ro = np.zeros((n_hits, n_edges), dtype=np.uint8)\n",
    "    y = np.zeros(n_edges, dtype=np.float32)\n",
    "\n",
    "    # We have the segments' hits given by dataframe label,\n",
    "    # so we need to translate into positional indices.\n",
    "    # Use a series to map hit label-index onto positional-index.\n",
    "    hit_idx = pd.Series(np.arange(n_hits), index=hits.index)\n",
    "    seg_start = hit_idx.loc[segments.index_1].values\n",
    "    seg_end = hit_idx.loc[segments.index_2].values\n",
    "\n",
    "    # Now we can fill the association matrices.\n",
    "    # Note that Ri maps hits onto their incoming edges,\n",
    "    # which are actually segment endings.\n",
    "    Ri[seg_end, np.arange(n_edges)] = 1\n",
    "    Ro[seg_start, np.arange(n_edges)] = 1\n",
    "    # Fill the segment labels\n",
    "    pid1 = hits.particle_id.loc[segments.index_1].values\n",
    "    pid2 = hits.particle_id.loc[segments.index_2].values\n",
    "    y[:] = (pid1 == pid2)\n",
    "    # Return a tuple of the results\n",
    "    return Graph(X, Ri, Ro, y)\n",
    "\n",
    "def select_hits(hits, truth, particles, pt_min=0):\n",
    "    # Barrel volume and layer ids\n",
    "    vlids = [(8,2), (8,4), (8,6), (8,8),\n",
    "             (13,2), (13,4), (13,6), (13,8),\n",
    "             (17,2), (17,4)]\n",
    "    n_det_layers = len(vlids)\n",
    "    # Select barrel layers and assign convenient layer number [0-9]\n",
    "    vlid_groups = hits.groupby(['volume_id', 'layer_id'])\n",
    "    hits = pd.concat([vlid_groups.get_group(vlids[i]).assign(layer=i)\n",
    "                      for i in range(n_det_layers)])\n",
    "    # Calculate particle transverse momentum\n",
    "    pt = np.sqrt(particles.px**2 + particles.py**2)\n",
    "    # True particle selection.\n",
    "    # Applies pt cut, removes all noise hits.\n",
    "    particles = particles[pt > pt_min]\n",
    "    truth = (truth[['hit_id', 'particle_id']]\n",
    "             .merge(particles[['particle_id']], on='particle_id'))\n",
    "    # Calculate derived hits variables\n",
    "    r = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    phi = np.arctan2(hits.y, hits.x)\n",
    "    # Select the data columns we need\n",
    "    hits = (hits[['hit_id', 'z', 'layer']]\n",
    "            .assign(r=r, phi=phi)\n",
    "            .merge(truth[['hit_id', 'particle_id']], on='hit_id'))\n",
    "    # Remove duplicate hits\n",
    "    #hits = hits.loc[\n",
    "        #hits.groupby(['particle_id', 'layer'], as_index=False).r.idxmin()\n",
    "    #]\n",
    "    hits.drop_duplicates(subset=['layer', 'particle_id'])\n",
    "    return hits\n",
    "\n",
    "def split_detector_sections(hits, phi_edges, eta_edges):\n",
    "    \"\"\"Split hits according to provided phi and eta boundaries.\"\"\"\n",
    "    hits_sections = []\n",
    "    # Loop over sections\n",
    "    for i in range(len(phi_edges) - 1):\n",
    "        phi_min, phi_max = phi_edges[i], phi_edges[i+1]\n",
    "        # Select hits in this phi section\n",
    "        phi_hits = hits[(hits.phi > phi_min) & (hits.phi < phi_max)]\n",
    "        # Center these hits on phi=0\n",
    "        centered_phi = phi_hits.phi - (phi_min + phi_max) / 2\n",
    "        phi_hits = phi_hits.assign(phi=centered_phi, phi_section=i)\n",
    "        for j in range(len(eta_edges) - 1):\n",
    "            eta_min, eta_max = eta_edges[j], eta_edges[j+1]\n",
    "            # Select hits in this eta section\n",
    "            eta = calc_eta(phi_hits.r, phi_hits.z)\n",
    "            sec_hits = phi_hits[(eta > eta_min) & (eta < eta_max)]\n",
    "            hits_sections.append(sec_hits.assign(eta_section=j))\n",
    "    return hits_sections\n",
    "\n",
    "def process_event(prefix, output_dir, pt_min, n_eta_sections, n_phi_sections,\n",
    "                  eta_range, phi_range, phi_slope_max, z0_max):\n",
    "    # Load the data\n",
    "    evtid = int(prefix[-9:])\n",
    "    logging.info('Event %i, loading data' % evtid)\n",
    "    hits, particles, truth = trackml.dataset.load_event(\n",
    "        prefix, parts=['hits', 'particles', 'truth'])\n",
    "\n",
    "    # Apply hit selection\n",
    "    logging.info('Event %i, selecting hits' % evtid)\n",
    "    hits = select_hits(hits, truth, particles, pt_min=pt_min).assign(evtid=evtid)\n",
    "\n",
    "    # Divide detector into sections\n",
    "    #phi_range = (-np.pi, np.pi)\n",
    "    phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)\n",
    "    eta_edges = np.linspace(*eta_range, num=n_eta_sections+1)\n",
    "    hits_sections = split_detector_sections(hits, phi_edges, eta_edges)\n",
    "\n",
    "    # Graph features and scale\n",
    "    feature_names = ['r', 'phi', 'z']\n",
    "    feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])\n",
    "\n",
    "    # Define adjacent layers\n",
    "    n_det_layers = 10\n",
    "    l = np.arange(n_det_layers)\n",
    "    layer_pairs = np.stack([l[:-1], l[1:]], axis=1)\n",
    "\n",
    "    # Construct the graph\n",
    "    logging.info('Event %i, constructing graphs' % evtid)\n",
    "    graphs = [construct_graph(section_hits, layer_pairs=layer_pairs,\n",
    "                              phi_slope_max=phi_slope_max, z0_max=z0_max,\n",
    "                              feature_names=feature_names,\n",
    "                              feature_scale=feature_scale)\n",
    "              for section_hits in hits_sections]\n",
    "\n",
    "    # Write these graphs to the output directory\n",
    "    try:\n",
    "        base_prefix = os.path.basename(prefix)\n",
    "        filenames = [os.path.join(output_dir, '%s_g%03i' % (base_prefix, i))\n",
    "                     for i in range(len(graphs))]\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "    logging.info('Event %i, writing graphs', evtid)\n",
    "    save_graphs(graphs, filenames)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "\n",
    "    # Parse the command line\n",
    "    args = parse_args()\n",
    "\n",
    "    # Setup logging\n",
    "    log_format = '%(asctime)s %(levelname)s %(message)s'\n",
    "    log_level = logging.DEBUG if args.verbose else logging.INFO\n",
    "    logging.basicConfig(level=log_level, format=log_format)\n",
    "    logging.info('Initializing')\n",
    "    if args.show_config:\n",
    "        logging.info('Command line config: %s' % args)\n",
    "\n",
    "    # Load configuration\n",
    "    with open(args.config) as f:\n",
    "        config = yaml.full_load(f)\n",
    "    if args.task == 0:\n",
    "        logging.info('Configuration: %s' % config)\n",
    "\n",
    "    # Construct layer pairs from adjacent layer numbers\n",
    "    layers = np.arange(10)\n",
    "    layer_pairs = np.stack([layers[:-1], layers[1:]], axis=1)\n",
    "\n",
    "    # Find the input files\n",
    "    input_dir = config['input_dir']\n",
    "    all_files = os.listdir(input_dir)\n",
    "    suffix = '-hits.csv'\n",
    "    file_prefixes = sorted(os.path.join(input_dir, f.replace(suffix, ''))\n",
    "                           for f in all_files if f.endswith(suffix))\n",
    "    file_prefixes = file_prefixes[:config['n_files']]\n",
    "\n",
    "    # Split the input files by number of tasks and select my chunk only\n",
    "    file_prefixes = np.array_split(file_prefixes, args.n_tasks)[args.task]\n",
    "\n",
    "    # Prepare output\n",
    "    output_dir = os.path.expandvars(config['output_dir'])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logging.info('Writing outputs to ' + output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pt_min= 1. # GeV\n",
    "    phi_slope_max= 0.0006\n",
    "    z0_max= 100\n",
    "    n_phi_sections= 8\n",
    "    n_eta_sections= 2\n",
    "    eta_range= [-5, 5]\n",
    "    phi_range=(-np.pi, np.pi)\n",
    "\n",
    "    # Process input files with a worker pool\n",
    "    print('HERE:',file_prefixes)\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        process_event(file_prefixes[i], output_dir, pt_min, n_eta_sections, n_phi_sections, eta_range, phi_range, phi_slope_max, z0_max)\n",
    "\n",
    "    # Drop to IPython interactive shell\n",
    "    if args.interactive:\n",
    "        logging.info('Starting IPython interactive session')\n",
    "        import IPython\n",
    "        IPython.embed()\n",
    "\n",
    "    logging.info('All done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=r'/Users/lucascurtin/Desktop/event_1000'\n",
    "output_dir=r'/Users/lucascurtin/Desktop/QGNN Repos/qtrkx-gnn-tracking/data_personal/cut_1event'\n",
    "n_files=1\n",
    "\n",
    "\n",
    "\n",
    "# Construct layer pairs from adjacent layer numbers\n",
    "layers = np.arange(10)\n",
    "layer_pairs = np.stack([layers[:-1], layers[1:]], axis=1)\n",
    "\n",
    "# Find the input files\n",
    "input_dir = input_dir\n",
    "all_files = os.listdir(input_dir)\n",
    "suffix = '-hits.csv'\n",
    "file_prefixes = sorted(os.path.join(input_dir, f.replace(suffix, ''))\n",
    "                        for f in all_files if f.endswith(suffix))\n",
    "file_prefixes = file_prefixes[:n_files]\n",
    "\n",
    "# Split the input files by number of tasks and select my chunk only\n",
    "#file_prefixes = np.array_split(file_prefixes, args.n_tasks)[args.task]\n",
    "\n",
    "# Prepare output\n",
    "output_dir = os.path.expandvars(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logging.info('Writing outputs to ' + output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pt_min= 1. # GeV\n",
    "phi_slope_max= 0.0006\n",
    "z0_max= 100\n",
    "n_phi_sections= 1\n",
    "n_eta_sections= 1\n",
    "eta_range= [-5, 5]\n",
    "phi_range=(-np.pi, np.pi)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "process_event(file_prefixes[0], output_dir, pt_min, n_eta_sections, n_phi_sections, eta_range, phi_range, phi_slope_max, z0_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lucascurtin/Desktop/event_1000/event000001000']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lucascurtin/Desktop/event_1000/event000001000']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtrkx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1f025ae0fd6678ab5af02de2366f5b3f871a900d87aa48ee8f6eb90e420573"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
