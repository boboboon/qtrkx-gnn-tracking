Printing configs: 
train_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_graphs
valid_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/valid_graphs
dataset: TuysuzPaper
log_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 2/
run_type: new_run
gpu: -1
n_files: 100
n_valid: 20
n_train: 80
batch_size: 1
lr_c: 0.01
n_iters: 3
n_epoch: 30
TEST_every: 50
hid_dim: 4
network: QGNN
optimizer: Adam
loss_func: BinaryCrossentropy
n_thread: 4
log_verbosity: 2
EN_qc: {'PQC_id': '10', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}
NN_qc: {'PQC_id': '10', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}
Log dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 2/
Training data input dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_graphs
Validation data input dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_graphs
2023-03-07 18:49:00.851927 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 2/log_validation.csv
2023-03-07 18:49:00.852359 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 2/log_training.csv
2023-03-07 18:49:00.852599 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 2/summary.csv
Model: "GNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
InputNet (Dense)             multiple                  16        
_________________________________________________________________
EdgeNet (EdgeNet)            multiple                  81        
_________________________________________________________________
NodeNet (NodeNet)            multiple                  124       
=================================================================
Total params: 221
Trainable params: 221
Non-trainable params: 0
_________________________________________________________________
None
2023-03-07 18:49:38.268264 Starting testing the valid set with 20 subgraphs!
2023-03-07 18:55:12.039143: validation Test:  Loss: 0.7092,  AUC: 0.4639, Acc: 46.9881,  Precision: 0.4897 -- Elapsed: 5m33s
2023-03-07 18:55:12.051228 Starting testing the train set with 20 subgraphs!
2023-03-07 19:17:08.199940: training Test:  Loss: 0.7094,  AUC: 0.4624, Acc: 46.7954,  Precision: 0.4878 -- Elapsed: 21m56s
2023-03-07 19:17:08.204622: Training is starting!
2023-03-07 19:17:31.347817: Epoch: 1, Batch: 1, Loss: 0.6997, Elapsed: 0m23s
2023-03-07 19:17:59.110408: Epoch: 1, Batch: 2, Loss: 0.6990, Elapsed: 0m27s
2023-03-07 19:18:37.098516: Epoch: 1, Batch: 3, Loss: 0.7007, Elapsed: 0m37s
2023-03-07 19:19:11.486402: Epoch: 1, Batch: 4, Loss: 0.6844, Elapsed: 0m34s
2023-03-07 19:19:47.512024: Epoch: 1, Batch: 5, Loss: 0.6747, Elapsed: 0m36s
2023-03-07 19:20:20.961621: Epoch: 1, Batch: 6, Loss: 0.6762, Elapsed: 0m33s
2023-03-07 19:20:45.289216: Epoch: 1, Batch: 7, Loss: 0.6598, Elapsed: 0m24s
2023-03-07 19:21:29.712008: Epoch: 1, Batch: 8, Loss: 0.6730, Elapsed: 0m44s
2023-03-07 19:21:53.527964: Epoch: 1, Batch: 9, Loss: 0.6631, Elapsed: 0m23s
2023-03-07 19:22:21.396004: Epoch: 1, Batch: 10, Loss: 0.6597, Elapsed: 0m27s
2023-03-07 19:22:51.446255: Epoch: 1, Batch: 11, Loss: 0.6577, Elapsed: 0m30s
2023-03-07 19:23:37.252559: Epoch: 1, Batch: 12, Loss: 0.6666, Elapsed: 0m45s
2023-03-07 19:24:07.942829: Epoch: 1, Batch: 13, Loss: 0.6530, Elapsed: 0m30s
2023-03-07 19:24:39.946747: Epoch: 1, Batch: 14, Loss: 0.6580, Elapsed: 0m31s
2023-03-07 19:25:10.640381: Epoch: 1, Batch: 15, Loss: 0.6523, Elapsed: 0m30s
2023-03-07 19:25:53.209879: Epoch: 1, Batch: 16, Loss: 0.6571, Elapsed: 0m42s
2023-03-07 19:26:20.027938: Epoch: 1, Batch: 17, Loss: 0.6523, Elapsed: 0m26s
2023-03-07 19:26:49.866497: Epoch: 1, Batch: 18, Loss: 0.6497, Elapsed: 0m29s
2023-03-07 19:27:25.462627: Epoch: 1, Batch: 19, Loss: 0.6431, Elapsed: 0m35s
2023-03-07 19:28:20.379444: Epoch: 1, Batch: 20, Loss: 0.6687, Elapsed: 0m54s
2023-03-07 19:28:50.919021: Epoch: 1, Batch: 21, Loss: 0.6378, Elapsed: 0m30s
2023-03-07 19:29:25.633571: Epoch: 1, Batch: 22, Loss: 0.6368, Elapsed: 0m34s
2023-03-07 19:29:56.301325: Epoch: 1, Batch: 23, Loss: 0.6466, Elapsed: 0m30s
2023-03-07 19:30:49.568181: Epoch: 1, Batch: 24, Loss: 0.6302, Elapsed: 0m53s
2023-03-07 19:31:16.379916: Epoch: 1, Batch: 25, Loss: 0.6358, Elapsed: 0m26s
2023-03-07 19:32:17.845607: Epoch: 1, Batch: 26, Loss: 0.6321, Elapsed: 1m1s
2023-03-07 19:32:44.930842: Epoch: 1, Batch: 27, Loss: 0.6329, Elapsed: 0m27s
2023-03-07 19:33:19.595624: Epoch: 1, Batch: 28, Loss: 0.6394, Elapsed: 0m34s
2023-03-07 19:34:06.689064: Epoch: 1, Batch: 29, Loss: 0.6249, Elapsed: 0m47s
2023-03-07 19:34:41.225491: Epoch: 1, Batch: 30, Loss: 0.6513, Elapsed: 0m34s
2023-03-07 19:35:17.048423: Epoch: 1, Batch: 31, Loss: 0.6318, Elapsed: 0m35s
2023-03-07 19:35:45.729617: Epoch: 1, Batch: 32, Loss: 0.6359, Elapsed: 0m28s
2023-03-07 19:36:18.661136: Epoch: 1, Batch: 33, Loss: 0.6257, Elapsed: 0m32s
2023-03-07 19:36:57.281000: Epoch: 1, Batch: 34, Loss: 0.6069, Elapsed: 0m38s
2023-03-07 19:37:42.031313: Epoch: 1, Batch: 35, Loss: 0.6123, Elapsed: 0m44s
2023-03-07 19:38:23.168970: Epoch: 1, Batch: 36, Loss: 0.5947, Elapsed: 0m41s
2023-03-07 19:38:57.662728: Epoch: 1, Batch: 37, Loss: 0.6148, Elapsed: 0m34s
2023-03-07 19:39:31.108790: Epoch: 1, Batch: 38, Loss: 0.5839, Elapsed: 0m33s
2023-03-07 19:40:25.139860: Epoch: 1, Batch: 39, Loss: 0.6138, Elapsed: 0m54s
2023-03-07 19:41:02.930065: Epoch: 1, Batch: 40, Loss: 0.5882, Elapsed: 0m37s
2023-03-07 19:41:53.370667: Epoch: 1, Batch: 41, Loss: 0.5880, Elapsed: 0m50s
2023-03-07 19:42:28.686790: Epoch: 1, Batch: 42, Loss: 0.6228, Elapsed: 0m35s
2023-03-07 19:43:25.814202: Epoch: 1, Batch: 43, Loss: 0.5837, Elapsed: 0m57s
2023-03-07 19:44:10.770981: Epoch: 1, Batch: 44, Loss: 0.5895, Elapsed: 0m44s
2023-03-07 19:44:54.052168: Epoch: 1, Batch: 45, Loss: 0.5725, Elapsed: 0m43s
2023-03-07 19:45:26.858246: Epoch: 1, Batch: 46, Loss: 0.5573, Elapsed: 0m32s
2023-03-07 19:46:14.272736: Epoch: 1, Batch: 47, Loss: 0.5642, Elapsed: 0m47s
2023-03-07 19:46:52.904141: Epoch: 1, Batch: 48, Loss: 0.5547, Elapsed: 0m38s
2023-03-07 19:47:29.133719: Epoch: 1, Batch: 49, Loss: 0.5600, Elapsed: 0m36s
2023-03-07 19:48:16.545971: Epoch: 1, Batch: 50, Loss: 0.5993, Elapsed: 0m47s
2023-03-07 19:48:16.557681 Starting testing the valid set with 20 subgraphs!
2023-03-07 19:53:53.249990: validation Test:  Loss: 0.5566,  AUC: 0.7957, Acc: 72.1742,  Precision: 0.7483 -- Elapsed: 5m36s
2023-03-07 19:53:53.255702 Starting testing the train set with 20 subgraphs!
2023-03-07 20:15:44.918895: training Test:  Loss: 0.5554,  AUC: 0.7954, Acc: 72.2183,  Precision: 0.7550 -- Elapsed: 21m51s
2023-03-07 20:16:11.516400: Epoch: 1, Batch: 51, Loss: 0.5369, Elapsed: 0m26s
2023-03-07 20:16:45.619218: Epoch: 1, Batch: 52, Loss: 0.5464, Elapsed: 0m34s
2023-03-07 20:17:14.226096: Epoch: 1, Batch: 53, Loss: 0.5558, Elapsed: 0m28s
2023-03-07 20:17:41.904604: Epoch: 1, Batch: 54, Loss: 0.5376, Elapsed: 0m27s
2023-03-07 20:18:14.826961: Epoch: 1, Batch: 55, Loss: 0.5502, Elapsed: 0m32s
2023-03-07 20:18:57.838681: Epoch: 1, Batch: 56, Loss: 0.5485, Elapsed: 0m42s
2023-03-07 20:19:28.277630: Epoch: 1, Batch: 57, Loss: 0.5720, Elapsed: 0m30s
2023-03-07 20:19:50.597692: Epoch: 1, Batch: 58, Loss: 0.5128, Elapsed: 0m22s
2023-03-07 20:20:12.720576: Epoch: 1, Batch: 59, Loss: 0.5112, Elapsed: 0m22s
2023-03-07 20:20:32.497783: Epoch: 1, Batch: 60, Loss: 0.4900, Elapsed: 0m19s
2023-03-07 20:21:01.162676: Epoch: 1, Batch: 61, Loss: 0.5446, Elapsed: 0m28s
2023-03-07 20:21:32.070629: Epoch: 1, Batch: 62, Loss: 0.5411, Elapsed: 0m30s
2023-03-07 20:22:14.987759: Epoch: 1, Batch: 63, Loss: 0.5380, Elapsed: 0m42s
2023-03-07 20:22:45.718571: Epoch: 1, Batch: 64, Loss: 0.5358, Elapsed: 0m30s
2023-03-07 20:23:11.669197: Epoch: 1, Batch: 65, Loss: 0.5367, Elapsed: 0m25s
2023-03-07 20:23:52.398313: Epoch: 1, Batch: 66, Loss: 0.5353, Elapsed: 0m40s
2023-03-07 20:24:32.491824: Epoch: 1, Batch: 67, Loss: 0.5475, Elapsed: 0m40s
2023-03-07 20:24:49.625563: Epoch: 1, Batch: 68, Loss: 0.4700, Elapsed: 0m17s
2023-03-07 20:25:12.466003: Epoch: 1, Batch: 69, Loss: 0.5136, Elapsed: 0m22s
2023-03-07 20:25:47.577697: Epoch: 1, Batch: 70, Loss: 0.5139, Elapsed: 0m35s
2023-03-07 20:26:25.297932: Epoch: 1, Batch: 71, Loss: 0.5155, Elapsed: 0m37s
2023-03-07 20:27:07.582123: Epoch: 1, Batch: 72, Loss: 0.5235, Elapsed: 0m42s
2023-03-07 20:27:36.486084: Epoch: 1, Batch: 73, Loss: 0.5081, Elapsed: 0m28s
2023-03-07 20:28:11.197579: Epoch: 1, Batch: 74, Loss: 0.5168, Elapsed: 0m34s
2023-03-07 20:28:40.894259: Epoch: 1, Batch: 75, Loss: 0.5294, Elapsed: 0m29s
2023-03-07 20:29:15.652467: Epoch: 1, Batch: 76, Loss: 0.5050, Elapsed: 0m34s
2023-03-07 20:30:03.700102: Epoch: 1, Batch: 77, Loss: 0.5234, Elapsed: 0m48s
2023-03-07 20:30:28.448238: Epoch: 1, Batch: 78, Loss: 0.5048, Elapsed: 0m24s
2023-03-07 20:30:58.866753: Epoch: 1, Batch: 79, Loss: 0.5107, Elapsed: 0m30s
2023-03-07 20:31:41.042275: Epoch: 1, Batch: 80, Loss: 0.5226, Elapsed: 0m42s
2023-03-07 20:32:00.455767: Epoch: 2, Batch: 1, Loss: 0.4944, Elapsed: 0m19s
2023-03-07 20:32:27.196993: Epoch: 2, Batch: 2, Loss: 0.5098, Elapsed: 0m26s
2023-03-07 20:32:59.466213: Epoch: 2, Batch: 3, Loss: 0.4914, Elapsed: 0m32s
2023-03-07 20:33:52.383255: Epoch: 2, Batch: 4, Loss: 0.5173, Elapsed: 0m52s
2023-03-07 20:34:22.172509: Epoch: 2, Batch: 5, Loss: 0.5052, Elapsed: 0m29s
2023-03-07 20:34:56.427161: Epoch: 2, Batch: 6, Loss: 0.5121, Elapsed: 0m34s
2023-03-07 20:35:30.877184: Epoch: 2, Batch: 7, Loss: 0.5048, Elapsed: 0m34s
2023-03-07 20:36:04.021602: Epoch: 2, Batch: 8, Loss: 0.4989, Elapsed: 0m33s
2023-03-07 20:36:39.678264: Epoch: 2, Batch: 9, Loss: 0.4950, Elapsed: 0m35s
2023-03-07 20:37:11.922174: Epoch: 2, Batch: 10, Loss: 0.4956, Elapsed: 0m32s
2023-03-07 20:37:37.196416: Epoch: 2, Batch: 11, Loss: 0.4804, Elapsed: 0m25s
2023-03-07 20:38:06.022861: Epoch: 2, Batch: 12, Loss: 0.4952, Elapsed: 0m28s
2023-03-07 20:38:51.954808: Epoch: 2, Batch: 13, Loss: 0.5082, Elapsed: 0m45s
2023-03-07 20:39:29.686237: Epoch: 2, Batch: 14, Loss: 0.4958, Elapsed: 0m37s
2023-03-07 20:39:56.167240: Epoch: 2, Batch: 15, Loss: 0.4867, Elapsed: 0m26s
2023-03-07 20:40:43.193007: Epoch: 2, Batch: 16, Loss: 0.5117, Elapsed: 0m47s
2023-03-07 20:41:12.577242: Epoch: 2, Batch: 17, Loss: 0.4754, Elapsed: 0m29s
2023-03-07 20:41:36.402936: Epoch: 2, Batch: 18, Loss: 0.4754, Elapsed: 0m23s
2023-03-07 20:41:59.508997: Epoch: 2, Batch: 19, Loss: 0.4752, Elapsed: 0m23s
2023-03-07 20:42:32.475144: Epoch: 2, Batch: 20, Loss: 0.4885, Elapsed: 0m32s
2023-03-07 20:43:27.648853: Epoch: 2, Batch: 21, Loss: 0.5088, Elapsed: 0m55s
2023-03-07 20:43:58.162119: Epoch: 2, Batch: 22, Loss: 0.4844, Elapsed: 0m30s
2023-03-07 20:44:32.146081: Epoch: 2, Batch: 23, Loss: 0.4803, Elapsed: 0m33s
2023-03-07 20:45:14.187875: Epoch: 2, Batch: 24, Loss: 0.4988, Elapsed: 0m42s
2023-03-07 20:45:49.352051: Epoch: 2, Batch: 25, Loss: 0.4840, Elapsed: 0m35s
2023-03-07 20:46:47.893307: Epoch: 2, Batch: 26, Loss: 0.5065, Elapsed: 0m58s
2023-03-07 20:47:35.859344: Epoch: 2, Batch: 27, Loss: 0.4994, Elapsed: 0m47s
2023-03-07 20:48:06.258971: Epoch: 2, Batch: 28, Loss: 0.4823, Elapsed: 0m30s
2023-03-07 20:48:35.958821: Epoch: 2, Batch: 29, Loss: 0.4717, Elapsed: 0m29s
2023-03-07 20:49:11.706277: Epoch: 2, Batch: 30, Loss: 0.4911, Elapsed: 0m35s
2023-03-07 20:49:44.632010: Epoch: 2, Batch: 31, Loss: 0.4926, Elapsed: 0m32s
2023-03-07 20:50:23.236329: Epoch: 2, Batch: 32, Loss: 0.4951, Elapsed: 0m38s
2023-03-07 20:51:07.541494: Epoch: 2, Batch: 33, Loss: 0.4949, Elapsed: 0m44s
2023-03-07 20:51:24.869678: Epoch: 2, Batch: 34, Loss: 0.4502, Elapsed: 0m17s
2023-03-07 20:51:50.356814: Epoch: 2, Batch: 35, Loss: 0.4739, Elapsed: 0m25s
2023-03-07 20:52:30.935843: Epoch: 2, Batch: 36, Loss: 0.4894, Elapsed: 0m40s
2023-03-07 20:53:27.189866: Epoch: 2, Batch: 37, Loss: 0.5622, Elapsed: 0m56s
2023-03-07 20:54:28.433322: Epoch: 2, Batch: 38, Loss: 0.5015, Elapsed: 1m1s
2023-03-07 20:55:13.181552: Epoch: 2, Batch: 39, Loss: 0.5082, Elapsed: 0m44s
2023-03-07 20:55:43.661635: Epoch: 2, Batch: 40, Loss: 0.4743, Elapsed: 0m30s
2023-03-07 20:56:18.668418: Epoch: 2, Batch: 41, Loss: 0.4884, Elapsed: 0m34s
2023-03-07 20:56:46.405293: Epoch: 2, Batch: 42, Loss: 0.4742, Elapsed: 0m27s
2023-03-07 20:57:16.864631: Epoch: 2, Batch: 43, Loss: 0.4885, Elapsed: 0m30s
2023-03-07 20:57:42.608989: Epoch: 2, Batch: 44, Loss: 0.4692, Elapsed: 0m25s
2023-03-07 20:58:08.236955: Epoch: 2, Batch: 45, Loss: 0.4683, Elapsed: 0m25s
2023-03-07 20:58:39.529732: Epoch: 2, Batch: 46, Loss: 0.4869, Elapsed: 0m31s
2023-03-07 20:59:14.147013: Epoch: 2, Batch: 47, Loss: 0.4819, Elapsed: 0m34s
2023-03-07 20:59:47.987793: Epoch: 2, Batch: 48, Loss: 0.4922, Elapsed: 0m33s
2023-03-07 21:00:25.875699: Epoch: 2, Batch: 49, Loss: 0.4854, Elapsed: 0m37s
2023-03-07 21:01:08.503609: Epoch: 2, Batch: 50, Loss: 0.4931, Elapsed: 0m42s
2023-03-07 21:01:08.516988 Starting testing the valid set with 20 subgraphs!
2023-03-07 21:06:34.475988: validation Test:  Loss: 0.4875,  AUC: 0.8270, Acc: 74.6101,  Precision: 0.7663 -- Elapsed: 5m25s
2023-03-07 21:06:34.481495 Starting testing the train set with 20 subgraphs!
2023-03-07 21:28:12.567914: training Test:  Loss: 0.4842,  AUC: 0.8279, Acc: 74.8492,  Precision: 0.7727 -- Elapsed: 21m38s
2023-03-07 21:28:39.550137: Epoch: 2, Batch: 51, Loss: 0.4633, Elapsed: 0m26s
2023-03-07 21:29:24.889608: Epoch: 2, Batch: 52, Loss: 0.5000, Elapsed: 0m45s
2023-03-07 21:30:11.457191: Epoch: 2, Batch: 53, Loss: 0.4938, Elapsed: 0m46s
2023-03-07 21:30:59.139408: Epoch: 2, Batch: 54, Loss: 0.4983, Elapsed: 0m47s
2023-03-07 21:31:30.810702: Epoch: 2, Batch: 55, Loss: 0.4824, Elapsed: 0m31s
2023-03-07 21:31:55.196721: Epoch: 2, Batch: 56, Loss: 0.4714, Elapsed: 0m24s
2023-03-07 21:32:30.390001: Epoch: 2, Batch: 57, Loss: 0.4854, Elapsed: 0m35s
2023-03-07 21:33:00.401077: Epoch: 2, Batch: 58, Loss: 0.4753, Elapsed: 0m29s
2023-03-07 21:33:41.127554: Epoch: 2, Batch: 59, Loss: 0.4842, Elapsed: 0m40s
2023-03-07 21:34:11.376224: Epoch: 2, Batch: 60, Loss: 0.4625, Elapsed: 0m30s
2023-03-07 21:35:01.793046: Epoch: 2, Batch: 61, Loss: 0.4860, Elapsed: 0m50s
2023-03-07 21:35:39.269210: Epoch: 2, Batch: 62, Loss: 0.4867, Elapsed: 0m37s
2023-03-07 21:36:06.984262: Epoch: 2, Batch: 63, Loss: 0.4600, Elapsed: 0m27s
2023-03-07 21:36:39.502999: Epoch: 2, Batch: 64, Loss: 0.4683, Elapsed: 0m32s
2023-03-07 21:37:02.274661: Epoch: 2, Batch: 65, Loss: 0.4394, Elapsed: 0m22s
2023-03-07 21:37:40.412862: Epoch: 2, Batch: 66, Loss: 0.4743, Elapsed: 0m38s
2023-03-07 21:38:14.768397: Epoch: 2, Batch: 67, Loss: 0.4784, Elapsed: 0m34s
2023-03-07 21:38:57.721725: Epoch: 2, Batch: 68, Loss: 0.4903, Elapsed: 0m42s
2023-03-07 21:39:43.399663: Epoch: 2, Batch: 69, Loss: 0.4789, Elapsed: 0m45s
2023-03-07 21:40:23.282586: Epoch: 2, Batch: 70, Loss: 0.4722, Elapsed: 0m39s
2023-03-07 21:40:53.324402: Epoch: 2, Batch: 71, Loss: 0.4569, Elapsed: 0m30s
2023-03-07 21:41:35.267306: Epoch: 2, Batch: 72, Loss: 0.4805, Elapsed: 0m41s
2023-03-07 21:42:05.821210: Epoch: 2, Batch: 73, Loss: 0.4601, Elapsed: 0m30s
2023-03-07 21:42:49.776936: Epoch: 2, Batch: 74, Loss: 0.4784, Elapsed: 0m43s
2023-03-07 21:43:24.274414: Epoch: 2, Batch: 75, Loss: 0.4564, Elapsed: 0m34s
2023-03-07 21:43:45.810534: Epoch: 2, Batch: 76, Loss: 0.4352, Elapsed: 0m21s
2023-03-07 21:44:08.660762: Epoch: 2, Batch: 77, Loss: 0.4366, Elapsed: 0m22s
2023-03-07 21:44:42.779921: Epoch: 2, Batch: 78, Loss: 0.4536, Elapsed: 0m34s
2023-03-07 21:45:10.217934: Epoch: 2, Batch: 79, Loss: 0.4481, Elapsed: 0m27s
2023-03-07 21:45:38.341185: Epoch: 2, Batch: 80, Loss: 0.4426, Elapsed: 0m28s
2023-03-07 21:46:26.079740: Epoch: 3, Batch: 1, Loss: 0.4821, Elapsed: 0m47s
2023-03-07 21:46:55.272871: Epoch: 3, Batch: 2, Loss: 0.4342, Elapsed: 0m29s
2023-03-07 21:47:29.444377: Epoch: 3, Batch: 3, Loss: 0.4570, Elapsed: 0m34s
2023-03-07 21:48:23.576483: Epoch: 3, Batch: 4, Loss: 0.4832, Elapsed: 0m54s
2023-03-07 21:48:55.414007: Epoch: 3, Batch: 5, Loss: 0.4630, Elapsed: 0m31s
2023-03-07 21:49:37.167496: Epoch: 3, Batch: 6, Loss: 0.4764, Elapsed: 0m41s
2023-03-07 21:49:59.713696: Epoch: 3, Batch: 7, Loss: 0.4271, Elapsed: 0m22s
2023-03-07 21:50:25.423308: Epoch: 3, Batch: 8, Loss: 0.4512, Elapsed: 0m25s
2023-03-07 21:50:55.236851: Epoch: 3, Batch: 9, Loss: 0.4568, Elapsed: 0m29s
2023-03-07 21:51:25.047002: Epoch: 3, Batch: 10, Loss: 0.4576, Elapsed: 0m29s
2023-03-07 21:51:50.851077: Epoch: 3, Batch: 11, Loss: 0.4465, Elapsed: 0m25s
2023-03-07 21:52:36.075549: Epoch: 3, Batch: 12, Loss: 0.4732, Elapsed: 0m45s
2023-03-07 21:53:23.341425: Epoch: 3, Batch: 13, Loss: 0.4745, Elapsed: 0m47s
2023-03-07 21:53:53.562229: Epoch: 3, Batch: 14, Loss: 0.4444, Elapsed: 0m30s
2023-03-07 21:54:22.893664: Epoch: 3, Batch: 15, Loss: 0.4498, Elapsed: 0m29s
2023-03-07 21:55:02.920904: Epoch: 3, Batch: 16, Loss: 0.4695, Elapsed: 0m40s
2023-03-07 21:55:35.400133: Epoch: 3, Batch: 17, Loss: 0.4597, Elapsed: 0m32s
2023-03-07 21:56:15.347951: Epoch: 3, Batch: 18, Loss: 0.4742, Elapsed: 0m39s
2023-03-07 21:56:49.144061: Epoch: 3, Batch: 19, Loss: 0.4638, Elapsed: 0m33s
2023-03-07 21:57:30.873615: Epoch: 3, Batch: 20, Loss: 0.4716, Elapsed: 0m41s
2023-03-07 21:57:50.356070: Epoch: 3, Batch: 21, Loss: 0.4061, Elapsed: 0m19s
2023-03-07 21:58:24.662528: Epoch: 3, Batch: 22, Loss: 0.4460, Elapsed: 0m34s
2023-03-07 21:59:02.372265: Epoch: 3, Batch: 23, Loss: 0.4680, Elapsed: 0m37s
2023-03-07 21:59:33.599730: Epoch: 3, Batch: 24, Loss: 0.4620, Elapsed: 0m31s
2023-03-07 22:00:23.990013: Epoch: 3, Batch: 25, Loss: 0.4795, Elapsed: 0m50s
2023-03-07 22:01:08.277477: Epoch: 3, Batch: 26, Loss: 0.4818, Elapsed: 0m44s
2023-03-07 22:01:50.686043: Epoch: 3, Batch: 27, Loss: 0.4800, Elapsed: 0m42s
2023-03-07 22:02:15.173413: Epoch: 3, Batch: 28, Loss: 0.4569, Elapsed: 0m24s
2023-03-07 22:02:48.250136: Epoch: 3, Batch: 29, Loss: 0.4619, Elapsed: 0m33s
2023-03-07 22:03:16.017714: Epoch: 3, Batch: 30, Loss: 0.4542, Elapsed: 0m27s
2023-03-07 22:03:46.223579: Epoch: 3, Batch: 31, Loss: 0.4577, Elapsed: 0m30s
2023-03-07 22:04:23.742340: Epoch: 3, Batch: 32, Loss: 0.4667, Elapsed: 0m37s
2023-03-07 22:05:10.261444: Epoch: 3, Batch: 33, Loss: 0.4733, Elapsed: 0m46s
2023-03-07 22:05:42.406330: Epoch: 3, Batch: 34, Loss: 0.4553, Elapsed: 0m32s
2023-03-07 22:06:29.887941: Epoch: 3, Batch: 35, Loss: 0.4754, Elapsed: 0m47s
2023-03-07 22:07:00.439282: Epoch: 3, Batch: 36, Loss: 0.4472, Elapsed: 0m30s
2023-03-07 22:07:52.714623: Epoch: 3, Batch: 37, Loss: 0.4777, Elapsed: 0m52s
2023-03-07 22:08:20.985806: Epoch: 3, Batch: 38, Loss: 0.4442, Elapsed: 0m28s
2023-03-07 22:09:18.661649: Epoch: 3, Batch: 39, Loss: 0.4799, Elapsed: 0m57s
2023-03-07 22:09:51.680002: Epoch: 3, Batch: 40, Loss: 0.4596, Elapsed: 0m33s
2023-03-07 22:10:27.650458: Epoch: 3, Batch: 41, Loss: 0.4595, Elapsed: 0m35s
2023-03-07 22:10:59.788833: Epoch: 3, Batch: 42, Loss: 0.4440, Elapsed: 0m32s
2023-03-07 22:11:35.079815: Epoch: 3, Batch: 43, Loss: 0.4632, Elapsed: 0m35s
2023-03-07 22:11:59.361591: Epoch: 3, Batch: 44, Loss: 0.4297, Elapsed: 0m24s
2023-03-07 22:12:42.384722: Epoch: 3, Batch: 45, Loss: 0.4692, Elapsed: 0m43s
2023-03-07 22:13:08.337967: Epoch: 3, Batch: 46, Loss: 0.4521, Elapsed: 0m25s
2023-03-07 22:13:35.597284: Epoch: 3, Batch: 47, Loss: 0.4359, Elapsed: 0m27s
2023-03-07 22:14:09.569779: Epoch: 3, Batch: 48, Loss: 0.4617, Elapsed: 0m33s
2023-03-07 22:14:44.463462: Epoch: 3, Batch: 49, Loss: 0.4507, Elapsed: 0m34s
2023-03-07 22:15:29.657028: Epoch: 3, Batch: 50, Loss: 0.4661, Elapsed: 0m45s
2023-03-07 22:15:29.669321 Starting testing the valid set with 20 subgraphs!
2023-03-07 22:20:55.062708: validation Test:  Loss: 0.4560,  AUC: 0.8391, Acc: 77.0227,  Precision: 0.8300 -- Elapsed: 5m25s
2023-03-07 22:20:55.068170 Starting testing the train set with 20 subgraphs!
2023-03-07 22:42:30.278528: training Test:  Loss: 0.4548,  AUC: 0.8390, Acc: 77.2189,  Precision: 0.8426 -- Elapsed: 21m35s
2023-03-07 22:43:05.347738: Epoch: 3, Batch: 51, Loss: 0.4570, Elapsed: 0m35s
2023-03-07 22:43:33.150999: Epoch: 3, Batch: 52, Loss: 0.4311, Elapsed: 0m27s
2023-03-07 22:44:06.807757: Epoch: 3, Batch: 53, Loss: 0.4467, Elapsed: 0m33s
2023-03-07 22:44:51.515844: Epoch: 3, Batch: 54, Loss: 0.4687, Elapsed: 0m44s
2023-03-07 22:45:14.434714: Epoch: 3, Batch: 55, Loss: 0.4210, Elapsed: 0m22s
2023-03-07 22:45:52.822470: Epoch: 3, Batch: 56, Loss: 0.4678, Elapsed: 0m38s
2023-03-07 22:46:23.799228: Epoch: 3, Batch: 57, Loss: 0.4495, Elapsed: 0m30s
2023-03-07 22:47:02.285423: Epoch: 3, Batch: 58, Loss: 0.4624, Elapsed: 0m38s
2023-03-07 22:47:46.798041: Epoch: 3, Batch: 59, Loss: 0.4794, Elapsed: 0m44s
2023-03-07 22:48:48.689884: Epoch: 3, Batch: 60, Loss: 0.4806, Elapsed: 1m1s
2023-03-07 22:49:28.389780: Epoch: 3, Batch: 61, Loss: 0.4585, Elapsed: 0m39s
2023-03-07 22:49:56.026254: Epoch: 3, Batch: 62, Loss: 0.4388, Elapsed: 0m27s
2023-03-07 22:50:21.511917: Epoch: 3, Batch: 63, Loss: 0.4351, Elapsed: 0m25s
2023-03-07 22:50:45.374112: Epoch: 3, Batch: 64, Loss: 0.4301, Elapsed: 0m23s
2023-03-07 22:51:11.626052: Epoch: 3, Batch: 65, Loss: 0.4344, Elapsed: 0m26s
2023-03-07 22:51:45.410054: Epoch: 3, Batch: 66, Loss: 0.4477, Elapsed: 0m33s
2023-03-07 22:52:16.440412: Epoch: 3, Batch: 67, Loss: 0.4448, Elapsed: 0m31s
2023-03-07 22:52:46.561300: Epoch: 3, Batch: 68, Loss: 0.4418, Elapsed: 0m30s
2023-03-07 22:53:21.037350: Epoch: 3, Batch: 69, Loss: 0.4560, Elapsed: 0m34s
2023-03-07 22:53:52.998204: Epoch: 3, Batch: 70, Loss: 0.4621, Elapsed: 0m31s
2023-03-07 22:54:27.781275: Epoch: 3, Batch: 71, Loss: 0.4534, Elapsed: 0m34s
2023-03-07 22:54:50.346180: Epoch: 3, Batch: 72, Loss: 0.4309, Elapsed: 0m22s
2023-03-07 22:55:37.650158: Epoch: 3, Batch: 73, Loss: 0.4670, Elapsed: 0m47s
2023-03-07 22:56:07.979710: Epoch: 3, Batch: 74, Loss: 0.4460, Elapsed: 0m30s
2023-03-07 22:56:24.853572: Epoch: 3, Batch: 75, Loss: 0.4137, Elapsed: 0m16s
2023-03-07 22:56:54.290172: Epoch: 3, Batch: 76, Loss: 0.4384, Elapsed: 0m29s
2023-03-07 22:57:50.379618: Epoch: 3, Batch: 77, Loss: 0.4502, Elapsed: 0m56s
2023-03-07 22:58:24.389848: Epoch: 3, Batch: 78, Loss: 0.4425, Elapsed: 0m33s
2023-03-07 22:58:46.850315: Epoch: 3, Batch: 79, Loss: 0.4277, Elapsed: 0m22s
2023-03-07 22:59:24.437849: Epoch: 3, Batch: 80, Loss: 0.4499, Elapsed: 0m37s
2023-03-07 23:00:19.157291: Epoch: 4, Batch: 1, Loss: 0.4739, Elapsed: 0m54s
2023-03-07 23:00:51.781602: Epoch: 4, Batch: 2, Loss: 0.4651, Elapsed: 0m32s
2023-03-07 23:01:39.002966: Epoch: 4, Batch: 3, Loss: 0.4699, Elapsed: 0m47s
2023-03-07 23:02:13.413756: Epoch: 4, Batch: 4, Loss: 0.4614, Elapsed: 0m34s
2023-03-07 23:02:43.568582: Epoch: 4, Batch: 5, Loss: 0.4415, Elapsed: 0m30s
2023-03-07 23:03:18.034151: Epoch: 4, Batch: 6, Loss: 0.4444, Elapsed: 0m34s
2023-03-07 23:03:52.591663: Epoch: 4, Batch: 7, Loss: 0.4560, Elapsed: 0m34s
2023-03-07 23:04:37.328934: Epoch: 4, Batch: 8, Loss: 0.4652, Elapsed: 0m44s
2023-03-07 23:05:13.391254: Epoch: 4, Batch: 9, Loss: 0.4521, Elapsed: 0m36s
2023-03-07 23:05:35.753371: Epoch: 4, Batch: 10, Loss: 0.4294, Elapsed: 0m22s
2023-03-07 23:06:20.108220: Epoch: 4, Batch: 11, Loss: 0.4619, Elapsed: 0m44s
2023-03-07 23:07:04.877906: Epoch: 4, Batch: 12, Loss: 0.4613, Elapsed: 0m44s
2023-03-07 23:07:43.686971: Epoch: 4, Batch: 13, Loss: 0.4597, Elapsed: 0m38s
2023-03-07 23:08:16.367101: Epoch: 4, Batch: 14, Loss: 0.4443, Elapsed: 0m32s
2023-03-07 23:09:08.683165: Epoch: 4, Batch: 15, Loss: 0.4657, Elapsed: 0m52s
2023-03-07 23:09:37.112998: Epoch: 4, Batch: 16, Loss: 0.4330, Elapsed: 0m28s
2023-03-07 23:10:14.735702: Epoch: 4, Batch: 17, Loss: 0.4542, Elapsed: 0m37s
2023-03-07 23:10:49.260047: Epoch: 4, Batch: 18, Loss: 0.4550, Elapsed: 0m34s
2023-03-07 23:11:30.838926: Epoch: 4, Batch: 19, Loss: 0.4624, Elapsed: 0m41s
2023-03-07 23:12:15.600083: Epoch: 4, Batch: 20, Loss: 0.4746, Elapsed: 0m44s
2023-03-07 23:12:43.642597: Epoch: 4, Batch: 21, Loss: 0.4412, Elapsed: 0m28s
2023-03-07 23:13:38.764329: Epoch: 4, Batch: 22, Loss: 0.4411, Elapsed: 0m55s
2023-03-07 23:14:25.416435: Epoch: 4, Batch: 23, Loss: 0.4576, Elapsed: 0m46s
2023-03-07 23:15:00.210201: Epoch: 4, Batch: 24, Loss: 0.4530, Elapsed: 0m34s
2023-03-07 23:15:37.822781: Epoch: 4, Batch: 25, Loss: 0.4593, Elapsed: 0m37s
2023-03-07 23:16:10.581385: Epoch: 4, Batch: 26, Loss: 0.4407, Elapsed: 0m32s
2023-03-07 23:16:34.391163: Epoch: 4, Batch: 27, Loss: 0.4264, Elapsed: 0m23s
2023-03-07 23:17:02.611633: Epoch: 4, Batch: 28, Loss: 0.4353, Elapsed: 0m28s
2023-03-07 23:17:44.663275: Epoch: 4, Batch: 29, Loss: 0.4587, Elapsed: 0m42s
2023-03-07 23:18:27.692783: Epoch: 4, Batch: 30, Loss: 0.4657, Elapsed: 0m43s
2023-03-07 23:19:14.942006: Epoch: 4, Batch: 31, Loss: 0.4609, Elapsed: 0m47s
2023-03-07 23:19:54.579912: Epoch: 4, Batch: 32, Loss: 0.4547, Elapsed: 0m39s
2023-03-07 23:20:20.365581: Epoch: 4, Batch: 33, Loss: 0.4330, Elapsed: 0m25s
2023-03-07 23:20:44.793480: Epoch: 4, Batch: 34, Loss: 0.4320, Elapsed: 0m24s
2023-03-07 23:21:13.544583: Epoch: 4, Batch: 35, Loss: 0.4387, Elapsed: 0m28s
2023-03-07 23:21:47.720420: Epoch: 4, Batch: 36, Loss: 0.4340, Elapsed: 0m34s
2023-03-07 23:22:18.231587: Epoch: 4, Batch: 37, Loss: 0.4307, Elapsed: 0m30s
2023-03-07 23:22:53.810134: Epoch: 4, Batch: 38, Loss: 0.4423, Elapsed: 0m35s
2023-03-07 23:23:27.733217: Epoch: 4, Batch: 39, Loss: 0.4492, Elapsed: 0m33s
2023-03-07 23:23:50.476386: Epoch: 4, Batch: 40, Loss: 0.4132, Elapsed: 0m22s
2023-03-07 23:24:19.125294: Epoch: 4, Batch: 41, Loss: 0.4364, Elapsed: 0m28s
2023-03-07 23:24:49.902775: Epoch: 4, Batch: 42, Loss: 0.4414, Elapsed: 0m30s
2023-03-07 23:25:14.983761: Epoch: 4, Batch: 43, Loss: 0.4235, Elapsed: 0m25s
2023-03-07 23:26:02.275201: Epoch: 4, Batch: 44, Loss: 0.4620, Elapsed: 0m47s
2023-03-07 23:26:36.616308: Epoch: 4, Batch: 45, Loss: 0.4468, Elapsed: 0m34s
2023-03-07 23:27:33.610827: Epoch: 4, Batch: 46, Loss: 0.4709, Elapsed: 0m56s
2023-03-07 23:27:55.979810: Epoch: 4, Batch: 47, Loss: 0.4084, Elapsed: 0m22s
2023-03-07 23:28:18.295615: Epoch: 4, Batch: 48, Loss: 0.4207, Elapsed: 0m22s
2023-03-07 23:29:03.723322: Epoch: 4, Batch: 49, Loss: 0.4570, Elapsed: 0m45s
2023-03-07 23:29:33.304512: Epoch: 4, Batch: 50, Loss: 0.4397, Elapsed: 0m29s
2023-03-07 23:29:33.317609 Starting testing the valid set with 20 subgraphs!
2023-03-07 23:34:59.484408: validation Test:  Loss: 0.4519,  AUC: 0.8452, Acc: 77.1249,  Precision: 0.7959 -- Elapsed: 5m26s
2023-03-07 23:34:59.490025 Starting testing the train set with 20 subgraphs!
2023-03-07 23:56:37.477865: training Test:  Loss: 0.4496,  AUC: 0.8461, Acc: 77.3870,  Precision: 0.8009 -- Elapsed: 21m37s
2023-03-07 23:57:10.309154: Epoch: 4, Batch: 51, Loss: 0.4351, Elapsed: 0m32s
2023-03-07 23:57:36.805296: Epoch: 4, Batch: 52, Loss: 0.4321, Elapsed: 0m26s
2023-03-07 23:58:08.425927: Epoch: 4, Batch: 53, Loss: 0.4477, Elapsed: 0m31s
2023-03-07 23:58:25.081758: Epoch: 4, Batch: 54, Loss: 0.3853, Elapsed: 0m16s
2023-03-07 23:58:50.846228: Epoch: 4, Batch: 55, Loss: 0.4327, Elapsed: 0m25s
2023-03-07 23:59:17.286116: Epoch: 4, Batch: 56, Loss: 0.4389, Elapsed: 0m26s
2023-03-07 23:59:55.078869: Epoch: 4, Batch: 57, Loss: 0.4491, Elapsed: 0m37s
2023-03-08 00:00:28.323029: Epoch: 4, Batch: 58, Loss: 0.4383, Elapsed: 0m33s
2023-03-08 00:01:30.444845: Epoch: 4, Batch: 59, Loss: 0.4712, Elapsed: 1m2s
2023-03-08 00:02:00.991605: Epoch: 4, Batch: 60, Loss: 0.4398, Elapsed: 0m30s
2023-03-08 00:02:52.049287: Epoch: 4, Batch: 61, Loss: 0.4554, Elapsed: 0m51s
2023-03-08 00:03:22.102350: Epoch: 4, Batch: 62, Loss: 0.4336, Elapsed: 0m30s
2023-03-08 00:03:51.403442: Epoch: 4, Batch: 63, Loss: 0.4141, Elapsed: 0m29s
2023-03-08 00:04:25.716170: Epoch: 4, Batch: 64, Loss: 0.4350, Elapsed: 0m34s
2023-03-08 00:04:59.351134: Epoch: 4, Batch: 65, Loss: 0.4347, Elapsed: 0m33s
2023-03-08 00:05:28.989467: Epoch: 4, Batch: 66, Loss: 0.4397, Elapsed: 0m29s
2023-03-08 00:06:07.013601: Epoch: 4, Batch: 67, Loss: 0.4405, Elapsed: 0m38s
2023-03-08 00:06:41.647869: Epoch: 4, Batch: 68, Loss: 0.4384, Elapsed: 0m34s
2023-03-08 00:07:07.131410: Epoch: 4, Batch: 69, Loss: 0.4253, Elapsed: 0m25s
2023-03-08 00:07:31.451528: Epoch: 4, Batch: 70, Loss: 0.4095, Elapsed: 0m24s
2023-03-08 00:07:59.854119: Epoch: 4, Batch: 71, Loss: 0.4168, Elapsed: 0m28s
2023-03-08 00:08:30.455550: Epoch: 4, Batch: 72, Loss: 0.4306, Elapsed: 0m30s
2023-03-08 00:09:01.659388: Epoch: 4, Batch: 73, Loss: 0.4391, Elapsed: 0m31s
2023-03-08 00:09:40.791845: Epoch: 4, Batch: 74, Loss: 0.4471, Elapsed: 0m39s
2023-03-08 00:10:21.523491: Epoch: 4, Batch: 75, Loss: 0.4526, Elapsed: 0m40s
2023-03-08 00:11:03.258348: Epoch: 4, Batch: 76, Loss: 0.4556, Elapsed: 0m41s
2023-03-08 00:11:49.851626: Epoch: 4, Batch: 77, Loss: 0.4585, Elapsed: 0m46s
2023-03-08 00:12:25.189352: Epoch: 4, Batch: 78, Loss: 0.4519, Elapsed: 0m35s
2023-03-08 00:12:55.085122: Epoch: 4, Batch: 79, Loss: 0.4382, Elapsed: 0m29s
2023-03-08 00:13:15.055035: Epoch: 4, Batch: 80, Loss: 0.3836, Elapsed: 0m19s
2023-03-08 00:13:47.175645: Epoch: 5, Batch: 1, Loss: 0.4350, Elapsed: 0m32s
2023-03-08 00:14:20.380724: Epoch: 5, Batch: 2, Loss: 0.4359, Elapsed: 0m33s
2023-03-08 00:15:02.110806: Epoch: 5, Batch: 3, Loss: 0.4545, Elapsed: 0m41s
2023-03-08 00:15:48.620637: Epoch: 5, Batch: 4, Loss: 0.4518, Elapsed: 0m46s
2023-03-08 00:16:19.207673: Epoch: 5, Batch: 5, Loss: 0.4233, Elapsed: 0m30s
2023-03-08 00:17:05.700432: Epoch: 5, Batch: 6, Loss: 0.4557, Elapsed: 0m46s
2023-03-08 00:17:32.885677: Epoch: 5, Batch: 7, Loss: 0.4239, Elapsed: 0m27s
2023-03-08 00:18:15.395464: Epoch: 5, Batch: 8, Loss: 0.4579, Elapsed: 0m42s
2023-03-08 00:18:53.492405: Epoch: 5, Batch: 9, Loss: 0.4518, Elapsed: 0m38s
2023-03-08 00:19:31.860721: Epoch: 5, Batch: 10, Loss: 0.4382, Elapsed: 0m38s
2023-03-08 00:20:16.103291: Epoch: 5, Batch: 11, Loss: 0.4652, Elapsed: 0m44s
2023-03-08 00:20:57.623757: Epoch: 5, Batch: 12, Loss: 0.4544, Elapsed: 0m41s
2023-03-08 00:21:27.842981: Epoch: 5, Batch: 13, Loss: 0.4267, Elapsed: 0m30s
2023-03-08 00:22:02.399191: Epoch: 5, Batch: 14, Loss: 0.4428, Elapsed: 0m34s
2023-03-08 00:22:36.204362: Epoch: 5, Batch: 15, Loss: 0.4430, Elapsed: 0m33s
2023-03-08 00:23:06.616163: Epoch: 5, Batch: 16, Loss: 0.4273, Elapsed: 0m30s
2023-03-08 00:23:51.811513: Epoch: 5, Batch: 17, Loss: 0.4537, Elapsed: 0m45s
2023-03-08 00:24:19.369722: Epoch: 5, Batch: 18, Loss: 0.4309, Elapsed: 0m27s
2023-03-08 00:24:50.600307: Epoch: 5, Batch: 19, Loss: 0.4440, Elapsed: 0m31s
2023-03-08 00:25:30.399702: Epoch: 5, Batch: 20, Loss: 0.4480, Elapsed: 0m39s
2023-03-08 00:26:00.937202: Epoch: 5, Batch: 21, Loss: 0.4336, Elapsed: 0m30s
2023-03-08 00:26:57.976922: Epoch: 5, Batch: 22, Loss: 0.4633, Elapsed: 0m57s
2023-03-08 00:27:41.816228: Epoch: 5, Batch: 23, Loss: 0.4554, Elapsed: 0m43s
2023-03-08 00:28:43.679508: Epoch: 5, Batch: 24, Loss: 0.4671, Elapsed: 1m1s
2023-03-08 00:29:13.945420: Epoch: 5, Batch: 25, Loss: 0.4335, Elapsed: 0m30s
2023-03-08 00:30:01.297171: Epoch: 5, Batch: 26, Loss: 0.4631, Elapsed: 0m47s
2023-03-08 00:30:26.434031: Epoch: 5, Batch: 27, Loss: 0.4219, Elapsed: 0m25s
2023-03-08 00:31:05.622598: Epoch: 5, Batch: 28, Loss: 0.4442, Elapsed: 0m39s
2023-03-08 00:31:57.567597: Epoch: 5, Batch: 29, Loss: 0.4601, Elapsed: 0m51s
2023-03-08 00:32:20.198193: Epoch: 5, Batch: 30, Loss: 0.4168, Elapsed: 0m22s
2023-03-08 00:32:44.937926: Epoch: 5, Batch: 31, Loss: 0.4084, Elapsed: 0m24s
2023-03-08 00:33:19.724338: Epoch: 5, Batch: 32, Loss: 0.4337, Elapsed: 0m34s
2023-03-08 00:33:57.318542: Epoch: 5, Batch: 33, Loss: 0.4413, Elapsed: 0m37s
2023-03-08 00:34:29.718096: Epoch: 5, Batch: 34, Loss: 0.4498, Elapsed: 0m32s
2023-03-08 00:34:59.045937: Epoch: 5, Batch: 35, Loss: 0.4113, Elapsed: 0m29s
2023-03-08 00:35:28.002846: Epoch: 5, Batch: 36, Loss: 0.4371, Elapsed: 0m28s
2023-03-08 00:36:14.700146: Epoch: 5, Batch: 37, Loss: 0.4533, Elapsed: 0m46s
2023-03-08 00:36:40.124273: Epoch: 5, Batch: 38, Loss: 0.4201, Elapsed: 0m25s
2023-03-08 00:37:10.513474: Epoch: 5, Batch: 39, Loss: 0.4334, Elapsed: 0m30s
2023-03-08 00:38:05.687574: Epoch: 5, Batch: 40, Loss: 0.5865, Elapsed: 0m55s
2023-03-08 00:38:39.521197: Epoch: 5, Batch: 41, Loss: 0.4505, Elapsed: 0m33s
2023-03-08 00:39:11.146837: Epoch: 5, Batch: 42, Loss: 0.4376, Elapsed: 0m31s
2023-03-08 00:39:46.502587: Epoch: 5, Batch: 43, Loss: 0.4478, Elapsed: 0m35s
2023-03-08 00:40:10.366785: Epoch: 5, Batch: 44, Loss: 0.4285, Elapsed: 0m23s
2023-03-08 00:40:44.005500: Epoch: 5, Batch: 45, Loss: 0.4439, Elapsed: 0m33s
2023-03-08 00:41:14.654045: Epoch: 5, Batch: 46, Loss: 0.4433, Elapsed: 0m30s
2023-03-08 00:42:04.643059: Epoch: 5, Batch: 47, Loss: 0.4655, Elapsed: 0m49s
2023-03-08 00:42:29.778263: Epoch: 5, Batch: 48, Loss: 0.4384, Elapsed: 0m25s
2023-03-08 00:42:49.304513: Epoch: 5, Batch: 49, Loss: 0.4021, Elapsed: 0m19s
2023-03-08 00:43:17.868915: Epoch: 5, Batch: 50, Loss: 0.4395, Elapsed: 0m28s
2023-03-08 00:43:17.891409 Starting testing the valid set with 20 subgraphs!
2023-03-08 00:48:42.333146: validation Test:  Loss: 0.4536,  AUC: 0.8407, Acc: 77.7497,  Precision: 0.9001 -- Elapsed: 5m24s
2023-03-08 00:48:42.338816 Starting testing the train set with 20 subgraphs!
2023-03-08 01:10:13.036827: training Test:  Loss: 0.4517,  AUC: 0.8417, Acc: 77.9029,  Precision: 0.8968 -- Elapsed: 21m30s
2023-03-08 01:10:46.869083: Epoch: 5, Batch: 51, Loss: 0.4394, Elapsed: 0m33s
2023-03-08 01:11:14.766447: Epoch: 5, Batch: 52, Loss: 0.4276, Elapsed: 0m27s
2023-03-08 01:11:52.987520: Epoch: 5, Batch: 53, Loss: 0.4593, Elapsed: 0m38s
2023-03-08 01:12:26.959451: Epoch: 5, Batch: 54, Loss: 0.4406, Elapsed: 0m33s
2023-03-08 01:13:01.176563: Epoch: 5, Batch: 55, Loss: 0.4424, Elapsed: 0m34s
2023-03-08 01:13:29.701867: Epoch: 5, Batch: 56, Loss: 0.4338, Elapsed: 0m28s
2023-03-08 01:14:04.425817: Epoch: 5, Batch: 57, Loss: 0.4529, Elapsed: 0m34s
2023-03-08 01:14:33.021678: Epoch: 5, Batch: 58, Loss: 0.4401, Elapsed: 0m28s
2023-03-08 01:14:58.888745: Epoch: 5, Batch: 59, Loss: 0.4340, Elapsed: 0m25s
2023-03-08 01:15:45.964201: Epoch: 5, Batch: 60, Loss: 0.4635, Elapsed: 0m47s
2023-03-08 01:16:19.581153: Epoch: 5, Batch: 61, Loss: 0.4458, Elapsed: 0m33s
2023-03-08 01:16:48.158454: Epoch: 5, Batch: 62, Loss: 0.4381, Elapsed: 0m28s
2023-03-08 01:17:11.020009: Epoch: 5, Batch: 63, Loss: 0.4184, Elapsed: 0m22s
2023-03-08 01:17:43.485876: Epoch: 5, Batch: 64, Loss: 0.4362, Elapsed: 0m32s
2023-03-08 01:18:37.503804: Epoch: 5, Batch: 65, Loss: 0.4708, Elapsed: 0m54s
2023-03-08 01:19:10.032938: Epoch: 5, Batch: 66, Loss: 0.4457, Elapsed: 0m32s
2023-03-08 01:19:40.010397: Epoch: 5, Batch: 67, Loss: 0.4435, Elapsed: 0m29s
2023-03-08 01:20:20.462527: Epoch: 5, Batch: 68, Loss: 0.4589, Elapsed: 0m40s
2023-03-08 01:20:53.326751: Epoch: 5, Batch: 69, Loss: 0.4496, Elapsed: 0m32s
2023-03-08 01:21:31.040429: Epoch: 5, Batch: 70, Loss: 0.4566, Elapsed: 0m37s
2023-03-08 01:21:53.958413: Epoch: 5, Batch: 71, Loss: 0.4148, Elapsed: 0m22s
2023-03-08 01:22:27.897462: Epoch: 5, Batch: 72, Loss: 0.4525, Elapsed: 0m33s
2023-03-08 01:23:02.407548: Epoch: 5, Batch: 73, Loss: 0.4547, Elapsed: 0m34s
2023-03-08 01:23:47.203127: Epoch: 5, Batch: 74, Loss: 0.4595, Elapsed: 0m44s
2023-03-08 01:24:29.956317: Epoch: 5, Batch: 75, Loss: 0.4590, Elapsed: 0m42s
2023-03-08 01:25:13.744862: Epoch: 5, Batch: 76, Loss: 0.4601, Elapsed: 0m43s
2023-03-08 01:25:30.497412: Epoch: 5, Batch: 77, Loss: 0.3942, Elapsed: 0m16s
2023-03-08 01:25:52.119058: Epoch: 5, Batch: 78, Loss: 0.4094, Elapsed: 0m21s
2023-03-08 01:26:18.750822: Epoch: 5, Batch: 79, Loss: 0.4376, Elapsed: 0m26s
2023-03-08 01:26:44.543605: Epoch: 5, Batch: 80, Loss: 0.4365, Elapsed: 0m25s
2023-03-08 01:27:24.359643: Epoch: 6, Batch: 1, Loss: 0.4557, Elapsed: 0m39s
2023-03-08 01:28:08.698485: Epoch: 6, Batch: 2, Loss: 0.4704, Elapsed: 0m44s
2023-03-08 01:28:35.207984: Epoch: 6, Batch: 3, Loss: 0.4386, Elapsed: 0m26s
2023-03-08 01:29:10.383847: Epoch: 6, Batch: 4, Loss: 0.4443, Elapsed: 0m35s
2023-03-08 01:29:44.741566: Epoch: 6, Batch: 5, Loss: 0.4477, Elapsed: 0m34s
2023-03-08 01:30:18.388730: Epoch: 6, Batch: 6, Loss: 0.4381, Elapsed: 0m33s
2023-03-08 01:30:52.267503: Epoch: 6, Batch: 7, Loss: 0.4489, Elapsed: 0m33s
2023-03-08 01:31:22.473229: Epoch: 6, Batch: 8, Loss: 0.4343, Elapsed: 0m30s
2023-03-08 01:31:44.125077: Epoch: 6, Batch: 9, Loss: 0.4053, Elapsed: 0m21s
2023-03-08 01:32:23.200991: Epoch: 6, Batch: 10, Loss: 0.4499, Elapsed: 0m39s
2023-03-08 01:32:49.295761: Epoch: 6, Batch: 11, Loss: 0.4260, Elapsed: 0m26s
2023-03-08 01:33:16.821756: Epoch: 6, Batch: 12, Loss: 0.4296, Elapsed: 0m27s
2023-03-08 01:33:59.078444: Epoch: 6, Batch: 13, Loss: 0.4635, Elapsed: 0m42s
2023-03-08 01:34:41.114430: Epoch: 6, Batch: 14, Loss: 0.4559, Elapsed: 0m42s
2023-03-08 01:35:19.277402: Epoch: 6, Batch: 15, Loss: 0.4582, Elapsed: 0m38s
2023-03-08 01:35:43.169708: Epoch: 6, Batch: 16, Loss: 0.4216, Elapsed: 0m23s
2023-03-08 01:36:13.456076: Epoch: 6, Batch: 17, Loss: 0.4263, Elapsed: 0m30s
2023-03-08 01:36:58.614669: Epoch: 6, Batch: 18, Loss: 0.4570, Elapsed: 0m45s
2023-03-08 01:37:18.116412: Epoch: 6, Batch: 19, Loss: 0.3845, Elapsed: 0m19s
2023-03-08 01:37:47.725934: Epoch: 6, Batch: 20, Loss: 0.4357, Elapsed: 0m29s
2023-03-08 01:38:10.012090: Epoch: 6, Batch: 21, Loss: 0.4078, Elapsed: 0m22s
2023-03-08 01:38:40.143775: Epoch: 6, Batch: 22, Loss: 0.4347, Elapsed: 0m30s
2023-03-08 01:39:07.991987: Epoch: 6, Batch: 23, Loss: 0.4161, Elapsed: 0m27s
2023-03-08 01:39:42.864502: Epoch: 6, Batch: 24, Loss: 0.4459, Elapsed: 0m34s
2023-03-08 01:40:27.897645: Epoch: 6, Batch: 25, Loss: 0.4545, Elapsed: 0m45s
2023-03-08 01:40:59.144534: Epoch: 6, Batch: 26, Loss: 0.4431, Elapsed: 0m31s
2023-03-08 01:41:24.030063: Epoch: 6, Batch: 27, Loss: 0.4079, Elapsed: 0m24s
2023-03-08 01:41:58.726792: Epoch: 6, Batch: 28, Loss: 0.4469, Elapsed: 0m34s
2023-03-08 01:42:23.924177: Epoch: 6, Batch: 29, Loss: 0.4222, Elapsed: 0m25s
2023-03-08 01:42:48.879053: Epoch: 6, Batch: 30, Loss: 0.4167, Elapsed: 0m24s
2023-03-08 01:43:33.342374: Epoch: 6, Batch: 31, Loss: 0.4573, Elapsed: 0m44s
2023-03-08 01:43:50.024008: Epoch: 6, Batch: 32, Loss: 0.3830, Elapsed: 0m16s
2023-03-08 01:44:24.044478: Epoch: 6, Batch: 33, Loss: 0.4312, Elapsed: 0m34s
2023-03-08 01:44:52.970299: Epoch: 6, Batch: 34, Loss: 0.4324, Elapsed: 0m28s
2023-03-08 01:45:49.035681: Epoch: 6, Batch: 35, Loss: 0.4328, Elapsed: 0m56s
2023-03-08 01:46:35.689460: Epoch: 6, Batch: 36, Loss: 0.4574, Elapsed: 0m46s
2023-03-08 01:47:16.924189: Epoch: 6, Batch: 37, Loss: 0.4531, Elapsed: 0m41s
2023-03-08 01:47:47.792648: Epoch: 6, Batch: 38, Loss: 0.4374, Elapsed: 0m30s
2023-03-08 01:48:40.114854: Epoch: 6, Batch: 39, Loss: 0.4610, Elapsed: 0m52s
2023-03-08 01:49:12.220500: Epoch: 6, Batch: 40, Loss: 0.4514, Elapsed: 0m32s
2023-03-08 01:49:36.643820: Epoch: 6, Batch: 41, Loss: 0.4242, Elapsed: 0m24s
2023-03-08 01:50:23.872360: Epoch: 6, Batch: 42, Loss: 0.4570, Elapsed: 0m47s
2023-03-08 01:51:02.507055: Epoch: 6, Batch: 43, Loss: 0.4466, Elapsed: 0m38s
2023-03-08 01:51:31.889329: Epoch: 6, Batch: 44, Loss: 0.4100, Elapsed: 0m29s
2023-03-08 01:52:06.359060: Epoch: 6, Batch: 45, Loss: 0.4308, Elapsed: 0m34s
2023-03-08 01:52:34.085563: Epoch: 6, Batch: 46, Loss: 0.4263, Elapsed: 0m27s
2023-03-08 01:52:56.863411: Epoch: 6, Batch: 47, Loss: 0.4028, Elapsed: 0m22s
2023-03-08 01:53:27.294775: Epoch: 6, Batch: 48, Loss: 0.4253, Elapsed: 0m30s
2023-03-08 01:54:14.529854: Epoch: 6, Batch: 49, Loss: 0.4521, Elapsed: 0m47s
2023-03-08 01:54:43.944327: Epoch: 6, Batch: 50, Loss: 0.4348, Elapsed: 0m29s
2023-03-08 01:54:43.956825 Starting testing the valid set with 20 subgraphs!
2023-03-08 02:00:09.252719: validation Test:  Loss: 0.4409,  AUC: 0.8445, Acc: 78.2849,  Precision: 0.8963 -- Elapsed: 5m25s
2023-03-08 02:00:09.258402 Starting testing the train set with 20 subgraphs!
2023-03-08 02:21:40.833201: training Test:  Loss: 0.4384,  AUC: 0.8462, Acc: 78.4296,  Precision: 0.8926 -- Elapsed: 21m31s
2023-03-08 02:22:15.131633: Epoch: 6, Batch: 51, Loss: 0.4438, Elapsed: 0m34s
2023-03-08 02:22:49.140715: Epoch: 6, Batch: 52, Loss: 0.4341, Elapsed: 0m33s
2023-03-08 02:23:21.603371: Epoch: 6, Batch: 53, Loss: 0.4320, Elapsed: 0m32s
2023-03-08 02:24:23.099029: Epoch: 6, Batch: 54, Loss: 0.4669, Elapsed: 1m1s
2023-03-08 02:25:07.521716: Epoch: 6, Batch: 55, Loss: 0.4511, Elapsed: 0m44s
2023-03-08 02:25:41.322429: Epoch: 6, Batch: 56, Loss: 0.4443, Elapsed: 0m33s
2023-03-08 02:26:19.116696: Epoch: 6, Batch: 57, Loss: 0.4411, Elapsed: 0m37s
2023-03-08 02:26:52.119501: Epoch: 6, Batch: 58, Loss: 0.4357, Elapsed: 0m32s
2023-03-08 02:27:29.873771: Epoch: 6, Batch: 59, Loss: 0.4380, Elapsed: 0m37s
2023-03-08 02:28:00.890444: Epoch: 6, Batch: 60, Loss: 0.4335, Elapsed: 0m31s
2023-03-08 02:28:34.379693: Epoch: 6, Batch: 61, Loss: 0.4271, Elapsed: 0m33s
2023-03-08 02:28:57.299580: Epoch: 6, Batch: 62, Loss: 0.4161, Elapsed: 0m22s
2023-03-08 02:29:44.469013: Epoch: 6, Batch: 63, Loss: 0.4567, Elapsed: 0m47s
2023-03-08 02:30:11.018462: Epoch: 6, Batch: 64, Loss: 0.4200, Elapsed: 0m26s
2023-03-08 02:30:56.920697: Epoch: 6, Batch: 65, Loss: 0.4475, Elapsed: 0m45s
2023-03-08 02:31:46.893369: Epoch: 6, Batch: 66, Loss: 0.4495, Elapsed: 0m49s
2023-03-08 02:32:19.765342: Epoch: 6, Batch: 67, Loss: 0.4325, Elapsed: 0m32s
2023-03-08 02:33:01.372040: Epoch: 6, Batch: 68, Loss: 0.4534, Elapsed: 0m41s
2023-03-08 02:33:27.826972: Epoch: 6, Batch: 69, Loss: 0.4253, Elapsed: 0m26s
2023-03-08 02:34:07.592271: Epoch: 6, Batch: 70, Loss: 0.4475, Elapsed: 0m39s
2023-03-08 02:34:42.916342: Epoch: 6, Batch: 71, Loss: 0.4384, Elapsed: 0m35s
2023-03-08 02:35:12.769143: Epoch: 6, Batch: 72, Loss: 0.4328, Elapsed: 0m29s
2023-03-08 02:35:45.741635: Epoch: 6, Batch: 73, Loss: 0.4267, Elapsed: 0m32s
2023-03-08 02:36:15.524330: Epoch: 6, Batch: 74, Loss: 0.4252, Elapsed: 0m29s
2023-03-08 02:37:09.605449: Epoch: 6, Batch: 75, Loss: 0.4606, Elapsed: 0m54s
2023-03-08 02:37:43.748764: Epoch: 6, Batch: 76, Loss: 0.4417, Elapsed: 0m34s
2023-03-08 02:38:41.167033: Epoch: 6, Batch: 77, Loss: 0.4626, Elapsed: 0m57s
2023-03-08 02:39:09.781350: Epoch: 6, Batch: 78, Loss: 0.4244, Elapsed: 0m28s
2023-03-08 02:39:47.864467: Epoch: 6, Batch: 79, Loss: 0.4488, Elapsed: 0m38s
2023-03-08 02:40:15.766406: Epoch: 6, Batch: 80, Loss: 0.4228, Elapsed: 0m27s
2023-03-08 02:40:41.223459: Epoch: 7, Batch: 1, Loss: 0.4235, Elapsed: 0m25s
2023-03-08 02:41:08.699394: Epoch: 7, Batch: 2, Loss: 0.4205, Elapsed: 0m27s
2023-03-08 02:41:55.864223: Epoch: 7, Batch: 3, Loss: 0.4494, Elapsed: 0m47s
2023-03-08 02:42:25.584192: Epoch: 7, Batch: 4, Loss: 0.4230, Elapsed: 0m29s
2023-03-08 02:42:47.296740: Epoch: 7, Batch: 5, Loss: 0.4016, Elapsed: 0m21s
2023-03-08 02:43:10.180047: Epoch: 7, Batch: 6, Loss: 0.4019, Elapsed: 0m22s
2023-03-08 02:43:41.401887: Epoch: 7, Batch: 7, Loss: 0.4388, Elapsed: 0m31s
2023-03-08 02:44:16.214456: Epoch: 7, Batch: 8, Loss: 0.4379, Elapsed: 0m34s
2023-03-08 02:44:46.636980: Epoch: 7, Batch: 9, Loss: 0.4231, Elapsed: 0m30s
2023-03-08 02:45:28.480559: Epoch: 7, Batch: 10, Loss: 0.4519, Elapsed: 0m41s
2023-03-08 02:46:09.965637: Epoch: 7, Batch: 11, Loss: 0.4493, Elapsed: 0m41s
2023-03-08 02:46:42.315649: Epoch: 7, Batch: 12, Loss: 0.4309, Elapsed: 0m32s
2023-03-08 02:47:20.489642: Epoch: 7, Batch: 13, Loss: 0.4364, Elapsed: 0m38s
2023-03-08 02:47:55.316140: Epoch: 7, Batch: 14, Loss: 0.4398, Elapsed: 0m34s
2023-03-08 02:48:36.160065: Epoch: 7, Batch: 15, Loss: 0.4453, Elapsed: 0m40s
2023-03-08 02:49:13.948654: Epoch: 7, Batch: 16, Loss: 0.4414, Elapsed: 0m37s
2023-03-08 02:50:04.210419: Epoch: 7, Batch: 17, Loss: 0.4485, Elapsed: 0m50s
2023-03-08 02:50:48.086855: Epoch: 7, Batch: 18, Loss: 0.4530, Elapsed: 0m43s
2023-03-08 02:51:35.126813: Epoch: 7, Batch: 19, Loss: 0.4560, Elapsed: 0m47s
2023-03-08 02:51:57.664536: Epoch: 7, Batch: 20, Loss: 0.4150, Elapsed: 0m22s
2023-03-08 02:52:26.786047: Epoch: 7, Batch: 21, Loss: 0.4045, Elapsed: 0m29s
2023-03-08 02:52:59.546414: Epoch: 7, Batch: 22, Loss: 0.4442, Elapsed: 0m32s
2023-03-08 02:53:34.444997: Epoch: 7, Batch: 23, Loss: 0.4313, Elapsed: 0m34s
2023-03-08 02:54:08.627780: Epoch: 7, Batch: 24, Loss: 0.4278, Elapsed: 0m34s
2023-03-08 02:54:41.288811: Epoch: 7, Batch: 25, Loss: 0.4294, Elapsed: 0m32s
2023-03-08 02:55:26.170185: Epoch: 7, Batch: 26, Loss: 0.4478, Elapsed: 0m44s
2023-03-08 02:55:52.190360: Epoch: 7, Batch: 27, Loss: 0.4257, Elapsed: 0m26s
2023-03-08 02:56:18.653033: Epoch: 7, Batch: 28, Loss: 0.4189, Elapsed: 0m26s
2023-03-08 02:56:52.844752: Epoch: 7, Batch: 29, Loss: 0.4427, Elapsed: 0m34s
2023-03-08 02:57:17.498029: Epoch: 7, Batch: 30, Loss: 0.4041, Elapsed: 0m24s
2023-03-08 02:57:57.913730: Epoch: 7, Batch: 31, Loss: 0.4450, Elapsed: 0m40s
2023-03-08 02:58:28.071398: Epoch: 7, Batch: 32, Loss: 0.4232, Elapsed: 0m30s
2023-03-08 02:59:21.096362: Epoch: 7, Batch: 33, Loss: 0.4574, Elapsed: 0m53s
2023-03-08 02:59:43.371161: Epoch: 7, Batch: 34, Loss: 0.4017, Elapsed: 0m22s
2023-03-08 03:00:12.976386: Epoch: 7, Batch: 35, Loss: 0.4317, Elapsed: 0m29s
2023-03-08 03:00:46.694972: Epoch: 7, Batch: 36, Loss: 0.4394, Elapsed: 0m33s
2023-03-08 03:01:41.426980: Epoch: 7, Batch: 37, Loss: 0.4591, Elapsed: 0m54s
2023-03-08 03:02:10.016519: Epoch: 7, Batch: 38, Loss: 0.4267, Elapsed: 0m28s
2023-03-08 03:02:54.679079: Epoch: 7, Batch: 39, Loss: 0.4589, Elapsed: 0m44s
2023-03-08 03:03:29.238360: Epoch: 7, Batch: 40, Loss: 0.4335, Elapsed: 0m34s
2023-03-08 03:03:58.898075: Epoch: 7, Batch: 41, Loss: 0.4297, Elapsed: 0m29s
2023-03-08 03:04:33.498894: Epoch: 7, Batch: 42, Loss: 0.4414, Elapsed: 0m34s
2023-03-08 03:05:08.105961: Epoch: 7, Batch: 43, Loss: 0.4411, Elapsed: 0m34s
2023-03-08 03:06:05.984623: Epoch: 7, Batch: 44, Loss: 0.4569, Elapsed: 0m57s
2023-03-08 03:06:50.886170: Epoch: 7, Batch: 45, Loss: 0.4462, Elapsed: 0m44s
2023-03-08 03:07:34.504120: Epoch: 7, Batch: 46, Loss: 0.4481, Elapsed: 0m43s
2023-03-08 03:08:21.894294: Epoch: 7, Batch: 47, Loss: 0.4495, Elapsed: 0m47s
2023-03-08 03:08:50.170655: Epoch: 7, Batch: 48, Loss: 0.4212, Elapsed: 0m28s
2023-03-08 03:09:06.844297: Epoch: 7, Batch: 49, Loss: 0.3745, Elapsed: 0m16s
2023-03-08 03:09:32.731813: Epoch: 7, Batch: 50, Loss: 0.4332, Elapsed: 0m25s
2023-03-08 03:09:32.745579 Starting testing the valid set with 20 subgraphs!
2023-03-08 03:14:58.907755: validation Test:  Loss: 0.4353,  AUC: 0.8491, Acc: 78.4796,  Precision: 0.8627 -- Elapsed: 5m26s
2023-03-08 03:14:58.913724 Starting testing the train set with 20 subgraphs!
2023-03-08 03:36:35.906120: training Test:  Loss: 0.4347,  AUC: 0.8499, Acc: 78.5249,  Precision: 0.8522 -- Elapsed: 21m36s
2023-03-08 03:37:23.521416: Epoch: 7, Batch: 51, Loss: 0.4511, Elapsed: 0m47s
2023-03-08 03:37:54.009561: Epoch: 7, Batch: 52, Loss: 0.4334, Elapsed: 0m30s
2023-03-08 03:38:21.890596: Epoch: 7, Batch: 53, Loss: 0.4238, Elapsed: 0m27s
2023-03-08 03:39:01.676476: Epoch: 7, Batch: 54, Loss: 0.4402, Elapsed: 0m39s
2023-03-08 03:39:34.369938: Epoch: 7, Batch: 55, Loss: 0.4370, Elapsed: 0m32s
2023-03-08 03:40:03.607040: Epoch: 7, Batch: 56, Loss: 0.4251, Elapsed: 0m29s
2023-03-08 03:40:27.458955: Epoch: 7, Batch: 57, Loss: 0.4298, Elapsed: 0m23s
2023-03-08 03:41:01.127044: Epoch: 7, Batch: 58, Loss: 0.4236, Elapsed: 0m33s
2023-03-08 03:41:39.474249: Epoch: 7, Batch: 59, Loss: 0.4426, Elapsed: 0m38s
2023-03-08 03:42:13.722408: Epoch: 7, Batch: 60, Loss: 0.4263, Elapsed: 0m34s
2023-03-08 03:42:55.544480: Epoch: 7, Batch: 61, Loss: 0.4512, Elapsed: 0m41s
2023-03-08 03:43:33.494963: Epoch: 7, Batch: 62, Loss: 0.4512, Elapsed: 0m37s
2023-03-08 03:44:05.935809: Epoch: 7, Batch: 63, Loss: 0.4327, Elapsed: 0m32s
2023-03-08 03:44:40.082840: Epoch: 7, Batch: 64, Loss: 0.4285, Elapsed: 0m34s
2023-03-08 03:45:12.001205: Epoch: 7, Batch: 65, Loss: 0.4254, Elapsed: 0m31s
2023-03-08 03:45:38.028571: Epoch: 7, Batch: 66, Loss: 0.4334, Elapsed: 0m26s
2023-03-08 03:46:02.981527: Epoch: 7, Batch: 67, Loss: 0.4134, Elapsed: 0m24s
2023-03-08 03:46:34.463705: Epoch: 7, Batch: 68, Loss: 0.4315, Elapsed: 0m31s
2023-03-08 03:47:09.676066: Epoch: 7, Batch: 69, Loss: 0.4346, Elapsed: 0m35s
2023-03-08 03:48:05.335373: Epoch: 7, Batch: 70, Loss: 0.4459, Elapsed: 0m55s
2023-03-08 03:48:35.480485: Epoch: 7, Batch: 71, Loss: 0.4311, Elapsed: 0m30s
2023-03-08 03:49:06.228228: Epoch: 7, Batch: 72, Loss: 0.4198, Elapsed: 0m30s
2023-03-08 03:49:48.237607: Epoch: 7, Batch: 73, Loss: 0.4631, Elapsed: 0m41s
2023-03-08 03:50:49.834273: Epoch: 7, Batch: 74, Loss: 0.4705, Elapsed: 1m1s
2023-03-08 03:51:17.642339: Epoch: 7, Batch: 75, Loss: 0.4155, Elapsed: 0m27s
2023-03-08 03:51:42.038281: Epoch: 7, Batch: 76, Loss: 0.4255, Elapsed: 0m24s
2023-03-08 03:52:11.890040: Epoch: 7, Batch: 77, Loss: 0.4341, Elapsed: 0m29s
2023-03-08 03:52:57.332016: Epoch: 7, Batch: 78, Loss: 0.4468, Elapsed: 0m45s
2023-03-08 03:53:36.126881: Epoch: 7, Batch: 79, Loss: 0.4480, Elapsed: 0m38s
2023-03-08 03:53:55.540070: Epoch: 7, Batch: 80, Loss: 0.3791, Elapsed: 0m19s
2023-03-08 03:54:34.338462: Epoch: 8, Batch: 1, Loss: 0.4458, Elapsed: 0m38s
2023-03-08 03:55:14.645260: Epoch: 8, Batch: 2, Loss: 0.4453, Elapsed: 0m40s
2023-03-08 03:55:54.426176: Epoch: 8, Batch: 3, Loss: 0.4481, Elapsed: 0m39s
2023-03-08 03:56:18.775844: Epoch: 8, Batch: 4, Loss: 0.4150, Elapsed: 0m24s
2023-03-08 03:56:41.198259: Epoch: 8, Batch: 5, Loss: 0.4159, Elapsed: 0m22s
2023-03-08 03:57:12.432398: Epoch: 8, Batch: 6, Loss: 0.4428, Elapsed: 0m31s
2023-03-08 03:57:43.170939: Epoch: 8, Batch: 7, Loss: 0.4245, Elapsed: 0m30s
2023-03-08 03:58:27.865421: Epoch: 8, Batch: 8, Loss: 0.4468, Elapsed: 0m44s
2023-03-08 03:59:01.456198: Epoch: 8, Batch: 9, Loss: 0.4375, Elapsed: 0m33s
2023-03-08 03:59:32.468951: Epoch: 8, Batch: 10, Loss: 0.4299, Elapsed: 0m31s
2023-03-08 04:00:03.297815: Epoch: 8, Batch: 11, Loss: 0.4306, Elapsed: 0m30s
2023-03-08 04:00:38.452731: Epoch: 8, Batch: 12, Loss: 0.4410, Elapsed: 0m35s
2023-03-08 04:01:12.372081: Epoch: 8, Batch: 13, Loss: 0.4421, Elapsed: 0m33s
2023-03-08 04:01:36.986545: Epoch: 8, Batch: 14, Loss: 0.4059, Elapsed: 0m24s
2023-03-08 04:02:05.428928: Epoch: 8, Batch: 15, Loss: 0.4221, Elapsed: 0m28s
2023-03-08 04:02:39.265635: Epoch: 8, Batch: 16, Loss: 0.4392, Elapsed: 0m33s
2023-03-08 04:03:26.363002: Epoch: 8, Batch: 17, Loss: 0.4480, Elapsed: 0m47s
2023-03-08 04:03:54.360113: Epoch: 8, Batch: 18, Loss: 0.4204, Elapsed: 0m27s
2023-03-08 04:04:31.697242: Epoch: 8, Batch: 19, Loss: 0.4362, Elapsed: 0m37s
2023-03-08 04:05:19.066458: Epoch: 8, Batch: 20, Loss: 0.4556, Elapsed: 0m47s
2023-03-08 04:05:53.643427: Epoch: 8, Batch: 21, Loss: 0.4409, Elapsed: 0m34s
2023-03-08 04:06:23.147916: Epoch: 8, Batch: 22, Loss: 0.4276, Elapsed: 0m29s
2023-03-08 04:06:44.675763: Epoch: 8, Batch: 23, Loss: 0.3956, Elapsed: 0m21s
2023-03-08 04:07:15.186888: Epoch: 8, Batch: 24, Loss: 0.4265, Elapsed: 0m30s
2023-03-08 04:07:59.197199: Epoch: 8, Batch: 25, Loss: 0.4579, Elapsed: 0m43s
2023-03-08 04:08:29.782702: Epoch: 8, Batch: 26, Loss: 0.4205, Elapsed: 0m30s
2023-03-08 04:08:52.514575: Epoch: 8, Batch: 27, Loss: 0.3971, Elapsed: 0m22s
2023-03-08 04:09:12.603656: Epoch: 8, Batch: 28, Loss: 0.3724, Elapsed: 0m20s
2023-03-08 04:09:58.202045: Epoch: 8, Batch: 29, Loss: 0.4431, Elapsed: 0m45s
2023-03-08 04:10:23.702424: Epoch: 8, Batch: 30, Loss: 0.4089, Elapsed: 0m25s
2023-03-08 04:10:53.347109: Epoch: 8, Batch: 31, Loss: 0.4198, Elapsed: 0m29s
2023-03-08 04:11:26.275569: Epoch: 8, Batch: 32, Loss: 0.4298, Elapsed: 0m32s
2023-03-08 04:12:21.652101: Epoch: 8, Batch: 33, Loss: 0.4210, Elapsed: 0m55s
2023-03-08 04:12:57.320956: Epoch: 8, Batch: 34, Loss: 0.4330, Elapsed: 0m35s
2023-03-08 04:13:23.882577: Epoch: 8, Batch: 35, Loss: 0.4122, Elapsed: 0m26s
2023-03-08 04:14:16.755661: Epoch: 8, Batch: 36, Loss: 0.4547, Elapsed: 0m52s
2023-03-08 04:15:11.551030: Epoch: 8, Batch: 37, Loss: 0.4571, Elapsed: 0m54s
2023-03-08 04:15:37.789548: Epoch: 8, Batch: 38, Loss: 0.4203, Elapsed: 0m26s
2023-03-08 04:16:05.609087: Epoch: 8, Batch: 39, Loss: 0.4178, Elapsed: 0m27s
2023-03-08 04:16:38.656159: Epoch: 8, Batch: 40, Loss: 0.4341, Elapsed: 0m33s
2023-03-08 04:17:12.829751: Epoch: 8, Batch: 41, Loss: 0.4210, Elapsed: 0m34s
2023-03-08 04:17:50.837186: Epoch: 8, Batch: 42, Loss: 0.4306, Elapsed: 0m37s
2023-03-08 04:18:37.916305: Epoch: 8, Batch: 43, Loss: 0.4500, Elapsed: 0m47s
2023-03-08 04:19:07.449290: Epoch: 8, Batch: 44, Loss: 0.4170, Elapsed: 0m29s
2023-03-08 04:19:49.480710: Epoch: 8, Batch: 45, Loss: 0.4477, Elapsed: 0m42s
2023-03-08 04:20:17.892395: Epoch: 8, Batch: 46, Loss: 0.4139, Elapsed: 0m28s
2023-03-08 04:20:43.373950: Epoch: 8, Batch: 47, Loss: 0.4176, Elapsed: 0m25s
2023-03-08 04:21:30.410818: Epoch: 8, Batch: 48, Loss: 0.4464, Elapsed: 0m47s
2023-03-08 04:22:02.386913: Epoch: 8, Batch: 49, Loss: 0.4391, Elapsed: 0m31s
2023-03-08 04:22:33.205325: Epoch: 8, Batch: 50, Loss: 0.4126, Elapsed: 0m30s
2023-03-08 04:22:33.226115 Starting testing the valid set with 20 subgraphs!
2023-03-08 04:27:59.867416: validation Test:  Loss: 0.4331,  AUC: 0.8518, Acc: 78.6283,  Precision: 0.9173 -- Elapsed: 5m26s
2023-03-08 04:27:59.874108 Starting testing the train set with 20 subgraphs!
2023-03-08 04:49:36.080293: training Test:  Loss: 0.4307,  AUC: 0.8529, Acc: 78.7491,  Precision: 0.9112 -- Elapsed: 21m36s
2023-03-08 04:50:05.662182: Epoch: 8, Batch: 51, Loss: 0.3998, Elapsed: 0m29s
2023-03-08 04:50:39.911953: Epoch: 8, Batch: 52, Loss: 0.4313, Elapsed: 0m34s
2023-03-08 04:50:56.580979: Epoch: 8, Batch: 53, Loss: 0.3731, Elapsed: 0m16s
2023-03-08 04:51:39.217500: Epoch: 8, Batch: 54, Loss: 0.4397, Elapsed: 0m42s
2023-03-08 04:52:36.949098: Epoch: 8, Batch: 55, Loss: 0.4523, Elapsed: 0m57s
2023-03-08 04:53:10.646526: Epoch: 8, Batch: 56, Loss: 0.4221, Elapsed: 0m33s
2023-03-08 04:53:52.437457: Epoch: 8, Batch: 57, Loss: 0.4441, Elapsed: 0m41s
2023-03-08 04:54:24.628540: Epoch: 8, Batch: 58, Loss: 0.4184, Elapsed: 0m32s
2023-03-08 04:54:58.502474: Epoch: 8, Batch: 59, Loss: 0.4212, Elapsed: 0m33s
2023-03-08 04:55:42.497682: Epoch: 8, Batch: 60, Loss: 0.4431, Elapsed: 0m43s
2023-03-08 04:56:16.118000: Epoch: 8, Batch: 61, Loss: 0.4250, Elapsed: 0m33s
2023-03-08 04:56:46.294872: Epoch: 8, Batch: 62, Loss: 0.4254, Elapsed: 0m30s
2023-03-08 04:57:13.561813: Epoch: 8, Batch: 63, Loss: 0.4048, Elapsed: 0m27s
2023-03-08 04:58:15.533229: Epoch: 8, Batch: 64, Loss: 0.4570, Elapsed: 1m1s
2023-03-08 04:58:51.076407: Epoch: 8, Batch: 65, Loss: 0.4208, Elapsed: 0m35s
2023-03-08 04:59:20.553672: Epoch: 8, Batch: 66, Loss: 0.4220, Elapsed: 0m29s
2023-03-08 04:59:53.581501: Epoch: 8, Batch: 67, Loss: 0.4217, Elapsed: 0m33s
2023-03-08 05:00:25.501519: Epoch: 8, Batch: 68, Loss: 0.4209, Elapsed: 0m31s
2023-03-08 05:01:07.607976: Epoch: 8, Batch: 69, Loss: 0.4403, Elapsed: 0m42s
2023-03-08 05:01:51.054327: Epoch: 8, Batch: 70, Loss: 0.4430, Elapsed: 0m43s
2023-03-08 05:02:36.495931: Epoch: 8, Batch: 71, Loss: 0.4407, Elapsed: 0m45s
2023-03-08 05:02:58.768604: Epoch: 8, Batch: 72, Loss: 0.3914, Elapsed: 0m22s
2023-03-08 05:03:48.874671: Epoch: 8, Batch: 73, Loss: 0.4393, Elapsed: 0m50s
2023-03-08 05:04:28.835209: Epoch: 8, Batch: 74, Loss: 0.4330, Elapsed: 0m39s
2023-03-08 05:05:06.396352: Epoch: 8, Batch: 75, Loss: 0.4399, Elapsed: 0m37s
2023-03-08 05:05:31.611061: Epoch: 8, Batch: 76, Loss: 0.4139, Elapsed: 0m25s
2023-03-08 05:05:57.456381: Epoch: 8, Batch: 77, Loss: 0.4205, Elapsed: 0m25s
2023-03-08 05:06:35.379712: Epoch: 8, Batch: 78, Loss: 0.4411, Elapsed: 0m37s
2023-03-08 05:07:09.004042: Epoch: 8, Batch: 79, Loss: 0.4324, Elapsed: 0m33s
2023-03-08 05:07:34.858241: Epoch: 8, Batch: 80, Loss: 0.4102, Elapsed: 0m25s
2023-03-08 05:08:26.983384: Epoch: 9, Batch: 1, Loss: 0.4502, Elapsed: 0m52s
2023-03-08 05:08:57.243736: Epoch: 9, Batch: 2, Loss: 0.4183, Elapsed: 0m30s
2023-03-08 05:09:39.234829: Epoch: 9, Batch: 3, Loss: 0.4431, Elapsed: 0m41s
2023-03-08 05:10:41.164436: Epoch: 9, Batch: 4, Loss: 0.4584, Elapsed: 1m1s
2023-03-08 05:11:27.477591: Epoch: 9, Batch: 5, Loss: 0.4442, Elapsed: 0m46s
2023-03-08 05:11:58.122336: Epoch: 9, Batch: 6, Loss: 0.4192, Elapsed: 0m30s
2023-03-08 05:12:27.826195: Epoch: 9, Batch: 7, Loss: 0.4127, Elapsed: 0m29s
2023-03-08 05:13:23.767251: Epoch: 9, Batch: 8, Loss: 0.4256, Elapsed: 0m55s
2023-03-08 05:13:56.950895: Epoch: 9, Batch: 9, Loss: 0.4192, Elapsed: 0m33s
2023-03-08 05:14:13.604678: Epoch: 9, Batch: 10, Loss: 0.3689, Elapsed: 0m16s
2023-03-08 05:14:40.845022: Epoch: 9, Batch: 11, Loss: 0.4033, Elapsed: 0m27s
2023-03-08 05:15:09.113449: Epoch: 9, Batch: 12, Loss: 0.4126, Elapsed: 0m28s
2023-03-08 05:15:44.486616: Epoch: 9, Batch: 13, Loss: 0.4256, Elapsed: 0m35s
2023-03-08 05:16:09.480014: Epoch: 9, Batch: 14, Loss: 0.4105, Elapsed: 0m24s
2023-03-08 05:16:41.559892: Epoch: 9, Batch: 15, Loss: 0.4201, Elapsed: 0m32s
2023-03-08 05:17:23.951983: Epoch: 9, Batch: 16, Loss: 0.4457, Elapsed: 0m42s
2023-03-08 05:18:12.021643: Epoch: 9, Batch: 17, Loss: 0.4471, Elapsed: 0m48s
2023-03-08 05:19:09.128153: Epoch: 9, Batch: 18, Loss: 0.4522, Elapsed: 0m57s
2023-03-08 05:19:43.817286: Epoch: 9, Batch: 19, Loss: 0.4180, Elapsed: 0m34s
2023-03-08 05:20:17.881844: Epoch: 9, Batch: 20, Loss: 0.4225, Elapsed: 0m34s
2023-03-08 05:20:45.657146: Epoch: 9, Batch: 21, Loss: 0.4110, Elapsed: 0m27s
2023-03-08 05:21:25.205414: Epoch: 9, Batch: 22, Loss: 0.4322, Elapsed: 0m39s
2023-03-08 05:21:58.944851: Epoch: 9, Batch: 23, Loss: 0.4163, Elapsed: 0m33s
2023-03-08 05:22:43.406895: Epoch: 9, Batch: 24, Loss: 0.4485, Elapsed: 0m44s
2023-03-08 05:23:23.492040: Epoch: 9, Batch: 25, Loss: 0.4331, Elapsed: 0m40s
2023-03-08 05:24:01.831274: Epoch: 9, Batch: 26, Loss: 0.4314, Elapsed: 0m38s
2023-03-08 05:24:26.135530: Epoch: 9, Batch: 27, Loss: 0.3918, Elapsed: 0m24s
2023-03-08 05:25:10.414474: Epoch: 9, Batch: 28, Loss: 0.4431, Elapsed: 0m44s
2023-03-08 05:25:51.873969: Epoch: 9, Batch: 29, Loss: 0.4385, Elapsed: 0m41s
2023-03-08 05:26:17.695864: Epoch: 9, Batch: 30, Loss: 0.4115, Elapsed: 0m25s
2023-03-08 05:26:39.308313: Epoch: 9, Batch: 31, Loss: 0.3851, Elapsed: 0m21s
2023-03-08 05:27:11.850919: Epoch: 9, Batch: 32, Loss: 0.4351, Elapsed: 0m32s
2023-03-08 05:27:38.211296: Epoch: 9, Batch: 33, Loss: 0.4036, Elapsed: 0m26s
2023-03-08 05:28:09.971291: Epoch: 9, Batch: 34, Loss: 0.4266, Elapsed: 0m31s
2023-03-08 05:28:43.507748: Epoch: 9, Batch: 35, Loss: 0.4156, Elapsed: 0m33s
2023-03-08 05:29:19.121687: Epoch: 9, Batch: 36, Loss: 0.4139, Elapsed: 0m35s
2023-03-08 05:29:56.680677: Epoch: 9, Batch: 37, Loss: 0.4220, Elapsed: 0m37s
2023-03-08 05:30:27.276299: Epoch: 9, Batch: 38, Loss: 0.4091, Elapsed: 0m30s
2023-03-08 05:30:51.284389: Epoch: 9, Batch: 39, Loss: 0.4043, Elapsed: 0m23s
2023-03-08 05:31:20.656354: Epoch: 9, Batch: 40, Loss: 0.4113, Elapsed: 0m29s
2023-03-08 05:32:04.973974: Epoch: 9, Batch: 41, Loss: 0.4347, Elapsed: 0m44s
2023-03-08 05:32:27.999414: Epoch: 9, Batch: 42, Loss: 0.4149, Elapsed: 0m23s
2023-03-08 05:33:07.955923: Epoch: 9, Batch: 43, Loss: 0.4316, Elapsed: 0m39s
2023-03-08 05:33:53.163491: Epoch: 9, Batch: 44, Loss: 0.4348, Elapsed: 0m45s
2023-03-08 05:34:31.878225: Epoch: 9, Batch: 45, Loss: 0.4388, Elapsed: 0m38s
2023-03-08 05:35:04.809761: Epoch: 9, Batch: 46, Loss: 0.4308, Elapsed: 0m32s
2023-03-08 05:35:42.671784: Epoch: 9, Batch: 47, Loss: 0.4325, Elapsed: 0m37s
2023-03-08 05:36:16.512660: Epoch: 9, Batch: 48, Loss: 0.4266, Elapsed: 0m33s
2023-03-08 05:36:51.382295: Epoch: 9, Batch: 49, Loss: 0.4265, Elapsed: 0m34s
2023-03-08 05:37:23.871558: Epoch: 9, Batch: 50, Loss: 0.4190, Elapsed: 0m32s
2023-03-08 05:37:23.883989 Starting testing the valid set with 20 subgraphs!
2023-03-08 05:42:49.894972: validation Test:  Loss: 0.4248,  AUC: 0.8591, Acc: 78.9915,  Precision: 0.9158 -- Elapsed: 5m26s
2023-03-08 05:42:49.901136 Starting testing the train set with 20 subgraphs!
2023-03-08 06:04:25.367033: training Test:  Loss: 0.4227,  AUC: 0.8603, Acc: 79.0524,  Precision: 0.9102 -- Elapsed: 21m35s
2023-03-08 06:04:49.324384: Epoch: 9, Batch: 51, Loss: 0.3886, Elapsed: 0m23s
2023-03-08 06:05:35.907587: Epoch: 9, Batch: 52, Loss: 0.4387, Elapsed: 0m46s
2023-03-08 06:06:13.185198: Epoch: 9, Batch: 53, Loss: 0.4230, Elapsed: 0m37s
2023-03-08 06:06:42.511192: Epoch: 9, Batch: 54, Loss: 0.3888, Elapsed: 0m29s
2023-03-08 06:07:14.367170: Epoch: 9, Batch: 55, Loss: 0.4085, Elapsed: 0m31s
2023-03-08 06:07:43.981828: Epoch: 9, Batch: 56, Loss: 0.4160, Elapsed: 0m29s
2023-03-08 06:08:15.462004: Epoch: 9, Batch: 57, Loss: 0.4163, Elapsed: 0m31s
2023-03-08 06:08:49.311916: Epoch: 9, Batch: 58, Loss: 0.4261, Elapsed: 0m33s
2023-03-08 06:09:23.351862: Epoch: 9, Batch: 59, Loss: 0.4062, Elapsed: 0m34s
2023-03-08 06:10:09.193927: Epoch: 9, Batch: 60, Loss: 0.4322, Elapsed: 0m45s
2023-03-08 06:10:38.568452: Epoch: 9, Batch: 61, Loss: 0.4152, Elapsed: 0m29s
2023-03-08 06:11:22.904959: Epoch: 9, Batch: 62, Loss: 0.4369, Elapsed: 0m44s
2023-03-08 06:11:53.156300: Epoch: 9, Batch: 63, Loss: 0.3999, Elapsed: 0m30s
2023-03-08 06:12:43.385847: Epoch: 9, Batch: 64, Loss: 0.4337, Elapsed: 0m50s
2023-03-08 06:13:18.335792: Epoch: 9, Batch: 65, Loss: 0.4257, Elapsed: 0m34s
2023-03-08 06:13:51.802376: Epoch: 9, Batch: 66, Loss: 0.4226, Elapsed: 0m33s
2023-03-08 06:14:21.237181: Epoch: 9, Batch: 67, Loss: 0.4080, Elapsed: 0m29s
2023-03-08 06:14:51.376465: Epoch: 9, Batch: 68, Loss: 0.4171, Elapsed: 0m30s
2023-03-08 06:15:21.468535: Epoch: 9, Batch: 69, Loss: 0.4150, Elapsed: 0m30s
2023-03-08 06:15:41.227034: Epoch: 9, Batch: 70, Loss: 0.3688, Elapsed: 0m19s
2023-03-08 06:16:03.586213: Epoch: 9, Batch: 71, Loss: 0.3894, Elapsed: 0m22s
2023-03-08 06:16:31.360698: Epoch: 9, Batch: 72, Loss: 0.4089, Elapsed: 0m27s
2023-03-08 06:16:57.252555: Epoch: 9, Batch: 73, Loss: 0.4156, Elapsed: 0m25s
2023-03-08 06:17:22.435233: Epoch: 9, Batch: 74, Loss: 0.4070, Elapsed: 0m25s
2023-03-08 06:18:17.096269: Epoch: 9, Batch: 75, Loss: 0.4470, Elapsed: 0m54s
2023-03-08 06:18:43.240479: Epoch: 9, Batch: 76, Loss: 0.4039, Elapsed: 0m26s
2023-03-08 06:19:17.681195: Epoch: 9, Batch: 77, Loss: 0.4292, Elapsed: 0m34s
2023-03-08 06:20:00.250498: Epoch: 9, Batch: 78, Loss: 0.4335, Elapsed: 0m42s
2023-03-08 06:20:25.281370: Epoch: 9, Batch: 79, Loss: 0.4022, Elapsed: 0m25s
2023-03-08 06:21:12.440097: Epoch: 9, Batch: 80, Loss: 0.4339, Elapsed: 0m47s
2023-03-08 06:22:07.831993: Epoch: 10, Batch: 1, Loss: 0.4348, Elapsed: 0m55s
2023-03-08 06:22:37.547432: Epoch: 10, Batch: 2, Loss: 0.4265, Elapsed: 0m29s
2023-03-08 06:23:12.740459: Epoch: 10, Batch: 3, Loss: 0.4360, Elapsed: 0m35s
2023-03-08 06:23:51.033779: Epoch: 10, Batch: 4, Loss: 0.4322, Elapsed: 0m38s
2023-03-08 06:24:13.983511: Epoch: 10, Batch: 5, Loss: 0.4070, Elapsed: 0m22s
2023-03-08 06:24:43.991985: Epoch: 10, Batch: 6, Loss: 0.4180, Elapsed: 0m29s
2023-03-08 06:25:10.570191: Epoch: 10, Batch: 7, Loss: 0.4131, Elapsed: 0m26s
2023-03-08 06:25:35.068280: Epoch: 10, Batch: 8, Loss: 0.3920, Elapsed: 0m24s
2023-03-08 06:26:04.140128: Epoch: 10, Batch: 9, Loss: 0.3911, Elapsed: 0m29s
2023-03-08 06:26:44.927693: Epoch: 10, Batch: 10, Loss: 0.4309, Elapsed: 0m40s
2023-03-08 06:27:31.621593: Epoch: 10, Batch: 11, Loss: 0.4410, Elapsed: 0m46s
2023-03-08 06:28:10.821889: Epoch: 10, Batch: 12, Loss: 0.4267, Elapsed: 0m39s
2023-03-08 06:28:37.429571: Epoch: 10, Batch: 13, Loss: 0.4123, Elapsed: 0m26s
2023-03-08 06:29:10.040613: Epoch: 10, Batch: 14, Loss: 0.4276, Elapsed: 0m32s
2023-03-08 06:29:43.145946: Epoch: 10, Batch: 15, Loss: 0.4165, Elapsed: 0m33s
2023-03-08 06:30:27.873329: Epoch: 10, Batch: 16, Loss: 0.4450, Elapsed: 0m44s
2023-03-08 06:31:12.336216: Epoch: 10, Batch: 17, Loss: 0.4449, Elapsed: 0m44s
2023-03-08 06:31:52.734681: Epoch: 10, Batch: 18, Loss: 0.4276, Elapsed: 0m40s
2023-03-08 06:32:24.559473: Epoch: 10, Batch: 19, Loss: 0.4309, Elapsed: 0m31s
2023-03-08 06:33:09.090054: Epoch: 10, Batch: 20, Loss: 0.4500, Elapsed: 0m44s
2023-03-08 06:33:34.264287: Epoch: 10, Batch: 21, Loss: 0.4002, Elapsed: 0m25s
2023-03-08 06:34:08.588937: Epoch: 10, Batch: 22, Loss: 0.4232, Elapsed: 0m34s
2023-03-08 06:35:06.691984: Epoch: 10, Batch: 23, Loss: 0.4405, Elapsed: 0m58s
2023-03-08 06:35:38.140687: Epoch: 10, Batch: 24, Loss: 0.4289, Elapsed: 0m31s
2023-03-08 06:36:02.678894: Epoch: 10, Batch: 25, Loss: 0.3988, Elapsed: 0m24s
2023-03-08 06:36:36.274852: Epoch: 10, Batch: 26, Loss: 0.4123, Elapsed: 0m33s
2023-03-08 06:37:06.292166: Epoch: 10, Batch: 27, Loss: 0.4119, Elapsed: 0m30s
2023-03-08 06:37:34.895914: Epoch: 10, Batch: 28, Loss: 0.4073, Elapsed: 0m28s
2023-03-08 06:38:12.470925: Epoch: 10, Batch: 29, Loss: 0.4321, Elapsed: 0m37s
2023-03-08 06:38:37.482857: Epoch: 10, Batch: 30, Loss: 0.3962, Elapsed: 0m24s
2023-03-08 06:39:15.706815: Epoch: 10, Batch: 31, Loss: 0.4333, Elapsed: 0m38s
2023-03-08 06:39:48.079505: Epoch: 10, Batch: 32, Loss: 0.4148, Elapsed: 0m32s
2023-03-08 06:40:14.335583: Epoch: 10, Batch: 33, Loss: 0.4002, Elapsed: 0m26s
2023-03-08 06:41:06.340281: Epoch: 10, Batch: 34, Loss: 0.4380, Elapsed: 0m51s
2023-03-08 06:41:33.571460: Epoch: 10, Batch: 35, Loss: 0.4011, Elapsed: 0m27s
2023-03-08 06:42:02.525463: Epoch: 10, Batch: 36, Loss: 0.4171, Elapsed: 0m28s
2023-03-08 06:42:33.261798: Epoch: 10, Batch: 37, Loss: 0.4154, Elapsed: 0m30s
2023-03-08 06:43:00.839343: Epoch: 10, Batch: 38, Loss: 0.4053, Elapsed: 0m27s
2023-03-08 06:43:43.584943: Epoch: 10, Batch: 39, Loss: 0.4394, Elapsed: 0m42s
2023-03-08 06:44:05.170857: Epoch: 10, Batch: 40, Loss: 0.3828, Elapsed: 0m21s
2023-03-08 06:44:24.907584: Epoch: 10, Batch: 41, Loss: 0.3615, Elapsed: 0m19s
2023-03-08 06:44:49.663769: Epoch: 10, Batch: 42, Loss: 0.4008, Elapsed: 0m24s
2023-03-08 06:45:36.896798: Epoch: 10, Batch: 43, Loss: 0.4318, Elapsed: 0m47s
2023-03-08 06:45:59.575304: Epoch: 10, Batch: 44, Loss: 0.3815, Elapsed: 0m22s
2023-03-08 06:46:33.665114: Epoch: 10, Batch: 45, Loss: 0.4207, Elapsed: 0m34s
2023-03-08 06:47:04.170986: Epoch: 10, Batch: 46, Loss: 0.4067, Elapsed: 0m30s
2023-03-08 06:47:46.229803: Epoch: 10, Batch: 47, Loss: 0.4321, Elapsed: 0m42s
2023-03-08 06:48:13.822222: Epoch: 10, Batch: 48, Loss: 0.4059, Elapsed: 0m27s
2023-03-08 06:48:44.160518: Epoch: 10, Batch: 49, Loss: 0.3981, Elapsed: 0m30s
2023-03-08 06:49:29.864816: Epoch: 10, Batch: 50, Loss: 0.4289, Elapsed: 0m45s
2023-03-08 06:49:29.879212 Starting testing the valid set with 20 subgraphs!
2023-03-08 06:54:56.172027: validation Test:  Loss: 0.4191,  AUC: 0.8616, Acc: 78.9938,  Precision: 0.8770 -- Elapsed: 5m26s
2023-03-08 06:54:56.178256 Starting testing the train set with 20 subgraphs!
2023-03-08 07:16:33.189401: training Test:  Loss: 0.4175,  AUC: 0.8627, Acc: 79.0862,  Precision: 0.8833 -- Elapsed: 21m37s
2023-03-08 07:17:04.096873: Epoch: 10, Batch: 51, Loss: 0.4047, Elapsed: 0m30s
2023-03-08 07:17:20.686816: Epoch: 10, Batch: 52, Loss: 0.3643, Elapsed: 0m16s
2023-03-08 07:17:58.890511: Epoch: 10, Batch: 53, Loss: 0.4180, Elapsed: 0m38s
2023-03-08 07:18:32.827261: Epoch: 10, Batch: 54, Loss: 0.4229, Elapsed: 0m33s
2023-03-08 07:18:55.021639: Epoch: 10, Batch: 55, Loss: 0.3855, Elapsed: 0m22s
2023-03-08 07:19:37.088535: Epoch: 10, Batch: 56, Loss: 0.4330, Elapsed: 0m42s
2023-03-08 07:20:21.041929: Epoch: 10, Batch: 57, Loss: 0.4320, Elapsed: 0m43s
2023-03-08 07:20:55.012176: Epoch: 10, Batch: 58, Loss: 0.4212, Elapsed: 0m33s
2023-03-08 07:21:23.035436: Epoch: 10, Batch: 59, Loss: 0.4067, Elapsed: 0m28s
2023-03-08 07:21:57.564204: Epoch: 10, Batch: 60, Loss: 0.4261, Elapsed: 0m34s
2023-03-08 07:22:27.775440: Epoch: 10, Batch: 61, Loss: 0.4051, Elapsed: 0m30s
2023-03-08 07:23:02.700615: Epoch: 10, Batch: 62, Loss: 0.4135, Elapsed: 0m34s
2023-03-08 07:23:30.834150: Epoch: 10, Batch: 63, Loss: 0.3919, Elapsed: 0m28s
2023-03-08 07:24:25.165923: Epoch: 10, Batch: 64, Loss: 0.4442, Elapsed: 0m54s
2023-03-08 07:25:10.063603: Epoch: 10, Batch: 65, Loss: 0.4328, Elapsed: 0m44s
2023-03-08 07:25:41.600557: Epoch: 10, Batch: 66, Loss: 0.4134, Elapsed: 0m31s
2023-03-08 07:26:13.747572: Epoch: 10, Batch: 67, Loss: 0.4127, Elapsed: 0m32s
2023-03-08 07:26:51.591139: Epoch: 10, Batch: 68, Loss: 0.4169, Elapsed: 0m37s
2023-03-08 07:27:38.298968: Epoch: 10, Batch: 69, Loss: 0.4333, Elapsed: 0m46s
2023-03-08 07:28:07.209724: Epoch: 10, Batch: 70, Loss: 0.4047, Elapsed: 0m28s
2023-03-08 07:28:39.133751: Epoch: 10, Batch: 71, Loss: 0.4134, Elapsed: 0m31s
2023-03-08 07:29:41.105248: Epoch: 10, Batch: 72, Loss: 0.4469, Elapsed: 1m1s
2023-03-08 07:30:23.703001: Epoch: 10, Batch: 73, Loss: 0.4312, Elapsed: 0m42s
2023-03-08 07:30:57.830688: Epoch: 10, Batch: 74, Loss: 0.4046, Elapsed: 0m34s
2023-03-08 07:31:44.677089: Epoch: 10, Batch: 75, Loss: 0.4351, Elapsed: 0m46s
2023-03-08 07:32:20.753898: Epoch: 10, Batch: 76, Loss: 0.4187, Elapsed: 0m36s
2023-03-08 07:32:54.165993: Epoch: 10, Batch: 77, Loss: 0.4171, Elapsed: 0m33s
2023-03-08 07:33:28.104160: Epoch: 10, Batch: 78, Loss: 0.4133, Elapsed: 0m33s
2023-03-08 07:34:18.837529: Epoch: 10, Batch: 79, Loss: 0.4344, Elapsed: 0m50s
2023-03-08 07:34:52.639646: Epoch: 10, Batch: 80, Loss: 0.4077, Elapsed: 0m33s
2023-03-08 07:35:27.078012: Epoch: 11, Batch: 1, Loss: 0.4240, Elapsed: 0m34s
2023-03-08 07:36:24.821939: Epoch: 11, Batch: 2, Loss: 0.4418, Elapsed: 0m57s
2023-03-08 07:36:52.477914: Epoch: 11, Batch: 3, Loss: 0.4028, Elapsed: 0m27s
2023-03-08 07:37:22.806860: Epoch: 11, Batch: 4, Loss: 0.4069, Elapsed: 0m30s
2023-03-08 07:38:09.516537: Epoch: 11, Batch: 5, Loss: 0.4335, Elapsed: 0m46s
2023-03-08 07:38:42.853295: Epoch: 11, Batch: 6, Loss: 0.4202, Elapsed: 0m33s
2023-03-08 07:39:08.380772: Epoch: 11, Batch: 7, Loss: 0.3998, Elapsed: 0m25s
2023-03-08 07:39:38.350774: Epoch: 11, Batch: 8, Loss: 0.4109, Elapsed: 0m29s
2023-03-08 07:40:40.037948: Epoch: 11, Batch: 9, Loss: 0.4473, Elapsed: 1m1s
2023-03-08 07:41:11.887493: Epoch: 11, Batch: 10, Loss: 0.4335, Elapsed: 0m31s
2023-03-08 07:41:49.789934: Epoch: 11, Batch: 11, Loss: 0.4288, Elapsed: 0m37s
2023-03-08 07:42:09.159831: Epoch: 11, Batch: 12, Loss: 0.3579, Elapsed: 0m19s
2023-03-08 07:42:49.255503: Epoch: 11, Batch: 13, Loss: 0.4252, Elapsed: 0m40s
2023-03-08 07:43:36.289631: Epoch: 11, Batch: 14, Loss: 0.4364, Elapsed: 0m47s
2023-03-08 07:44:11.423551: Epoch: 11, Batch: 15, Loss: 0.4209, Elapsed: 0m35s
2023-03-08 07:44:45.159436: Epoch: 11, Batch: 16, Loss: 0.4234, Elapsed: 0m33s
2023-03-08 07:45:27.310725: Epoch: 11, Batch: 17, Loss: 0.4356, Elapsed: 0m42s
2023-03-08 07:46:05.788586: Epoch: 11, Batch: 18, Loss: 0.4158, Elapsed: 0m38s
2023-03-08 07:46:43.291102: Epoch: 11, Batch: 19, Loss: 0.4206, Elapsed: 0m37s
2023-03-08 07:47:16.169147: Epoch: 11, Batch: 20, Loss: 0.4090, Elapsed: 0m32s
2023-03-08 07:47:48.109610: Epoch: 11, Batch: 21, Loss: 0.4106, Elapsed: 0m31s
2023-03-08 07:48:04.776126: Epoch: 11, Batch: 22, Loss: 0.3595, Elapsed: 0m16s
2023-03-08 07:48:32.716796: Epoch: 11, Batch: 23, Loss: 0.3926, Elapsed: 0m27s
2023-03-08 07:49:06.328979: Epoch: 11, Batch: 24, Loss: 0.4037, Elapsed: 0m33s
2023-03-08 07:49:31.657404: Epoch: 11, Batch: 25, Loss: 0.3996, Elapsed: 0m25s
2023-03-08 07:50:01.223532: Epoch: 11, Batch: 26, Loss: 0.4026, Elapsed: 0m29s
2023-03-08 07:50:46.188188: Epoch: 11, Batch: 27, Loss: 0.4302, Elapsed: 0m44s
2023-03-08 07:51:20.955152: Epoch: 11, Batch: 28, Loss: 0.4230, Elapsed: 0m34s
2023-03-08 07:52:01.541724: Epoch: 11, Batch: 29, Loss: 0.4237, Elapsed: 0m40s
2023-03-08 07:52:44.309564: Epoch: 11, Batch: 30, Loss: 0.4374, Elapsed: 0m42s
2023-03-08 07:53:11.961431: Epoch: 11, Batch: 31, Loss: 0.4056, Elapsed: 0m27s
2023-03-08 07:53:57.310671: Epoch: 11, Batch: 32, Loss: 0.4285, Elapsed: 0m45s
2023-03-08 07:54:47.201123: Epoch: 11, Batch: 33, Loss: 0.4308, Elapsed: 0m49s
2023-03-08 07:55:12.459160: Epoch: 11, Batch: 34, Loss: 0.4062, Elapsed: 0m25s
2023-03-08 07:56:05.132756: Epoch: 11, Batch: 35, Loss: 0.4405, Elapsed: 0m52s
2023-03-08 07:56:31.042564: Epoch: 11, Batch: 36, Loss: 0.4067, Elapsed: 0m25s
2023-03-08 07:57:05.404978: Epoch: 11, Batch: 37, Loss: 0.4223, Elapsed: 0m34s
2023-03-08 07:57:39.580617: Epoch: 11, Batch: 38, Loss: 0.4245, Elapsed: 0m34s
2023-03-08 07:58:06.119859: Epoch: 11, Batch: 39, Loss: 0.4083, Elapsed: 0m26s
2023-03-08 07:58:33.707674: Epoch: 11, Batch: 40, Loss: 0.4070, Elapsed: 0m27s
2023-03-08 07:59:03.110397: Epoch: 11, Batch: 41, Loss: 0.4106, Elapsed: 0m29s
2023-03-08 07:59:46.541784: Epoch: 11, Batch: 42, Loss: 0.4338, Elapsed: 0m43s
2023-03-08 08:00:16.107487: Epoch: 11, Batch: 43, Loss: 0.4229, Elapsed: 0m29s
2023-03-08 08:00:47.935927: Epoch: 11, Batch: 44, Loss: 0.4102, Elapsed: 0m31s
2023-03-08 08:01:19.553167: Epoch: 11, Batch: 45, Loss: 0.4248, Elapsed: 0m31s
2023-03-08 08:01:51.824913: Epoch: 11, Batch: 46, Loss: 0.4180, Elapsed: 0m32s
2023-03-08 08:02:21.602177: Epoch: 11, Batch: 47, Loss: 0.3907, Elapsed: 0m29s
2023-03-08 08:02:45.468611: Epoch: 11, Batch: 48, Loss: 0.3954, Elapsed: 0m23s
2023-03-08 08:03:19.830447: Epoch: 11, Batch: 49, Loss: 0.4211, Elapsed: 0m34s
2023-03-08 08:03:53.221596: Epoch: 11, Batch: 50, Loss: 0.4126, Elapsed: 0m33s
2023-03-08 08:03:53.234024 Starting testing the valid set with 20 subgraphs!
2023-03-08 08:09:18.602665: validation Test:  Loss: 0.4185,  AUC: 0.8622, Acc: 78.9728,  Precision: 0.8943 -- Elapsed: 5m25s
2023-03-08 08:09:18.608782 Starting testing the train set with 20 subgraphs!
2023-03-08 08:30:49.338398: training Test:  Loss: 0.4169,  AUC: 0.8634, Acc: 79.0477,  Precision: 0.8987 -- Elapsed: 21m30s
2023-03-08 08:31:24.203141: Epoch: 11, Batch: 51, Loss: 0.4091, Elapsed: 0m34s
2023-03-08 08:31:54.285809: Epoch: 11, Batch: 52, Loss: 0.3980, Elapsed: 0m30s
2023-03-08 08:32:15.971036: Epoch: 11, Batch: 53, Loss: 0.3809, Elapsed: 0m21s
2023-03-08 08:32:51.082631: Epoch: 11, Batch: 54, Loss: 0.4180, Elapsed: 0m35s
2023-03-08 08:33:21.355121: Epoch: 11, Batch: 55, Loss: 0.4048, Elapsed: 0m30s
2023-03-08 08:33:58.905179: Epoch: 11, Batch: 56, Loss: 0.4329, Elapsed: 0m37s
2023-03-08 08:34:42.802655: Epoch: 11, Batch: 57, Loss: 0.4472, Elapsed: 0m43s
2023-03-08 08:35:17.673814: Epoch: 11, Batch: 58, Loss: 0.4125, Elapsed: 0m34s
2023-03-08 08:36:04.606439: Epoch: 11, Batch: 59, Loss: 0.4353, Elapsed: 0m46s
