Printing configs: 
train_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_graphs
valid_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/valid_graphs
dataset: TuysuzPaper
log_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run thesis2/
run_type: new_run
gpu: -1
n_files: 100
n_valid: 20
n_train: 80
batch_size: 1
lr_c: 0.01
n_iters: 3
n_epoch: 20
TEST_every: 50
hid_dim: 4
network: QGNN
optimizer: Adam
loss_func: BinaryCrossentropy
n_thread: 4
log_verbosity: 2
EN_qc: {'PQC_id': '10', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}
NN_qc: {'PQC_id': '10', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}
Log dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run thesis2/
Training data input dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_graphs
Validation data input dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_graphs
Model: "GNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
InputNet (Dense)             multiple                  16        
_________________________________________________________________
EdgeNet (EdgeNet)            multiple                  81        
_________________________________________________________________
NodeNet (NodeNet)            multiple                  124       
=================================================================
Total params: 221
Trainable params: 221
Non-trainable params: 0
_________________________________________________________________
None
2023-03-31 12:29:46.679861 Starting testing the valid set with 20 subgraphs!
2023-03-31 12:34:41.081393: validation Test:  Loss: 0.7098,  AUC: 0.5223, Acc: 50.6669,  Precision: 0.5185 -- Elapsed: 4m54s
2023-03-31 12:34:41.082504 Starting testing the train set with 20 subgraphs!
2023-03-31 12:54:06.123063: training Test:  Loss: 0.7239,  AUC: 0.5231, Acc: 50.8225,  Precision: 0.5310 -- Elapsed: 19m24s
2023-03-31 12:54:06.125825: Training is starting!
2023-03-31 12:54:34.287571: Epoch: 1, Batch: 1, Loss: 0.6804, Elapsed: 0m28s
2023-03-31 12:54:59.269774: Epoch: 1, Batch: 2, Loss: 0.6815, Elapsed: 0m24s
2023-03-31 12:55:31.761536: Epoch: 1, Batch: 3, Loss: 0.7657, Elapsed: 0m32s
2023-03-31 12:55:59.220221: Epoch: 1, Batch: 4, Loss: 0.7007, Elapsed: 0m27s
2023-03-31 12:56:24.764587: Epoch: 1, Batch: 5, Loss: 0.6967, Elapsed: 0m25s
2023-03-31 12:56:50.854286: Epoch: 1, Batch: 6, Loss: 0.7279, Elapsed: 0m26s
2023-03-31 12:57:10.713545: Epoch: 1, Batch: 7, Loss: 0.7145, Elapsed: 0m19s
2023-03-31 12:57:39.900815: Epoch: 1, Batch: 8, Loss: 0.6869, Elapsed: 0m29s
2023-03-31 12:58:19.767799: Epoch: 1, Batch: 9, Loss: 0.7581, Elapsed: 0m39s
2023-03-31 12:58:41.285202: Epoch: 1, Batch: 10, Loss: 0.7106, Elapsed: 0m21s
2023-03-31 12:59:10.535811: Epoch: 1, Batch: 11, Loss: 0.7238, Elapsed: 0m29s
2023-03-31 12:59:36.428735: Epoch: 1, Batch: 12, Loss: 0.7158, Elapsed: 0m25s
2023-03-31 13:00:17.182544: Epoch: 1, Batch: 13, Loss: 0.7144, Elapsed: 0m40s
2023-03-31 13:00:38.626223: Epoch: 1, Batch: 14, Loss: 0.7847, Elapsed: 0m21s
2023-03-31 13:01:00.020807: Epoch: 1, Batch: 15, Loss: 0.7300, Elapsed: 0m21s
2023-03-31 13:01:27.097052: Epoch: 1, Batch: 16, Loss: 0.6944, Elapsed: 0m27s
2023-03-31 13:02:04.358738: Epoch: 1, Batch: 17, Loss: 0.7420, Elapsed: 0m37s
2023-03-31 13:02:33.828354: Epoch: 1, Batch: 18, Loss: 0.7212, Elapsed: 0m29s
2023-03-31 13:03:08.042559: Epoch: 1, Batch: 19, Loss: 0.6889, Elapsed: 0m34s
2023-03-31 13:03:45.303214: Epoch: 1, Batch: 20, Loss: 0.7496, Elapsed: 0m37s
2023-03-31 13:04:23.347121: Epoch: 1, Batch: 21, Loss: 0.6822, Elapsed: 0m29s
2023-03-31 13:04:56.517672: Epoch: 1, Batch: 22, Loss: 0.7099, Elapsed: 0m33s
2023-03-31 13:05:36.680298: Epoch: 1, Batch: 23, Loss: 0.6899, Elapsed: 0m40s
2023-03-31 13:06:04.382933: Epoch: 1, Batch: 24, Loss: 0.6647, Elapsed: 0m27s
2023-03-31 13:06:32.402402: Epoch: 1, Batch: 25, Loss: 0.6992, Elapsed: 0m28s
2023-03-31 13:07:03.694887: Epoch: 1, Batch: 26, Loss: 0.6729, Elapsed: 0m30s
2023-03-31 13:07:40.971101: Epoch: 1, Batch: 27, Loss: 0.7092, Elapsed: 0m37s
2023-03-31 13:08:01.858305: Epoch: 1, Batch: 28, Loss: 0.6952, Elapsed: 0m20s
2023-03-31 13:08:25.008138: Epoch: 1, Batch: 29, Loss: 0.6735, Elapsed: 0m23s
2023-03-31 13:08:52.284005: Epoch: 1, Batch: 30, Loss: 0.6701, Elapsed: 0m27s
2023-03-31 13:09:34.561404: Epoch: 1, Batch: 31, Loss: 0.7326, Elapsed: 0m42s
2023-03-31 13:10:12.181616: Epoch: 1, Batch: 32, Loss: 0.7233, Elapsed: 0m37s
2023-03-31 13:10:46.001195: Epoch: 1, Batch: 33, Loss: 0.7014, Elapsed: 0m33s
2023-03-31 13:11:12.678401: Epoch: 1, Batch: 34, Loss: 0.6802, Elapsed: 0m26s
2023-03-31 13:11:45.804286: Epoch: 1, Batch: 35, Loss: 0.7182, Elapsed: 0m33s
2023-03-31 13:12:04.351886: Epoch: 1, Batch: 36, Loss: 0.6654, Elapsed: 0m18s
2023-03-31 13:12:51.944129: Epoch: 1, Batch: 37, Loss: 0.6897, Elapsed: 0m47s
2023-03-31 13:13:27.196083: Epoch: 1, Batch: 38, Loss: 0.7238, Elapsed: 0m35s
2023-03-31 13:13:42.002434: Epoch: 1, Batch: 39, Loss: 0.6858, Elapsed: 0m14s
2023-03-31 13:14:05.964397: Epoch: 1, Batch: 40, Loss: 0.6897, Elapsed: 0m23s
2023-03-31 13:14:51.143438: Epoch: 1, Batch: 41, Loss: 0.6930, Elapsed: 0m45s
2023-03-31 13:15:20.906891: Epoch: 1, Batch: 42, Loss: 0.7008, Elapsed: 0m29s
2023-03-31 13:15:49.058182: Epoch: 1, Batch: 43, Loss: 0.6878, Elapsed: 0m28s
2023-03-31 13:16:16.262567: Epoch: 1, Batch: 44, Loss: 0.7169, Elapsed: 0m27s
2023-03-31 13:16:37.689457: Epoch: 1, Batch: 45, Loss: 0.7077, Elapsed: 0m21s
2023-03-31 13:17:17.249299: Epoch: 1, Batch: 46, Loss: 0.7025, Elapsed: 0m39s
2023-03-31 13:17:40.876292: Epoch: 1, Batch: 47, Loss: 0.6774, Elapsed: 0m23s
2023-03-31 13:18:08.825885: Epoch: 1, Batch: 48, Loss: 0.7107, Elapsed: 0m27s
2023-03-31 13:18:29.611550: Epoch: 1, Batch: 49, Loss: 0.7002, Elapsed: 0m20s
2023-03-31 13:18:51.429874: Epoch: 1, Batch: 50, Loss: 0.7251, Elapsed: 0m21s
2023-03-31 13:18:51.441771 Starting testing the valid set with 20 subgraphs!
2023-03-31 13:23:48.264479: validation Test:  Loss: 0.6935,  AUC: 0.5364, Acc: 55.1402,  Precision: 0.5463 -- Elapsed: 4m56s
2023-03-31 13:23:48.265916 Starting testing the train set with 20 subgraphs!
2023-03-31 13:43:12.525348: training Test:  Loss: 0.6942,  AUC: 0.5308, Acc: 52.0685,  Precision: 0.5258 -- Elapsed: 19m24s
2023-03-31 13:43:46.531247: Epoch: 1, Batch: 51, Loss: 0.7041, Elapsed: 0m34s
2023-03-31 13:44:14.921983: Epoch: 1, Batch: 52, Loss: 0.6817, Elapsed: 0m28s
2023-03-31 13:44:52.083603: Epoch: 1, Batch: 53, Loss: 0.7005, Elapsed: 0m37s
2023-03-31 13:45:23.507837: Epoch: 1, Batch: 54, Loss: 0.6885, Elapsed: 0m31s
2023-03-31 13:45:59.469317: Epoch: 1, Batch: 55, Loss: 0.7252, Elapsed: 0m35s
2023-03-31 13:46:30.015945: Epoch: 1, Batch: 56, Loss: 0.6992, Elapsed: 0m30s
2023-03-31 13:46:55.435219: Epoch: 1, Batch: 57, Loss: 0.6795, Elapsed: 0m25s
2023-03-31 13:47:18.211773: Epoch: 1, Batch: 58, Loss: 0.6906, Elapsed: 0m22s
2023-03-31 13:47:48.432206: Epoch: 1, Batch: 59, Loss: 0.7474, Elapsed: 0m30s
2023-03-31 13:48:24.646463: Epoch: 1, Batch: 60, Loss: 0.7023, Elapsed: 0m36s
2023-03-31 13:49:19.625017: Epoch: 1, Batch: 61, Loss: 0.6802, Elapsed: 0m54s
2023-03-31 13:49:50.128228: Epoch: 1, Batch: 62, Loss: 0.6988, Elapsed: 0m30s
2023-03-31 13:50:27.312925: Epoch: 1, Batch: 63, Loss: 0.7011, Elapsed: 0m37s
2023-03-31 13:50:57.560398: Epoch: 1, Batch: 64, Loss: 0.6813, Elapsed: 0m30s
2023-03-31 13:51:26.810353: Epoch: 1, Batch: 65, Loss: 0.6849, Elapsed: 0m29s
2023-03-31 13:51:51.814062: Epoch: 1, Batch: 66, Loss: 0.6746, Elapsed: 0m24s
2023-03-31 13:52:25.285248: Epoch: 1, Batch: 67, Loss: 0.6594, Elapsed: 0m33s
2023-03-31 13:53:10.838797: Epoch: 1, Batch: 68, Loss: 0.6722, Elapsed: 0m45s
2023-03-31 13:53:38.991399: Epoch: 1, Batch: 69, Loss: 0.7006, Elapsed: 0m28s
2023-03-31 13:54:04.781416: Epoch: 1, Batch: 70, Loss: 0.7057, Elapsed: 0m25s
2023-03-31 13:54:30.144343: Epoch: 1, Batch: 71, Loss: 0.6741, Elapsed: 0m25s
2023-03-31 13:55:08.758061: Epoch: 1, Batch: 72, Loss: 0.7331, Elapsed: 0m38s
2023-03-31 13:55:29.134603: Epoch: 1, Batch: 73, Loss: 0.6531, Elapsed: 0m20s
2023-03-31 13:55:51.274540: Epoch: 1, Batch: 74, Loss: 0.7201, Elapsed: 0m22s
2023-03-31 13:56:18.632792: Epoch: 1, Batch: 75, Loss: 0.6787, Elapsed: 0m27s
2023-03-31 13:57:04.503044: Epoch: 1, Batch: 76, Loss: 0.7141, Elapsed: 0m45s
2023-03-31 13:57:37.272733: Epoch: 1, Batch: 77, Loss: 0.6725, Elapsed: 0m32s
2023-03-31 13:58:01.179321: Epoch: 1, Batch: 78, Loss: 0.6742, Elapsed: 0m23s
2023-03-31 13:58:39.592448: Epoch: 1, Batch: 79, Loss: 0.6651, Elapsed: 0m38s
2023-03-31 13:59:03.845892: Epoch: 1, Batch: 80, Loss: 0.6819, Elapsed: 0m24s
2023-03-31 13:59:44.251305: Epoch: 2, Batch: 1, Loss: 0.6576, Elapsed: 0m40s
2023-03-31 14:00:28.843561: Epoch: 2, Batch: 2, Loss: 0.6820, Elapsed: 0m44s
2023-03-31 14:01:04.345402: Epoch: 2, Batch: 3, Loss: 0.6679, Elapsed: 0m35s
2023-03-31 14:01:32.126959: Epoch: 2, Batch: 4, Loss: 0.6797, Elapsed: 0m27s
2023-03-31 14:02:01.195542: Epoch: 2, Batch: 5, Loss: 0.6496, Elapsed: 0m29s
2023-03-31 14:02:38.915399: Epoch: 2, Batch: 6, Loss: 0.6846, Elapsed: 0m37s
2023-03-31 14:03:15.866089: Epoch: 2, Batch: 7, Loss: 0.6796, Elapsed: 0m36s
2023-03-31 14:03:44.057987: Epoch: 2, Batch: 8, Loss: 0.6677, Elapsed: 0m28s
2023-03-31 14:04:20.178039: Epoch: 2, Batch: 9, Loss: 0.6723, Elapsed: 0m36s
2023-03-31 14:04:46.043721: Epoch: 2, Batch: 10, Loss: 0.6859, Elapsed: 0m25s
2023-03-31 14:05:09.738617: Epoch: 2, Batch: 11, Loss: 0.6509, Elapsed: 0m23s
2023-03-31 14:05:38.634405: Epoch: 2, Batch: 12, Loss: 0.6540, Elapsed: 0m28s
2023-03-31 14:06:09.157980: Epoch: 2, Batch: 13, Loss: 0.6638, Elapsed: 0m30s
2023-03-31 14:06:48.420571: Epoch: 2, Batch: 14, Loss: 0.6386, Elapsed: 0m39s
2023-03-31 14:07:14.681442: Epoch: 2, Batch: 15, Loss: 0.6663, Elapsed: 0m26s
2023-03-31 14:07:50.761087: Epoch: 2, Batch: 16, Loss: 0.6541, Elapsed: 0m36s
2023-03-31 14:08:13.832663: Epoch: 2, Batch: 17, Loss: 0.6265, Elapsed: 0m23s
2023-03-31 14:08:44.066909: Epoch: 2, Batch: 18, Loss: 0.6609, Elapsed: 0m30s
2023-03-31 14:09:25.436979: Epoch: 2, Batch: 19, Loss: 0.6587, Elapsed: 0m41s
2023-03-31 14:09:48.105882: Epoch: 2, Batch: 20, Loss: 0.6367, Elapsed: 0m22s
2023-03-31 14:10:35.072129: Epoch: 2, Batch: 21, Loss: 0.6796, Elapsed: 0m46s
2023-03-31 14:11:00.947814: Epoch: 2, Batch: 22, Loss: 0.6621, Elapsed: 0m25s
2023-03-31 14:11:28.016230: Epoch: 2, Batch: 23, Loss: 0.6661, Elapsed: 0m27s
2023-03-31 14:12:06.106361: Epoch: 2, Batch: 24, Loss: 0.6530, Elapsed: 0m38s
2023-03-31 14:12:38.853857: Epoch: 2, Batch: 25, Loss: 0.6513, Elapsed: 0m32s
2023-03-31 14:12:57.946954: Epoch: 2, Batch: 26, Loss: 0.6548, Elapsed: 0m19s
2023-03-31 14:13:24.632924: Epoch: 2, Batch: 27, Loss: 0.6330, Elapsed: 0m26s
2023-03-31 14:14:14.806926: Epoch: 2, Batch: 28, Loss: 0.6501, Elapsed: 0m50s
2023-03-31 14:14:45.357854: Epoch: 2, Batch: 29, Loss: 0.6117, Elapsed: 0m30s
2023-03-31 14:15:11.309077: Epoch: 2, Batch: 30, Loss: 0.6212, Elapsed: 0m25s
2023-03-31 14:15:49.418603: Epoch: 2, Batch: 31, Loss: 0.6406, Elapsed: 0m38s
2023-03-31 14:16:22.727418: Epoch: 2, Batch: 32, Loss: 0.6404, Elapsed: 0m33s
2023-03-31 14:16:48.349359: Epoch: 2, Batch: 33, Loss: 0.6559, Elapsed: 0m25s
2023-03-31 14:17:10.783153: Epoch: 2, Batch: 34, Loss: 0.6153, Elapsed: 0m22s
2023-03-31 14:17:31.276516: Epoch: 2, Batch: 35, Loss: 0.5980, Elapsed: 0m20s
2023-03-31 14:17:55.714365: Epoch: 2, Batch: 36, Loss: 0.6072, Elapsed: 0m24s
2023-03-31 14:18:23.126121: Epoch: 2, Batch: 37, Loss: 0.5758, Elapsed: 0m27s
2023-03-31 14:18:45.165607: Epoch: 2, Batch: 38, Loss: 0.5823, Elapsed: 0m22s
2023-03-31 14:19:19.961487: Epoch: 2, Batch: 39, Loss: 0.5786, Elapsed: 0m34s
2023-03-31 14:19:51.042449: Epoch: 2, Batch: 40, Loss: 0.5781, Elapsed: 0m31s
2023-03-31 14:20:09.296107: Epoch: 2, Batch: 41, Loss: 0.5437, Elapsed: 0m18s
2023-03-31 14:20:36.560711: Epoch: 2, Batch: 42, Loss: 0.5872, Elapsed: 0m27s
2023-03-31 14:20:56.369699: Epoch: 2, Batch: 43, Loss: 0.5789, Elapsed: 0m19s
2023-03-31 14:21:26.531441: Epoch: 2, Batch: 44, Loss: 0.5875, Elapsed: 0m30s
2023-03-31 14:21:47.607388: Epoch: 2, Batch: 45, Loss: 0.5779, Elapsed: 0m21s
2023-03-31 14:22:29.393742: Epoch: 2, Batch: 46, Loss: 0.5813, Elapsed: 0m41s
2023-03-31 14:22:54.858324: Epoch: 2, Batch: 47, Loss: 0.5508, Elapsed: 0m25s
2023-03-31 14:23:25.632114: Epoch: 2, Batch: 48, Loss: 0.5523, Elapsed: 0m30s
2023-03-31 14:23:58.628922: Epoch: 2, Batch: 49, Loss: 0.5586, Elapsed: 0m32s
2023-03-31 14:24:41.112032: Epoch: 2, Batch: 50, Loss: 0.5984, Elapsed: 0m42s
2023-03-31 14:24:41.126684 Starting testing the valid set with 20 subgraphs!
2023-03-31 14:29:29.892634: validation Test:  Loss: 0.5603,  AUC: 0.8109, Acc: 74.1351,  Precision: 0.9203 -- Elapsed: 4m48s
2023-03-31 14:29:29.893832 Starting testing the train set with 20 subgraphs!
2023-03-31 14:48:56.623257: training Test:  Loss: 0.5669,  AUC: 0.8032, Acc: 73.6534,  Precision: 0.9322 -- Elapsed: 19m26s
2023-03-31 14:49:44.270651: Epoch: 2, Batch: 51, Loss: 0.5299, Elapsed: 0m47s
2023-03-31 14:50:23.634782: Epoch: 2, Batch: 52, Loss: 0.5814, Elapsed: 0m39s
2023-03-31 14:50:46.406766: Epoch: 2, Batch: 53, Loss: 0.6134, Elapsed: 0m22s
2023-03-31 14:51:15.130541: Epoch: 2, Batch: 54, Loss: 0.5615, Elapsed: 0m28s
2023-03-31 14:51:54.789012: Epoch: 2, Batch: 55, Loss: 0.5346, Elapsed: 0m39s
2023-03-31 14:52:33.829501: Epoch: 2, Batch: 56, Loss: 0.5223, Elapsed: 0m39s
2023-03-31 14:53:07.101044: Epoch: 2, Batch: 57, Loss: 0.5303, Elapsed: 0m33s
2023-03-31 14:53:36.207005: Epoch: 2, Batch: 58, Loss: 0.5138, Elapsed: 0m29s
2023-03-31 14:54:04.285535: Epoch: 2, Batch: 59, Loss: 0.5256, Elapsed: 0m28s
2023-03-31 14:54:33.639349: Epoch: 2, Batch: 60, Loss: 0.5344, Elapsed: 0m29s
2023-03-31 14:54:56.734645: Epoch: 2, Batch: 61, Loss: 0.5069, Elapsed: 0m23s
2023-03-31 14:55:23.477800: Epoch: 2, Batch: 62, Loss: 0.5211, Elapsed: 0m26s
2023-03-31 14:55:53.870172: Epoch: 2, Batch: 63, Loss: 0.5530, Elapsed: 0m30s
2023-03-31 14:56:26.564854: Epoch: 2, Batch: 64, Loss: 0.5284, Elapsed: 0m32s
2023-03-31 14:56:56.865134: Epoch: 2, Batch: 65, Loss: 0.5368, Elapsed: 0m30s
2023-03-31 14:57:23.711706: Epoch: 2, Batch: 66, Loss: 0.5041, Elapsed: 0m26s
2023-03-31 14:57:44.276983: Epoch: 2, Batch: 67, Loss: 0.4983, Elapsed: 0m20s
2023-03-31 14:58:16.789014: Epoch: 2, Batch: 68, Loss: 0.5183, Elapsed: 0m32s
2023-03-31 14:58:45.179883: Epoch: 2, Batch: 69, Loss: 0.5175, Elapsed: 0m28s
2023-03-31 14:59:23.349542: Epoch: 2, Batch: 70, Loss: 0.5067, Elapsed: 0m38s
2023-03-31 14:59:52.944152: Epoch: 2, Batch: 71, Loss: 0.5283, Elapsed: 0m29s
2023-03-31 15:00:25.878156: Epoch: 2, Batch: 72, Loss: 0.5050, Elapsed: 0m32s
2023-03-31 15:00:40.500433: Epoch: 2, Batch: 73, Loss: 0.4652, Elapsed: 0m14s
2023-03-31 15:01:05.864575: Epoch: 2, Batch: 74, Loss: 0.4926, Elapsed: 0m25s
2023-03-31 15:01:36.012396: Epoch: 2, Batch: 75, Loss: 0.5314, Elapsed: 0m30s
2023-03-31 15:02:01.039842: Epoch: 2, Batch: 76, Loss: 0.4908, Elapsed: 0m25s
2023-03-31 15:02:25.633044: Epoch: 2, Batch: 77, Loss: 0.4953, Elapsed: 0m24s
2023-03-31 15:02:54.644188: Epoch: 2, Batch: 78, Loss: 0.4953, Elapsed: 0m28s
2023-03-31 15:03:16.303112: Epoch: 2, Batch: 79, Loss: 0.5224, Elapsed: 0m21s
2023-03-31 15:03:42.636460: Epoch: 2, Batch: 80, Loss: 0.4773, Elapsed: 0m26s
2023-03-31 15:04:19.131204: Epoch: 3, Batch: 1, Loss: 0.5022, Elapsed: 0m36s
2023-03-31 15:04:51.333647: Epoch: 3, Batch: 2, Loss: 0.4892, Elapsed: 0m32s
2023-03-31 15:05:18.127036: Epoch: 3, Batch: 3, Loss: 0.5036, Elapsed: 0m26s
2023-03-31 15:05:59.440957: Epoch: 3, Batch: 4, Loss: 0.5047, Elapsed: 0m41s
2023-03-31 15:06:48.874775: Epoch: 3, Batch: 5, Loss: 0.5181, Elapsed: 0m49s
2023-03-31 15:07:13.933308: Epoch: 3, Batch: 6, Loss: 0.5208, Elapsed: 0m25s
2023-03-31 15:07:38.332631: Epoch: 3, Batch: 7, Loss: 0.4685, Elapsed: 0m24s
2023-03-31 15:07:55.788933: Epoch: 3, Batch: 8, Loss: 0.5009, Elapsed: 0m17s
2023-03-31 15:08:15.865871: Epoch: 3, Batch: 9, Loss: 0.4768, Elapsed: 0m20s
2023-03-31 15:08:41.493790: Epoch: 3, Batch: 10, Loss: 0.4777, Elapsed: 0m25s
2023-03-31 15:09:15.618333: Epoch: 3, Batch: 11, Loss: 0.5394, Elapsed: 0m34s
2023-03-31 15:09:46.510044: Epoch: 3, Batch: 12, Loss: 0.4902, Elapsed: 0m30s
2023-03-31 15:10:22.392891: Epoch: 3, Batch: 13, Loss: 0.4968, Elapsed: 0m35s
2023-03-31 15:10:42.502109: Epoch: 3, Batch: 14, Loss: 0.4582, Elapsed: 0m20s
2023-03-31 15:11:15.486226: Epoch: 3, Batch: 15, Loss: 0.4847, Elapsed: 0m32s
2023-03-31 15:11:43.691149: Epoch: 3, Batch: 16, Loss: 0.4729, Elapsed: 0m28s
2023-03-31 15:12:17.000982: Epoch: 3, Batch: 17, Loss: 0.5144, Elapsed: 0m33s
2023-03-31 15:12:45.165300: Epoch: 3, Batch: 18, Loss: 0.5284, Elapsed: 0m28s
2023-03-31 15:13:22.024951: Epoch: 3, Batch: 19, Loss: 0.5031, Elapsed: 0m36s
2023-03-31 15:13:47.213827: Epoch: 3, Batch: 20, Loss: 0.4765, Elapsed: 0m25s
2023-03-31 15:14:17.645533: Epoch: 3, Batch: 21, Loss: 0.4798, Elapsed: 0m30s
2023-03-31 15:14:48.072137: Epoch: 3, Batch: 22, Loss: 0.4835, Elapsed: 0m30s
2023-03-31 15:15:13.475797: Epoch: 3, Batch: 23, Loss: 0.5004, Elapsed: 0m25s
2023-03-31 15:15:39.763376: Epoch: 3, Batch: 24, Loss: 0.5140, Elapsed: 0m26s
2023-03-31 15:16:18.323331: Epoch: 3, Batch: 25, Loss: 0.5218, Elapsed: 0m38s
2023-03-31 15:16:51.516820: Epoch: 3, Batch: 26, Loss: 0.4969, Elapsed: 0m33s
2023-03-31 15:17:18.643635: Epoch: 3, Batch: 27, Loss: 0.5461, Elapsed: 0m27s
2023-03-31 15:17:48.512613: Epoch: 3, Batch: 28, Loss: 0.4942, Elapsed: 0m29s
2023-03-31 15:18:15.389781: Epoch: 3, Batch: 29, Loss: 0.4902, Elapsed: 0m26s
2023-03-31 15:18:37.341186: Epoch: 3, Batch: 30, Loss: 0.4868, Elapsed: 0m21s
2023-03-31 15:19:05.490553: Epoch: 3, Batch: 31, Loss: 0.5109, Elapsed: 0m28s
2023-03-31 15:19:30.334317: Epoch: 3, Batch: 32, Loss: 0.5116, Elapsed: 0m24s
2023-03-31 15:20:09.318685: Epoch: 3, Batch: 33, Loss: 0.5103, Elapsed: 0m38s
2023-03-31 15:20:53.826504: Epoch: 3, Batch: 34, Loss: 0.5232, Elapsed: 0m44s
2023-03-31 15:21:31.982441: Epoch: 3, Batch: 35, Loss: 0.4987, Elapsed: 0m38s
2023-03-31 15:22:07.692441: Epoch: 3, Batch: 36, Loss: 0.4977, Elapsed: 0m35s
2023-03-31 15:22:31.813040: Epoch: 3, Batch: 37, Loss: 0.4811, Elapsed: 0m24s
2023-03-31 15:22:55.991429: Epoch: 3, Batch: 38, Loss: 0.4623, Elapsed: 0m24s
2023-03-31 15:23:26.102437: Epoch: 3, Batch: 39, Loss: 0.4868, Elapsed: 0m30s
2023-03-31 15:23:53.238665: Epoch: 3, Batch: 40, Loss: 0.4659, Elapsed: 0m27s
2023-03-31 15:24:35.035579: Epoch: 3, Batch: 41, Loss: 0.5088, Elapsed: 0m41s
2023-03-31 15:25:23.047115: Epoch: 3, Batch: 42, Loss: 0.5097, Elapsed: 0m47s
2023-03-31 15:25:53.520139: Epoch: 3, Batch: 43, Loss: 0.4974, Elapsed: 0m30s
2023-03-31 15:26:27.149450: Epoch: 3, Batch: 44, Loss: 0.4973, Elapsed: 0m33s
2023-03-31 15:26:55.772718: Epoch: 3, Batch: 45, Loss: 0.4757, Elapsed: 0m28s
2023-03-31 15:27:34.142735: Epoch: 3, Batch: 46, Loss: 0.4978, Elapsed: 0m38s
2023-03-31 15:28:00.656605: Epoch: 3, Batch: 47, Loss: 0.4628, Elapsed: 0m26s
2023-03-31 15:28:40.716151: Epoch: 3, Batch: 48, Loss: 0.4935, Elapsed: 0m40s
2023-03-31 15:29:10.792504: Epoch: 3, Batch: 49, Loss: 0.4685, Elapsed: 0m30s
2023-03-31 15:29:59.097308: Epoch: 3, Batch: 50, Loss: 0.4983, Elapsed: 0m48s
2023-03-31 15:29:59.111467 Starting testing the valid set with 20 subgraphs!
2023-03-31 15:34:53.046228: validation Test:  Loss: 0.4883,  AUC: 0.8270, Acc: 75.1680,  Precision: 0.8853 -- Elapsed: 4m53s
2023-03-31 15:34:53.047535 Starting testing the train set with 20 subgraphs!
2023-03-31 15:54:26.950901: training Test:  Loss: 0.4932,  AUC: 0.8242, Acc: 74.9110,  Precision: 0.8952 -- Elapsed: 19m33s
2023-03-31 15:54:55.370287: Epoch: 3, Batch: 51, Loss: 0.4805, Elapsed: 0m28s
2023-03-31 15:55:24.780076: Epoch: 3, Batch: 52, Loss: 0.4858, Elapsed: 0m29s
2023-03-31 15:56:04.029563: Epoch: 3, Batch: 53, Loss: 0.4980, Elapsed: 0m39s
2023-03-31 15:56:26.348378: Epoch: 3, Batch: 54, Loss: 0.4775, Elapsed: 0m22s
2023-03-31 15:56:56.867556: Epoch: 3, Batch: 55, Loss: 0.5038, Elapsed: 0m30s
2023-03-31 15:57:22.168042: Epoch: 3, Batch: 56, Loss: 0.4628, Elapsed: 0m24s
2023-03-31 15:57:49.199084: Epoch: 3, Batch: 57, Loss: 0.5040, Elapsed: 0m27s
2023-03-31 15:58:19.954134: Epoch: 3, Batch: 58, Loss: 0.4668, Elapsed: 0m30s
2023-03-31 15:58:50.596315: Epoch: 3, Batch: 59, Loss: 0.4874, Elapsed: 0m30s
2023-03-31 15:59:14.192856: Epoch: 3, Batch: 60, Loss: 0.4688, Elapsed: 0m23s
2023-03-31 15:59:46.759469: Epoch: 3, Batch: 61, Loss: 0.4987, Elapsed: 0m32s
2023-03-31 16:00:17.583960: Epoch: 3, Batch: 62, Loss: 0.4784, Elapsed: 0m30s
2023-03-31 16:00:43.727233: Epoch: 3, Batch: 63, Loss: 0.4977, Elapsed: 0m26s
2023-03-31 16:01:25.080995: Epoch: 3, Batch: 64, Loss: 0.5077, Elapsed: 0m41s
2023-03-31 16:01:55.163512: Epoch: 3, Batch: 65, Loss: 0.4968, Elapsed: 0m30s
2023-03-31 16:02:21.672907: Epoch: 3, Batch: 66, Loss: 0.4912, Elapsed: 0m26s
2023-03-31 16:02:43.964464: Epoch: 3, Batch: 67, Loss: 0.4886, Elapsed: 0m22s
2023-03-31 16:03:29.772675: Epoch: 3, Batch: 68, Loss: 0.4838, Elapsed: 0m45s
2023-03-31 16:04:00.206576: Epoch: 3, Batch: 69, Loss: 0.4857, Elapsed: 0m30s
2023-03-31 16:04:19.537344: Epoch: 3, Batch: 70, Loss: 0.4560, Elapsed: 0m19s
2023-03-31 16:04:42.536717: Epoch: 3, Batch: 71, Loss: 0.4652, Elapsed: 0m22s
2023-03-31 16:04:57.434845: Epoch: 3, Batch: 72, Loss: 0.4631, Elapsed: 0m14s
2023-03-31 16:05:17.533026: Epoch: 3, Batch: 73, Loss: 0.4808, Elapsed: 0m20s
2023-03-31 16:06:07.810540: Epoch: 3, Batch: 74, Loss: 0.5066, Elapsed: 0m50s
2023-03-31 16:06:45.667588: Epoch: 3, Batch: 75, Loss: 0.4891, Elapsed: 0m37s
2023-03-31 16:07:21.135084: Epoch: 3, Batch: 76, Loss: 0.5044, Elapsed: 0m35s
2023-03-31 16:07:50.595321: Epoch: 3, Batch: 77, Loss: 0.5007, Elapsed: 0m29s
2023-03-31 16:08:13.745791: Epoch: 3, Batch: 78, Loss: 0.4741, Elapsed: 0m23s
2023-03-31 16:08:35.263224: Epoch: 3, Batch: 79, Loss: 0.4731, Elapsed: 0m21s
2023-03-31 16:09:04.950569: Epoch: 3, Batch: 80, Loss: 0.4608, Elapsed: 0m29s
2023-03-31 16:09:29.239132: Epoch: 4, Batch: 1, Loss: 0.4565, Elapsed: 0m24s
2023-03-31 16:09:44.100502: Epoch: 4, Batch: 2, Loss: 0.4688, Elapsed: 0m14s
2023-03-31 16:10:06.487016: Epoch: 4, Batch: 3, Loss: 0.4810, Elapsed: 0m22s
2023-03-31 16:10:32.517647: Epoch: 4, Batch: 4, Loss: 0.4633, Elapsed: 0m26s
2023-03-31 16:10:58.184048: Epoch: 4, Batch: 5, Loss: 0.4383, Elapsed: 0m25s
2023-03-31 16:11:27.886520: Epoch: 4, Batch: 6, Loss: 0.4599, Elapsed: 0m29s
2023-03-31 16:12:00.837515: Epoch: 4, Batch: 7, Loss: 0.4799, Elapsed: 0m32s
2023-03-31 16:12:22.570819: Epoch: 4, Batch: 8, Loss: 0.4908, Elapsed: 0m21s
2023-03-31 16:13:00.193784: Epoch: 4, Batch: 9, Loss: 0.4827, Elapsed: 0m37s
2023-03-31 16:13:30.567196: Epoch: 4, Batch: 10, Loss: 0.4851, Elapsed: 0m30s
2023-03-31 16:13:57.079426: Epoch: 4, Batch: 11, Loss: 0.4793, Elapsed: 0m26s
2023-03-31 16:14:27.619204: Epoch: 4, Batch: 12, Loss: 0.4782, Elapsed: 0m30s
2023-03-31 16:14:54.548388: Epoch: 4, Batch: 13, Loss: 0.4638, Elapsed: 0m26s
2023-03-31 16:15:29.022530: Epoch: 4, Batch: 14, Loss: 0.4773, Elapsed: 0m34s
2023-03-31 16:15:48.367014: Epoch: 4, Batch: 15, Loss: 0.4354, Elapsed: 0m19s
2023-03-31 16:16:16.164594: Epoch: 4, Batch: 16, Loss: 0.4667, Elapsed: 0m27s
2023-03-31 16:16:56.254961: Epoch: 4, Batch: 17, Loss: 0.4824, Elapsed: 0m40s
2023-03-31 16:17:19.579202: Epoch: 4, Batch: 18, Loss: 0.4638, Elapsed: 0m23s
2023-03-31 16:17:47.235291: Epoch: 4, Batch: 19, Loss: 0.4793, Elapsed: 0m27s
2023-03-31 16:18:14.221989: Epoch: 4, Batch: 20, Loss: 0.4544, Elapsed: 0m26s
2023-03-31 16:18:49.571013: Epoch: 4, Batch: 21, Loss: 0.4803, Elapsed: 0m35s
2023-03-31 16:19:22.155656: Epoch: 4, Batch: 22, Loss: 0.4848, Elapsed: 0m32s
2023-03-31 16:20:04.305406: Epoch: 4, Batch: 23, Loss: 0.4919, Elapsed: 0m42s
2023-03-31 16:20:30.861804: Epoch: 4, Batch: 24, Loss: 0.4689, Elapsed: 0m26s
2023-03-31 16:21:00.967466: Epoch: 4, Batch: 25, Loss: 0.4873, Elapsed: 0m30s
2023-03-31 16:21:46.434222: Epoch: 4, Batch: 26, Loss: 0.4969, Elapsed: 0m45s
2023-03-31 16:22:16.315866: Epoch: 4, Batch: 27, Loss: 0.4849, Elapsed: 0m29s
2023-03-31 16:22:44.614318: Epoch: 4, Batch: 28, Loss: 0.4691, Elapsed: 0m28s
2023-03-31 16:23:14.456283: Epoch: 4, Batch: 29, Loss: 0.4946, Elapsed: 0m29s
2023-03-31 16:23:46.596499: Epoch: 4, Batch: 30, Loss: 0.4804, Elapsed: 0m32s
2023-03-31 16:24:05.773714: Epoch: 4, Batch: 31, Loss: 0.4502, Elapsed: 0m19s
2023-03-31 16:24:31.283571: Epoch: 4, Batch: 32, Loss: 0.4607, Elapsed: 0m25s
2023-03-31 16:24:55.148849: Epoch: 4, Batch: 33, Loss: 0.4495, Elapsed: 0m23s
2023-03-31 16:25:24.706760: Epoch: 4, Batch: 34, Loss: 0.4909, Elapsed: 0m29s
2023-03-31 16:25:50.852047: Epoch: 4, Batch: 35, Loss: 0.4765, Elapsed: 0m26s
2023-03-31 16:26:25.968620: Epoch: 4, Batch: 36, Loss: 0.4892, Elapsed: 0m35s
2023-03-31 16:27:11.352965: Epoch: 4, Batch: 37, Loss: 0.5139, Elapsed: 0m45s
2023-03-31 16:27:47.032115: Epoch: 4, Batch: 38, Loss: 0.4830, Elapsed: 0m35s
2023-03-31 16:28:24.595918: Epoch: 4, Batch: 39, Loss: 0.4943, Elapsed: 0m37s
2023-03-31 16:28:51.282709: Epoch: 4, Batch: 40, Loss: 0.4940, Elapsed: 0m26s
2023-03-31 16:29:18.496298: Epoch: 4, Batch: 41, Loss: 0.5039, Elapsed: 0m27s
2023-03-31 16:29:48.629757: Epoch: 4, Batch: 42, Loss: 0.4909, Elapsed: 0m30s
2023-03-31 16:30:18.818885: Epoch: 4, Batch: 43, Loss: 0.4641, Elapsed: 0m30s
2023-03-31 16:30:46.195588: Epoch: 4, Batch: 44, Loss: 0.4670, Elapsed: 0m27s
2023-03-31 16:31:25.772368: Epoch: 4, Batch: 45, Loss: 0.4752, Elapsed: 0m39s
2023-03-31 16:32:03.625125: Epoch: 4, Batch: 46, Loss: 0.4845, Elapsed: 0m37s
2023-03-31 16:32:33.891173: Epoch: 4, Batch: 47, Loss: 0.4641, Elapsed: 0m30s
2023-03-31 16:33:07.750153: Epoch: 4, Batch: 48, Loss: 0.4772, Elapsed: 0m33s
2023-03-31 16:33:29.517460: Epoch: 4, Batch: 49, Loss: 0.4637, Elapsed: 0m21s
2023-03-31 16:33:59.676572: Epoch: 4, Batch: 50, Loss: 0.4714, Elapsed: 0m30s
2023-03-31 16:33:59.691538 Starting testing the valid set with 20 subgraphs!
2023-03-31 16:38:52.747037: validation Test:  Loss: 0.4848,  AUC: 0.8296, Acc: 74.4886,  Precision: 0.7632 -- Elapsed: 4m53s
2023-03-31 16:38:52.748241 Starting testing the train set with 20 subgraphs!
2023-03-31 16:58:11.881052: training Test:  Loss: 0.4865,  AUC: 0.8277, Acc: 74.3192,  Precision: 0.7595 -- Elapsed: 19m19s
2023-03-31 16:58:45.530898: Epoch: 4, Batch: 51, Loss: 0.4842, Elapsed: 0m33s
2023-03-31 16:59:36.584689: Epoch: 4, Batch: 52, Loss: 0.4926, Elapsed: 0m51s
2023-03-31 16:59:59.729355: Epoch: 4, Batch: 53, Loss: 0.4743, Elapsed: 0m23s
2023-03-31 17:00:37.701843: Epoch: 4, Batch: 54, Loss: 0.5052, Elapsed: 0m37s
2023-03-31 17:00:55.240628: Epoch: 4, Batch: 55, Loss: 0.4557, Elapsed: 0m17s
2023-03-31 17:01:14.859588: Epoch: 4, Batch: 56, Loss: 0.4665, Elapsed: 0m19s
2023-03-31 17:01:44.464710: Epoch: 4, Batch: 57, Loss: 0.4665, Elapsed: 0m29s
2023-03-31 17:02:12.492495: Epoch: 4, Batch: 58, Loss: 0.4724, Elapsed: 0m28s
2023-03-31 17:02:34.687790: Epoch: 4, Batch: 59, Loss: 0.4659, Elapsed: 0m22s
2023-03-31 17:02:57.839813: Epoch: 4, Batch: 60, Loss: 0.4803, Elapsed: 0m23s
2023-03-31 17:03:37.395416: Epoch: 4, Batch: 61, Loss: 0.4982, Elapsed: 0m39s
2023-03-31 17:04:18.334978: Epoch: 4, Batch: 62, Loss: 0.5159, Elapsed: 0m40s
2023-03-31 17:04:54.129421: Epoch: 4, Batch: 63, Loss: 0.4945, Elapsed: 0m35s
2023-03-31 17:05:19.909163: Epoch: 4, Batch: 64, Loss: 0.4713, Elapsed: 0m25s
2023-03-31 17:05:44.097247: Epoch: 4, Batch: 65, Loss: 0.4837, Elapsed: 0m24s
2023-03-31 17:06:08.873827: Epoch: 4, Batch: 66, Loss: 0.4541, Elapsed: 0m24s
2023-03-31 17:06:36.894309: Epoch: 4, Batch: 67, Loss: 0.4657, Elapsed: 0m28s
2023-03-31 17:07:23.671862: Epoch: 4, Batch: 68, Loss: 0.4950, Elapsed: 0m46s
2023-03-31 17:07:52.784236: Epoch: 4, Batch: 69, Loss: 0.4907, Elapsed: 0m29s
2023-03-31 17:08:20.168161: Epoch: 4, Batch: 70, Loss: 0.4731, Elapsed: 0m27s
2023-03-31 17:08:52.135980: Epoch: 4, Batch: 71, Loss: 0.4871, Elapsed: 0m31s
2023-03-31 17:09:19.422642: Epoch: 4, Batch: 72, Loss: 0.4681, Elapsed: 0m27s
2023-03-31 17:09:47.304101: Epoch: 4, Batch: 73, Loss: 0.4807, Elapsed: 0m27s
2023-03-31 17:10:08.295322: Epoch: 4, Batch: 74, Loss: 0.4541, Elapsed: 0m20s
2023-03-31 17:10:47.625942: Epoch: 4, Batch: 75, Loss: 0.4801, Elapsed: 0m39s
2023-03-31 17:11:27.057189: Epoch: 4, Batch: 76, Loss: 0.5320, Elapsed: 0m39s
2023-03-31 17:12:10.606347: Epoch: 4, Batch: 77, Loss: 0.5008, Elapsed: 0m43s
2023-03-31 17:12:35.395152: Epoch: 4, Batch: 78, Loss: 0.4557, Elapsed: 0m24s
2023-03-31 17:12:54.651334: Epoch: 4, Batch: 79, Loss: 0.4442, Elapsed: 0m19s
2023-03-31 17:13:31.532208: Epoch: 4, Batch: 80, Loss: 0.4872, Elapsed: 0m36s
2023-03-31 17:13:54.741064: Epoch: 5, Batch: 1, Loss: 0.4614, Elapsed: 0m23s
2023-03-31 17:14:16.870623: Epoch: 5, Batch: 2, Loss: 0.4519, Elapsed: 0m22s
2023-03-31 17:14:48.316510: Epoch: 5, Batch: 3, Loss: 0.4757, Elapsed: 0m31s
2023-03-31 17:15:24.589558: Epoch: 5, Batch: 4, Loss: 0.4892, Elapsed: 0m36s
2023-03-31 17:16:03.719236: Epoch: 5, Batch: 5, Loss: 0.5314, Elapsed: 0m39s
2023-03-31 17:16:29.219751: Epoch: 5, Batch: 6, Loss: 0.4650, Elapsed: 0m25s
2023-03-31 17:17:02.472141: Epoch: 5, Batch: 7, Loss: 0.4919, Elapsed: 0m33s
2023-03-31 17:17:24.239587: Epoch: 5, Batch: 8, Loss: 0.4689, Elapsed: 0m21s
2023-03-31 17:17:46.944375: Epoch: 5, Batch: 9, Loss: 0.4511, Elapsed: 0m22s
2023-03-31 17:18:13.597793: Epoch: 5, Batch: 10, Loss: 0.4673, Elapsed: 0m26s
2023-03-31 17:18:36.514701: Epoch: 5, Batch: 11, Loss: 0.4532, Elapsed: 0m22s
2023-03-31 17:19:05.799163: Epoch: 5, Batch: 12, Loss: 0.4712, Elapsed: 0m29s
2023-03-31 17:19:31.646482: Epoch: 5, Batch: 13, Loss: 0.4642, Elapsed: 0m25s
2023-03-31 17:19:59.496637: Epoch: 5, Batch: 14, Loss: 0.4632, Elapsed: 0m27s
2023-03-31 17:20:29.958434: Epoch: 5, Batch: 15, Loss: 0.4660, Elapsed: 0m30s
2023-03-31 17:21:06.454210: Epoch: 5, Batch: 16, Loss: 0.5129, Elapsed: 0m36s
2023-03-31 17:21:34.678403: Epoch: 5, Batch: 17, Loss: 0.4745, Elapsed: 0m28s
2023-03-31 17:22:05.738655: Epoch: 5, Batch: 18, Loss: 0.4711, Elapsed: 0m31s
2023-03-31 17:22:26.395402: Epoch: 5, Batch: 19, Loss: 0.4562, Elapsed: 0m20s
2023-03-31 17:23:18.173193: Epoch: 5, Batch: 20, Loss: 0.4861, Elapsed: 0m51s
2023-03-31 17:24:04.051445: Epoch: 5, Batch: 21, Loss: 0.4922, Elapsed: 0m45s
2023-03-31 17:24:23.667856: Epoch: 5, Batch: 22, Loss: 0.4858, Elapsed: 0m19s
2023-03-31 17:25:00.975408: Epoch: 5, Batch: 23, Loss: 0.4766, Elapsed: 0m37s
2023-03-31 17:25:29.712955: Epoch: 5, Batch: 24, Loss: 0.4889, Elapsed: 0m28s
2023-03-31 17:26:12.955931: Epoch: 5, Batch: 25, Loss: 0.4999, Elapsed: 0m43s
2023-03-31 17:26:56.337386: Epoch: 5, Batch: 26, Loss: 0.4797, Elapsed: 0m43s
2023-03-31 17:27:36.237983: Epoch: 5, Batch: 27, Loss: 0.4771, Elapsed: 0m39s
2023-03-31 17:28:05.712362: Epoch: 5, Batch: 28, Loss: 0.4764, Elapsed: 0m29s
2023-03-31 17:28:25.945569: Epoch: 5, Batch: 29, Loss: 0.4385, Elapsed: 0m20s
2023-03-31 17:29:11.369703: Epoch: 5, Batch: 30, Loss: 0.4885, Elapsed: 0m45s
2023-03-31 17:29:31.289572: Epoch: 5, Batch: 31, Loss: 0.4491, Elapsed: 0m19s
2023-03-31 17:29:58.644079: Epoch: 5, Batch: 32, Loss: 0.4698, Elapsed: 0m27s
2023-03-31 17:30:27.709854: Epoch: 5, Batch: 33, Loss: 0.4801, Elapsed: 0m29s
2023-03-31 17:31:03.102135: Epoch: 5, Batch: 34, Loss: 0.4914, Elapsed: 0m35s
2023-03-31 17:31:36.905139: Epoch: 5, Batch: 35, Loss: 0.4751, Elapsed: 0m33s
2023-03-31 17:32:05.614879: Epoch: 5, Batch: 36, Loss: 0.4565, Elapsed: 0m28s
2023-03-31 17:32:38.933582: Epoch: 5, Batch: 37, Loss: 0.4832, Elapsed: 0m33s
2023-03-31 17:33:11.855032: Epoch: 5, Batch: 38, Loss: 0.4818, Elapsed: 0m32s
2023-03-31 17:33:33.720684: Epoch: 5, Batch: 39, Loss: 0.4577, Elapsed: 0m21s
2023-03-31 17:34:15.149523: Epoch: 5, Batch: 40, Loss: 0.4829, Elapsed: 0m41s
2023-03-31 17:34:44.881429: Epoch: 5, Batch: 41, Loss: 0.4610, Elapsed: 0m29s
2023-03-31 17:35:16.573451: Epoch: 5, Batch: 42, Loss: 0.4735, Elapsed: 0m31s
2023-03-31 17:35:42.832856: Epoch: 5, Batch: 43, Loss: 0.4786, Elapsed: 0m26s
2023-03-31 17:36:05.674446: Epoch: 5, Batch: 44, Loss: 0.4547, Elapsed: 0m22s
2023-03-31 17:36:36.429506: Epoch: 5, Batch: 45, Loss: 0.4707, Elapsed: 0m30s
2023-03-31 17:36:53.407695: Epoch: 5, Batch: 46, Loss: 0.4239, Elapsed: 0m16s
2023-03-31 17:37:23.055907: Epoch: 5, Batch: 47, Loss: 0.4601, Elapsed: 0m29s
2023-03-31 17:37:55.951633: Epoch: 5, Batch: 48, Loss: 0.4688, Elapsed: 0m32s
2023-03-31 17:38:20.835814: Epoch: 5, Batch: 49, Loss: 0.4563, Elapsed: 0m24s
2023-03-31 17:38:50.878288: Epoch: 5, Batch: 50, Loss: 0.4520, Elapsed: 0m30s
2023-03-31 17:38:50.892090 Starting testing the valid set with 20 subgraphs!
2023-03-31 17:43:42.406709: validation Test:  Loss: 0.4683,  AUC: 0.8351, Acc: 75.9291,  Precision: 0.8109 -- Elapsed: 4m51s
2023-03-31 17:43:42.408217 Starting testing the train set with 20 subgraphs!
2023-03-31 18:03:04.955466: training Test:  Loss: 0.4653,  AUC: 0.8374, Acc: 76.3040,  Precision: 0.8200 -- Elapsed: 19m22s
2023-03-31 18:03:43.922603: Epoch: 5, Batch: 51, Loss: 0.4732, Elapsed: 0m38s
2023-03-31 18:04:28.623254: Epoch: 5, Batch: 52, Loss: 0.4832, Elapsed: 0m44s
2023-03-31 18:04:52.692064: Epoch: 5, Batch: 53, Loss: 0.4478, Elapsed: 0m24s
2023-03-31 18:05:19.924045: Epoch: 5, Batch: 54, Loss: 0.4579, Elapsed: 0m27s
2023-03-31 18:05:50.255294: Epoch: 5, Batch: 55, Loss: 0.4616, Elapsed: 0m30s
2023-03-31 18:06:13.487064: Epoch: 5, Batch: 56, Loss: 0.4621, Elapsed: 0m23s
2023-03-31 18:06:39.776381: Epoch: 5, Batch: 57, Loss: 0.4634, Elapsed: 0m26s
2023-03-31 18:06:54.263042: Epoch: 5, Batch: 58, Loss: 0.4233, Elapsed: 0m14s
2023-03-31 18:07:24.706584: Epoch: 5, Batch: 59, Loss: 0.4766, Elapsed: 0m30s
2023-03-31 18:08:00.326555: Epoch: 5, Batch: 60, Loss: 0.4750, Elapsed: 0m35s
2023-03-31 18:08:38.053317: Epoch: 5, Batch: 61, Loss: 0.4813, Elapsed: 0m37s
2023-03-31 18:09:16.703094: Epoch: 5, Batch: 62, Loss: 0.4802, Elapsed: 0m38s
2023-03-31 18:09:44.377308: Epoch: 5, Batch: 63, Loss: 0.4728, Elapsed: 0m27s
2023-03-31 18:10:12.116092: Epoch: 5, Batch: 64, Loss: 0.4648, Elapsed: 0m27s
2023-03-31 18:10:45.529046: Epoch: 5, Batch: 65, Loss: 0.4667, Elapsed: 0m33s
2023-03-31 18:11:18.837069: Epoch: 5, Batch: 66, Loss: 0.4764, Elapsed: 0m33s
2023-03-31 18:11:46.475102: Epoch: 5, Batch: 67, Loss: 0.4572, Elapsed: 0m27s
2023-03-31 18:12:11.905423: Epoch: 5, Batch: 68, Loss: 0.4642, Elapsed: 0m25s
2023-03-31 18:12:42.064908: Epoch: 5, Batch: 69, Loss: 0.4812, Elapsed: 0m30s
2023-03-31 18:13:06.683709: Epoch: 5, Batch: 70, Loss: 0.4437, Elapsed: 0m24s
2023-03-31 18:13:36.124912: Epoch: 5, Batch: 71, Loss: 0.4628, Elapsed: 0m29s
2023-03-31 18:14:03.402541: Epoch: 5, Batch: 72, Loss: 0.4566, Elapsed: 0m27s
2023-03-31 18:14:36.334357: Epoch: 5, Batch: 73, Loss: 0.4732, Elapsed: 0m32s
2023-03-31 18:14:59.035119: Epoch: 5, Batch: 74, Loss: 0.4612, Elapsed: 0m22s
2023-03-31 18:15:23.183974: Epoch: 5, Batch: 75, Loss: 0.4570, Elapsed: 0m24s
2023-03-31 18:16:01.755538: Epoch: 5, Batch: 76, Loss: 0.4745, Elapsed: 0m38s
2023-03-31 18:16:39.902530: Epoch: 5, Batch: 77, Loss: 0.4705, Elapsed: 0m38s
2023-03-31 18:17:05.005206: Epoch: 5, Batch: 78, Loss: 0.4667, Elapsed: 0m25s
2023-03-31 18:17:33.353324: Epoch: 5, Batch: 79, Loss: 0.4563, Elapsed: 0m28s
2023-03-31 18:18:10.176166: Epoch: 5, Batch: 80, Loss: 0.4747, Elapsed: 0m36s
2023-03-31 18:18:38.671260: Epoch: 6, Batch: 1, Loss: 0.4591, Elapsed: 0m28s
2023-03-31 18:19:11.337482: Epoch: 6, Batch: 2, Loss: 0.4750, Elapsed: 0m32s
2023-03-31 18:19:34.003961: Epoch: 6, Batch: 3, Loss: 0.4586, Elapsed: 0m22s
2023-03-31 18:20:12.368398: Epoch: 6, Batch: 4, Loss: 0.4769, Elapsed: 0m38s
2023-03-31 18:20:45.734363: Epoch: 6, Batch: 5, Loss: 0.4714, Elapsed: 0m33s
2023-03-31 18:21:31.397345: Epoch: 6, Batch: 6, Loss: 0.5002, Elapsed: 0m45s
2023-03-31 18:21:59.695072: Epoch: 6, Batch: 7, Loss: 0.4662, Elapsed: 0m28s
2023-03-31 18:22:24.370351: Epoch: 6, Batch: 8, Loss: 0.4679, Elapsed: 0m24s
2023-03-31 18:22:58.129981: Epoch: 6, Batch: 9, Loss: 0.4751, Elapsed: 0m33s
2023-03-31 18:23:19.971361: Epoch: 6, Batch: 10, Loss: 0.4943, Elapsed: 0m21s
2023-03-31 18:23:46.791514: Epoch: 6, Batch: 11, Loss: 0.4460, Elapsed: 0m26s
2023-03-31 18:24:06.793931: Epoch: 6, Batch: 12, Loss: 0.4313, Elapsed: 0m19s
2023-03-31 18:24:35.978736: Epoch: 6, Batch: 13, Loss: 0.4557, Elapsed: 0m29s
2023-03-31 18:25:02.403422: Epoch: 6, Batch: 14, Loss: 0.4696, Elapsed: 0m26s
2023-03-31 18:25:37.265331: Epoch: 6, Batch: 15, Loss: 0.4815, Elapsed: 0m34s
2023-03-31 18:26:17.417128: Epoch: 6, Batch: 16, Loss: 0.4904, Elapsed: 0m40s
2023-03-31 18:26:52.611406: Epoch: 6, Batch: 17, Loss: 0.4963, Elapsed: 0m35s
2023-03-31 18:27:12.422984: Epoch: 6, Batch: 18, Loss: 0.4481, Elapsed: 0m19s
2023-03-31 18:27:41.975913: Epoch: 6, Batch: 19, Loss: 0.4693, Elapsed: 0m29s
2023-03-31 18:28:08.436257: Epoch: 6, Batch: 20, Loss: 0.4553, Elapsed: 0m26s
2023-03-31 18:28:50.088860: Epoch: 6, Batch: 21, Loss: 0.4807, Elapsed: 0m41s
2023-03-31 18:29:17.169245: Epoch: 6, Batch: 22, Loss: 0.4538, Elapsed: 0m27s
2023-03-31 18:29:55.041987: Epoch: 6, Batch: 23, Loss: 0.4737, Elapsed: 0m37s
2023-03-31 18:30:22.426202: Epoch: 6, Batch: 24, Loss: 0.4678, Elapsed: 0m27s
2023-03-31 18:30:51.015279: Epoch: 6, Batch: 25, Loss: 0.4798, Elapsed: 0m28s
2023-03-31 18:31:16.328854: Epoch: 6, Batch: 26, Loss: 0.4554, Elapsed: 0m25s
2023-03-31 18:31:53.768533: Epoch: 6, Batch: 27, Loss: 0.4771, Elapsed: 0m37s
2023-03-31 18:32:34.971733: Epoch: 6, Batch: 28, Loss: 0.4783, Elapsed: 0m41s
2023-03-31 18:33:09.798841: Epoch: 6, Batch: 29, Loss: 0.4727, Elapsed: 0m34s
2023-03-31 18:33:34.093792: Epoch: 6, Batch: 30, Loss: 0.4585, Elapsed: 0m24s
2023-03-31 18:34:03.062493: Epoch: 6, Batch: 31, Loss: 0.4606, Elapsed: 0m28s
2023-03-31 18:34:30.021914: Epoch: 6, Batch: 32, Loss: 0.4589, Elapsed: 0m26s
2023-03-31 18:35:05.484914: Epoch: 6, Batch: 33, Loss: 0.4804, Elapsed: 0m35s
2023-03-31 18:35:32.194465: Epoch: 6, Batch: 34, Loss: 0.4549, Elapsed: 0m26s
2023-03-31 18:36:01.808461: Epoch: 6, Batch: 35, Loss: 0.4716, Elapsed: 0m29s
2023-03-31 18:36:24.675125: Epoch: 6, Batch: 36, Loss: 0.4397, Elapsed: 0m22s
2023-03-31 18:37:04.277119: Epoch: 6, Batch: 37, Loss: 0.4730, Elapsed: 0m39s
2023-03-31 18:37:33.962011: Epoch: 6, Batch: 38, Loss: 0.4525, Elapsed: 0m29s
2023-03-31 18:38:03.379197: Epoch: 6, Batch: 39, Loss: 0.4551, Elapsed: 0m29s
2023-03-31 18:38:48.019462: Epoch: 6, Batch: 40, Loss: 0.4763, Elapsed: 0m44s
2023-03-31 18:39:28.642619: Epoch: 6, Batch: 41, Loss: 0.4668, Elapsed: 0m40s
2023-03-31 18:39:50.439339: Epoch: 6, Batch: 42, Loss: 0.4386, Elapsed: 0m21s
2023-03-31 18:40:15.964697: Epoch: 6, Batch: 43, Loss: 0.4560, Elapsed: 0m25s
2023-03-31 18:40:46.786438: Epoch: 6, Batch: 44, Loss: 0.4647, Elapsed: 0m30s
2023-03-31 18:41:15.931315: Epoch: 6, Batch: 45, Loss: 0.4759, Elapsed: 0m29s
2023-03-31 18:41:43.900166: Epoch: 6, Batch: 46, Loss: 0.4668, Elapsed: 0m27s
2023-03-31 18:42:14.924061: Epoch: 6, Batch: 47, Loss: 0.4691, Elapsed: 0m31s
2023-03-31 18:42:38.489993: Epoch: 6, Batch: 48, Loss: 0.4621, Elapsed: 0m23s
2023-03-31 18:43:08.088906: Epoch: 6, Batch: 49, Loss: 0.4813, Elapsed: 0m29s
2023-03-31 18:43:32.569122: Epoch: 6, Batch: 50, Loss: 0.4471, Elapsed: 0m24s
2023-03-31 18:43:32.582746 Starting testing the valid set with 20 subgraphs!
2023-03-31 18:48:24.952144: validation Test:  Loss: 0.4668,  AUC: 0.8381, Acc: 75.3144,  Precision: 0.7695 -- Elapsed: 4m52s
2023-03-31 18:48:24.953308 Starting testing the train set with 20 subgraphs!
2023-03-31 19:07:45.289213: training Test:  Loss: 0.4644,  AUC: 0.8383, Acc: 75.6359,  Precision: 0.7780 -- Elapsed: 19m20s
2023-03-31 19:08:11.158137: Epoch: 6, Batch: 51, Loss: 0.4360, Elapsed: 0m25s
2023-03-31 19:08:56.722188: Epoch: 6, Batch: 52, Loss: 0.4779, Elapsed: 0m45s
2023-03-31 19:09:19.675036: Epoch: 6, Batch: 53, Loss: 0.4356, Elapsed: 0m22s
2023-03-31 19:09:49.216335: Epoch: 6, Batch: 54, Loss: 0.4565, Elapsed: 0m29s
2023-03-31 19:10:15.638548: Epoch: 6, Batch: 55, Loss: 0.4562, Elapsed: 0m26s
2023-03-31 19:10:42.517359: Epoch: 6, Batch: 56, Loss: 0.4484, Elapsed: 0m26s
2023-03-31 19:11:32.849007: Epoch: 6, Batch: 57, Loss: 0.4831, Elapsed: 0m50s
2023-03-31 19:12:10.187318: Epoch: 6, Batch: 58, Loss: 0.4701, Elapsed: 0m37s
2023-03-31 19:12:27.695762: Epoch: 6, Batch: 59, Loss: 0.4250, Elapsed: 0m17s
2023-03-31 19:12:59.528223: Epoch: 6, Batch: 60, Loss: 0.4590, Elapsed: 0m31s
2023-03-31 19:13:24.196818: Epoch: 6, Batch: 61, Loss: 0.4394, Elapsed: 0m24s
2023-03-31 19:13:46.800354: Epoch: 6, Batch: 62, Loss: 0.4478, Elapsed: 0m22s
2023-03-31 19:14:12.273994: Epoch: 6, Batch: 63, Loss: 0.4502, Elapsed: 0m25s
2023-03-31 19:14:27.436464: Epoch: 6, Batch: 64, Loss: 0.4314, Elapsed: 0m15s
2023-03-31 19:14:57.414576: Epoch: 6, Batch: 65, Loss: 0.4613, Elapsed: 0m29s
2023-03-31 19:15:34.054218: Epoch: 6, Batch: 66, Loss: 0.4714, Elapsed: 0m36s
2023-03-31 19:16:02.689076: Epoch: 6, Batch: 67, Loss: 0.4532, Elapsed: 0m28s
2023-03-31 19:16:35.576443: Epoch: 6, Batch: 68, Loss: 0.4622, Elapsed: 0m32s
2023-03-31 19:17:02.008708: Epoch: 6, Batch: 69, Loss: 0.4565, Elapsed: 0m26s
2023-03-31 19:17:29.382668: Epoch: 6, Batch: 70, Loss: 0.4660, Elapsed: 0m27s
2023-03-31 19:17:51.913537: Epoch: 6, Batch: 71, Loss: 0.4464, Elapsed: 0m22s
2023-03-31 19:18:24.924720: Epoch: 6, Batch: 72, Loss: 0.4637, Elapsed: 0m32s
2023-03-31 19:19:01.061982: Epoch: 6, Batch: 73, Loss: 0.4661, Elapsed: 0m36s
2023-03-31 19:19:21.099270: Epoch: 6, Batch: 74, Loss: 0.4225, Elapsed: 0m20s
2023-03-31 19:20:00.835364: Epoch: 6, Batch: 75, Loss: 0.4672, Elapsed: 0m39s
2023-03-31 19:20:22.221646: Epoch: 6, Batch: 76, Loss: 0.4496, Elapsed: 0m21s
2023-03-31 19:21:00.633787: Epoch: 6, Batch: 77, Loss: 0.4821, Elapsed: 0m38s
2023-03-31 19:21:35.705223: Epoch: 6, Batch: 78, Loss: 0.4703, Elapsed: 0m35s
2023-03-31 19:21:55.595871: Epoch: 6, Batch: 79, Loss: 0.4217, Elapsed: 0m19s
2023-03-31 19:22:24.139523: Epoch: 6, Batch: 80, Loss: 0.4460, Elapsed: 0m28s
2023-03-31 19:23:04.390577: Epoch: 7, Batch: 1, Loss: 0.4716, Elapsed: 0m40s
2023-03-31 19:23:41.925647: Epoch: 7, Batch: 2, Loss: 0.4828, Elapsed: 0m37s
2023-03-31 19:24:02.223679: Epoch: 7, Batch: 3, Loss: 0.4212, Elapsed: 0m20s
2023-03-31 19:24:27.355805: Epoch: 7, Batch: 4, Loss: 0.4270, Elapsed: 0m25s
2023-03-31 19:24:53.257402: Epoch: 7, Batch: 5, Loss: 0.4465, Elapsed: 0m25s
2023-03-31 19:25:22.263745: Epoch: 7, Batch: 6, Loss: 0.4442, Elapsed: 0m28s
2023-03-31 19:25:51.573292: Epoch: 7, Batch: 7, Loss: 0.4495, Elapsed: 0m29s
2023-03-31 19:26:33.828209: Epoch: 7, Batch: 8, Loss: 0.4662, Elapsed: 0m42s
2023-03-31 19:27:01.055736: Epoch: 7, Batch: 9, Loss: 0.4577, Elapsed: 0m27s
2023-03-31 19:27:27.861014: Epoch: 7, Batch: 10, Loss: 0.4470, Elapsed: 0m26s
2023-03-31 19:28:02.599943: Epoch: 7, Batch: 11, Loss: 0.4697, Elapsed: 0m34s
2023-03-31 19:28:31.229079: Epoch: 7, Batch: 12, Loss: 0.4508, Elapsed: 0m28s
2023-03-31 19:29:05.942541: Epoch: 7, Batch: 13, Loss: 0.4653, Elapsed: 0m34s
2023-03-31 19:29:36.364165: Epoch: 7, Batch: 14, Loss: 0.4612, Elapsed: 0m30s
2023-03-31 19:30:00.611237: Epoch: 7, Batch: 15, Loss: 0.4494, Elapsed: 0m24s
2023-03-31 19:30:46.495238: Epoch: 7, Batch: 16, Loss: 0.4850, Elapsed: 0m45s
2023-03-31 19:31:24.357608: Epoch: 7, Batch: 17, Loss: 0.4690, Elapsed: 0m37s
2023-03-31 19:31:52.667596: Epoch: 7, Batch: 18, Loss: 0.4461, Elapsed: 0m28s
2023-03-31 19:32:22.253407: Epoch: 7, Batch: 19, Loss: 0.4674, Elapsed: 0m29s
2023-03-31 19:32:51.107092: Epoch: 7, Batch: 20, Loss: 0.4486, Elapsed: 0m28s
2023-03-31 19:33:28.793031: Epoch: 7, Batch: 21, Loss: 0.4643, Elapsed: 0m37s
2023-03-31 19:33:48.872041: Epoch: 7, Batch: 22, Loss: 0.4408, Elapsed: 0m20s
2023-03-31 19:34:14.644052: Epoch: 7, Batch: 23, Loss: 0.4511, Elapsed: 0m25s
2023-03-31 19:34:41.554337: Epoch: 7, Batch: 24, Loss: 0.4494, Elapsed: 0m26s
2023-03-31 19:35:07.773587: Epoch: 7, Batch: 25, Loss: 0.4432, Elapsed: 0m26s
2023-03-31 19:35:41.010993: Epoch: 7, Batch: 26, Loss: 0.4647, Elapsed: 0m33s
2023-03-31 19:36:07.084476: Epoch: 7, Batch: 27, Loss: 0.4560, Elapsed: 0m26s
2023-03-31 19:36:35.312581: Epoch: 7, Batch: 28, Loss: 0.4655, Elapsed: 0m28s
2023-03-31 19:36:59.181456: Epoch: 7, Batch: 29, Loss: 0.4564, Elapsed: 0m23s
2023-03-31 19:37:31.974751: Epoch: 7, Batch: 30, Loss: 0.4628, Elapsed: 0m32s
2023-03-31 19:37:59.072094: Epoch: 7, Batch: 31, Loss: 0.4538, Elapsed: 0m27s
2023-03-31 19:38:26.801694: Epoch: 7, Batch: 32, Loss: 0.4528, Elapsed: 0m27s
2023-03-31 19:38:49.393007: Epoch: 7, Batch: 33, Loss: 0.4363, Elapsed: 0m22s
2023-03-31 19:39:39.466085: Epoch: 7, Batch: 34, Loss: 0.4777, Elapsed: 0m50s
2023-03-31 19:40:05.267148: Epoch: 7, Batch: 35, Loss: 0.4385, Elapsed: 0m25s
2023-03-31 19:40:31.206924: Epoch: 7, Batch: 36, Loss: 0.4556, Elapsed: 0m25s
2023-03-31 19:41:00.380513: Epoch: 7, Batch: 37, Loss: 0.4602, Elapsed: 0m29s
2023-03-31 19:41:46.919371: Epoch: 7, Batch: 38, Loss: 0.4761, Elapsed: 0m46s
2023-03-31 19:42:15.742721: Epoch: 7, Batch: 39, Loss: 0.4474, Elapsed: 0m28s
2023-03-31 19:42:41.815908: Epoch: 7, Batch: 40, Loss: 0.4496, Elapsed: 0m26s
2023-03-31 19:43:20.806967: Epoch: 7, Batch: 41, Loss: 0.4668, Elapsed: 0m38s
2023-03-31 19:43:50.452602: Epoch: 7, Batch: 42, Loss: 0.4606, Elapsed: 0m29s
2023-03-31 19:44:36.630617: Epoch: 7, Batch: 43, Loss: 0.4737, Elapsed: 0m46s
2023-03-31 19:45:05.716097: Epoch: 7, Batch: 44, Loss: 0.4692, Elapsed: 0m29s
2023-03-31 19:45:27.508678: Epoch: 7, Batch: 45, Loss: 0.4374, Elapsed: 0m21s
2023-03-31 19:45:59.515110: Epoch: 7, Batch: 46, Loss: 0.4581, Elapsed: 0m31s
2023-03-31 19:46:24.386998: Epoch: 7, Batch: 47, Loss: 0.4385, Elapsed: 0m24s
2023-03-31 19:47:05.440002: Epoch: 7, Batch: 48, Loss: 0.4649, Elapsed: 0m41s
2023-03-31 19:47:26.365017: Epoch: 7, Batch: 49, Loss: 0.4207, Elapsed: 0m20s
2023-03-31 19:48:04.775979: Epoch: 7, Batch: 50, Loss: 0.4633, Elapsed: 0m38s
2023-03-31 19:48:04.790612 Starting testing the valid set with 20 subgraphs!
2023-03-31 19:53:01.243609: validation Test:  Loss: 0.4567,  AUC: 0.8414, Acc: 76.6288,  Precision: 0.8359 -- Elapsed: 4m56s
2023-03-31 19:53:01.244796 Starting testing the train set with 20 subgraphs!
2023-03-31 20:12:22.916599: training Test:  Loss: 0.4557,  AUC: 0.8418, Acc: 76.7770,  Precision: 0.8428 -- Elapsed: 19m21s
2023-03-31 20:12:53.550212: Epoch: 7, Batch: 51, Loss: 0.4607, Elapsed: 0m30s
2023-03-31 20:13:15.854971: Epoch: 7, Batch: 52, Loss: 0.4474, Elapsed: 0m22s
2023-03-31 20:13:43.315980: Epoch: 7, Batch: 53, Loss: 0.4565, Elapsed: 0m27s
2023-03-31 20:14:12.641352: Epoch: 7, Batch: 54, Loss: 0.4492, Elapsed: 0m29s
2023-03-31 20:14:35.996817: Epoch: 7, Batch: 55, Loss: 0.4420, Elapsed: 0m23s
2023-03-31 20:14:59.317925: Epoch: 7, Batch: 56, Loss: 0.4348, Elapsed: 0m23s
2023-03-31 20:15:16.793732: Epoch: 7, Batch: 57, Loss: 0.4096, Elapsed: 0m17s
2023-03-31 20:15:38.518935: Epoch: 7, Batch: 58, Loss: 0.4228, Elapsed: 0m21s
2023-03-31 20:16:14.785104: Epoch: 7, Batch: 59, Loss: 0.4669, Elapsed: 0m36s
2023-03-31 20:16:46.916977: Epoch: 7, Batch: 60, Loss: 0.4614, Elapsed: 0m32s
2023-03-31 20:17:14.062394: Epoch: 7, Batch: 61, Loss: 0.4523, Elapsed: 0m27s
2023-03-31 20:17:38.601595: Epoch: 7, Batch: 62, Loss: 0.4447, Elapsed: 0m24s
2023-03-31 20:18:16.803850: Epoch: 7, Batch: 63, Loss: 0.4634, Elapsed: 0m38s
2023-03-31 20:19:00.847224: Epoch: 7, Batch: 64, Loss: 0.4711, Elapsed: 0m44s
2023-03-31 20:19:30.326605: Epoch: 7, Batch: 65, Loss: 0.4448, Elapsed: 0m29s
2023-03-31 20:19:53.878822: Epoch: 7, Batch: 66, Loss: 0.4349, Elapsed: 0m23s
2023-03-31 20:20:09.068356: Epoch: 7, Batch: 67, Loss: 0.4112, Elapsed: 0m15s
2023-03-31 20:20:40.899643: Epoch: 7, Batch: 68, Loss: 0.4598, Elapsed: 0m31s
2023-03-31 20:21:13.103867: Epoch: 7, Batch: 69, Loss: 0.4715, Elapsed: 0m32s
2023-03-31 20:21:49.352989: Epoch: 7, Batch: 70, Loss: 0.4749, Elapsed: 0m36s
2023-03-31 20:22:22.313512: Epoch: 7, Batch: 71, Loss: 0.4587, Elapsed: 0m32s
2023-03-31 20:22:52.563549: Epoch: 7, Batch: 72, Loss: 0.4735, Elapsed: 0m30s
2023-03-31 20:23:15.470006: Epoch: 7, Batch: 73, Loss: 0.4473, Elapsed: 0m22s
2023-03-31 20:23:46.269067: Epoch: 7, Batch: 74, Loss: 0.4508, Elapsed: 0m30s
2023-03-31 20:24:26.602615: Epoch: 7, Batch: 75, Loss: 0.4718, Elapsed: 0m40s
2023-03-31 20:25:02.272284: Epoch: 7, Batch: 76, Loss: 0.4659, Elapsed: 0m35s
2023-03-31 20:25:21.894971: Epoch: 7, Batch: 77, Loss: 0.4192, Elapsed: 0m19s
2023-03-31 20:26:01.441252: Epoch: 7, Batch: 78, Loss: 0.4707, Elapsed: 0m39s
2023-03-31 20:26:31.179459: Epoch: 7, Batch: 79, Loss: 0.4546, Elapsed: 0m29s
2023-03-31 20:26:52.617369: Epoch: 7, Batch: 80, Loss: 0.4410, Elapsed: 0m21s
2023-03-31 20:27:28.703897: Epoch: 8, Batch: 1, Loss: 0.4691, Elapsed: 0m36s
2023-03-31 20:27:49.473176: Epoch: 8, Batch: 2, Loss: 0.4222, Elapsed: 0m20s
2023-03-31 20:28:31.174887: Epoch: 8, Batch: 3, Loss: 0.4655, Elapsed: 0m41s
2023-03-31 20:28:59.088735: Epoch: 8, Batch: 4, Loss: 0.4662, Elapsed: 0m27s
2023-03-31 20:29:28.589415: Epoch: 8, Batch: 5, Loss: 0.4474, Elapsed: 0m29s
2023-03-31 20:29:55.100307: Epoch: 8, Batch: 6, Loss: 0.4466, Elapsed: 0m26s
2023-03-31 20:30:18.121261: Epoch: 8, Batch: 7, Loss: 0.4432, Elapsed: 0m23s
2023-03-31 20:30:44.366933: Epoch: 8, Batch: 8, Loss: 0.4569, Elapsed: 0m26s
2023-03-31 20:31:12.566895: Epoch: 8, Batch: 9, Loss: 0.4518, Elapsed: 0m28s
2023-03-31 20:31:36.571304: Epoch: 8, Batch: 10, Loss: 0.4438, Elapsed: 0m23s
2023-03-31 20:32:04.950502: Epoch: 8, Batch: 11, Loss: 0.4448, Elapsed: 0m28s
2023-03-31 20:32:31.445408: Epoch: 8, Batch: 12, Loss: 0.4485, Elapsed: 0m26s
2023-03-31 20:33:18.155534: Epoch: 8, Batch: 13, Loss: 0.4759, Elapsed: 0m46s
2023-03-31 20:33:47.414750: Epoch: 8, Batch: 14, Loss: 0.4421, Elapsed: 0m29s
2023-03-31 20:34:10.092255: Epoch: 8, Batch: 15, Loss: 0.4310, Elapsed: 0m22s
2023-03-31 20:34:49.981740: Epoch: 8, Batch: 16, Loss: 0.4675, Elapsed: 0m39s
2023-03-31 20:35:14.840371: Epoch: 8, Batch: 17, Loss: 0.4339, Elapsed: 0m24s
2023-03-31 20:35:46.386866: Epoch: 8, Batch: 18, Loss: 0.4685, Elapsed: 0m31s
2023-03-31 20:36:21.947870: Epoch: 8, Batch: 19, Loss: 0.4657, Elapsed: 0m35s
2023-03-31 20:36:44.254269: Epoch: 8, Batch: 20, Loss: 0.4336, Elapsed: 0m22s
2023-03-31 20:37:27.216739: Epoch: 8, Batch: 21, Loss: 0.4706, Elapsed: 0m42s
2023-03-31 20:38:05.135590: Epoch: 8, Batch: 22, Loss: 0.4684, Elapsed: 0m37s
2023-03-31 20:38:35.203587: Epoch: 8, Batch: 23, Loss: 0.4585, Elapsed: 0m30s
2023-03-31 20:39:05.781756: Epoch: 8, Batch: 24, Loss: 0.4622, Elapsed: 0m30s
2023-03-31 20:39:32.747725: Epoch: 8, Batch: 25, Loss: 0.4261, Elapsed: 0m26s
2023-03-31 20:40:10.618834: Epoch: 8, Batch: 26, Loss: 0.4625, Elapsed: 0m37s
2023-03-31 20:40:37.161335: Epoch: 8, Batch: 27, Loss: 0.4483, Elapsed: 0m26s
2023-03-31 20:41:01.504811: Epoch: 8, Batch: 28, Loss: 0.4432, Elapsed: 0m24s
2023-03-31 20:41:45.680515: Epoch: 8, Batch: 29, Loss: 0.4700, Elapsed: 0m44s
2023-03-31 20:42:09.740576: Epoch: 8, Batch: 30, Loss: 0.4370, Elapsed: 0m24s
2023-03-31 20:42:31.632324: Epoch: 8, Batch: 31, Loss: 0.4366, Elapsed: 0m21s
2023-03-31 20:43:02.454428: Epoch: 8, Batch: 32, Loss: 0.4476, Elapsed: 0m30s
2023-03-31 20:43:21.668084: Epoch: 8, Batch: 33, Loss: 0.4181, Elapsed: 0m19s
2023-03-31 20:43:55.196413: Epoch: 8, Batch: 34, Loss: 0.4573, Elapsed: 0m33s
2023-03-31 20:44:29.143648: Epoch: 8, Batch: 35, Loss: 0.4712, Elapsed: 0m33s
2023-03-31 20:44:54.404082: Epoch: 8, Batch: 36, Loss: 0.4448, Elapsed: 0m25s
2023-03-31 20:45:27.167455: Epoch: 8, Batch: 37, Loss: 0.4530, Elapsed: 0m32s
2023-03-31 20:46:00.114987: Epoch: 8, Batch: 38, Loss: 0.4592, Elapsed: 0m32s
2023-03-31 20:46:24.456131: Epoch: 8, Batch: 39, Loss: 0.4377, Elapsed: 0m24s
2023-03-31 20:46:47.869894: Epoch: 8, Batch: 40, Loss: 0.4451, Elapsed: 0m23s
2023-03-31 20:47:33.452766: Epoch: 8, Batch: 41, Loss: 0.4667, Elapsed: 0m45s
2023-03-31 20:47:53.201993: Epoch: 8, Batch: 42, Loss: 0.4373, Elapsed: 0m19s
2023-03-31 20:48:20.197510: Epoch: 8, Batch: 43, Loss: 0.4507, Elapsed: 0m26s
2023-03-31 20:48:46.469384: Epoch: 8, Batch: 44, Loss: 0.4690, Elapsed: 0m26s
2023-03-31 20:49:14.653890: Epoch: 8, Batch: 45, Loss: 0.4484, Elapsed: 0m28s
2023-03-31 20:49:50.935631: Epoch: 8, Batch: 46, Loss: 0.4667, Elapsed: 0m36s
2023-03-31 20:50:12.772458: Epoch: 8, Batch: 47, Loss: 0.4398, Elapsed: 0m21s
2023-03-31 20:50:41.316000: Epoch: 8, Batch: 48, Loss: 0.4524, Elapsed: 0m28s
2023-03-31 20:51:20.113679: Epoch: 8, Batch: 49, Loss: 0.4842, Elapsed: 0m38s
2023-03-31 20:52:10.948568: Epoch: 8, Batch: 50, Loss: 0.5115, Elapsed: 0m50s
2023-03-31 20:52:10.962229 Starting testing the valid set with 20 subgraphs!
2023-03-31 20:57:01.142127: validation Test:  Loss: 0.4660,  AUC: 0.8360, Acc: 75.9018,  Precision: 0.7970 -- Elapsed: 4m50s
2023-03-31 20:57:01.143562 Starting testing the train set with 20 subgraphs!
2023-03-31 21:16:20.572899: training Test:  Loss: 0.4608,  AUC: 0.8371, Acc: 76.1403,  Precision: 0.8033 -- Elapsed: 19m19s
2023-03-31 21:17:00.250858: Epoch: 8, Batch: 51, Loss: 0.4729, Elapsed: 0m39s
2023-03-31 21:17:30.268452: Epoch: 8, Batch: 52, Loss: 0.4623, Elapsed: 0m30s
2023-03-31 21:18:05.867197: Epoch: 8, Batch: 53, Loss: 0.4688, Elapsed: 0m35s
2023-03-31 21:18:31.951665: Epoch: 8, Batch: 54, Loss: 0.4549, Elapsed: 0m26s
2023-03-31 21:19:03.979465: Epoch: 8, Batch: 55, Loss: 0.4662, Elapsed: 0m32s
2023-03-31 21:19:37.697063: Epoch: 8, Batch: 56, Loss: 0.4665, Elapsed: 0m33s
2023-03-31 21:20:07.039450: Epoch: 8, Batch: 57, Loss: 0.4589, Elapsed: 0m29s
2023-03-31 21:20:26.905072: Epoch: 8, Batch: 58, Loss: 0.4303, Elapsed: 0m19s
2023-03-31 21:20:56.436806: Epoch: 8, Batch: 59, Loss: 0.4729, Elapsed: 0m29s
2023-03-31 21:21:13.828920: Epoch: 8, Batch: 60, Loss: 0.4175, Elapsed: 0m17s
2023-03-31 21:21:54.151038: Epoch: 8, Batch: 61, Loss: 0.4704, Elapsed: 0m40s
2023-03-31 21:22:22.909114: Epoch: 8, Batch: 62, Loss: 0.4534, Elapsed: 0m28s
2023-03-31 21:22:51.862532: Epoch: 8, Batch: 63, Loss: 0.4577, Elapsed: 0m28s
2023-03-31 21:23:16.253787: Epoch: 8, Batch: 64, Loss: 0.4474, Elapsed: 0m24s
2023-03-31 21:23:53.947903: Epoch: 8, Batch: 65, Loss: 0.4872, Elapsed: 0m37s
2023-03-31 21:24:20.216713: Epoch: 8, Batch: 66, Loss: 0.4541, Elapsed: 0m26s
2023-03-31 21:24:59.357257: Epoch: 8, Batch: 67, Loss: 0.4698, Elapsed: 0m39s
2023-03-31 21:25:29.092766: Epoch: 8, Batch: 68, Loss: 0.4614, Elapsed: 0m29s
2023-03-31 21:25:51.603087: Epoch: 8, Batch: 69, Loss: 0.4438, Elapsed: 0m22s
2023-03-31 21:26:27.251690: Epoch: 8, Batch: 70, Loss: 0.4710, Elapsed: 0m35s
2023-03-31 21:26:58.530914: Epoch: 8, Batch: 71, Loss: 0.4621, Elapsed: 0m31s
2023-03-31 21:27:28.493379: Epoch: 8, Batch: 72, Loss: 0.4644, Elapsed: 0m29s
2023-03-31 21:28:06.831318: Epoch: 8, Batch: 73, Loss: 0.4703, Elapsed: 0m38s
2023-03-31 21:28:35.264837: Epoch: 8, Batch: 74, Loss: 0.4670, Elapsed: 0m28s
2023-03-31 21:29:10.400660: Epoch: 8, Batch: 75, Loss: 0.4649, Elapsed: 0m35s
2023-03-31 21:29:32.927967: Epoch: 8, Batch: 76, Loss: 0.4520, Elapsed: 0m22s
2023-03-31 21:29:47.685690: Epoch: 8, Batch: 77, Loss: 0.4189, Elapsed: 0m14s
2023-03-31 21:30:17.365301: Epoch: 8, Batch: 78, Loss: 0.4655, Elapsed: 0m29s
2023-03-31 21:30:44.962684: Epoch: 8, Batch: 79, Loss: 0.4493, Elapsed: 0m27s
2023-03-31 21:31:11.663551: Epoch: 8, Batch: 80, Loss: 0.4525, Elapsed: 0m26s
2023-03-31 21:31:45.297133: Epoch: 9, Batch: 1, Loss: 0.4643, Elapsed: 0m33s
2023-03-31 21:32:28.007447: Epoch: 9, Batch: 2, Loss: 0.4641, Elapsed: 0m42s
2023-03-31 21:33:06.134741: Epoch: 9, Batch: 3, Loss: 0.4722, Elapsed: 0m38s
2023-03-31 21:33:29.787333: Epoch: 9, Batch: 4, Loss: 0.4546, Elapsed: 0m23s
2023-03-31 21:33:52.802456: Epoch: 9, Batch: 5, Loss: 0.4363, Elapsed: 0m23s
2023-03-31 21:34:38.705512: Epoch: 9, Batch: 6, Loss: 0.4766, Elapsed: 0m45s
2023-03-31 21:35:04.171143: Epoch: 9, Batch: 7, Loss: 0.4500, Elapsed: 0m25s
2023-03-31 21:35:31.290228: Epoch: 9, Batch: 8, Loss: 0.4451, Elapsed: 0m27s
2023-03-31 21:36:00.211862: Epoch: 9, Batch: 9, Loss: 0.4465, Elapsed: 0m28s
2023-03-31 21:36:34.751951: Epoch: 9, Batch: 10, Loss: 0.4660, Elapsed: 0m34s
2023-03-31 21:36:57.799407: Epoch: 9, Batch: 11, Loss: 0.4467, Elapsed: 0m23s
2023-03-31 21:37:17.457162: Epoch: 9, Batch: 12, Loss: 0.4174, Elapsed: 0m19s
2023-03-31 21:37:41.821762: Epoch: 9, Batch: 13, Loss: 0.4360, Elapsed: 0m24s
2023-03-31 21:38:11.162495: Epoch: 9, Batch: 14, Loss: 0.4419, Elapsed: 0m29s
2023-03-31 21:38:54.338333: Epoch: 9, Batch: 15, Loss: 0.4684, Elapsed: 0m43s
2023-03-31 21:39:11.641794: Epoch: 9, Batch: 16, Loss: 0.4003, Elapsed: 0m17s
2023-03-31 21:39:38.244978: Epoch: 9, Batch: 17, Loss: 0.4438, Elapsed: 0m26s
2023-03-31 21:40:10.723649: Epoch: 9, Batch: 18, Loss: 0.4505, Elapsed: 0m32s
2023-03-31 21:40:56.007538: Epoch: 9, Batch: 19, Loss: 0.4700, Elapsed: 0m45s
2023-03-31 21:41:18.175401: Epoch: 9, Batch: 20, Loss: 0.4417, Elapsed: 0m22s
2023-03-31 21:41:41.897538: Epoch: 9, Batch: 21, Loss: 0.4329, Elapsed: 0m23s
2023-03-31 21:42:10.870008: Epoch: 9, Batch: 22, Loss: 0.4434, Elapsed: 0m28s
2023-03-31 21:42:40.263698: Epoch: 9, Batch: 23, Loss: 0.4535, Elapsed: 0m29s
2023-03-31 21:43:01.719736: Epoch: 9, Batch: 24, Loss: 0.4369, Elapsed: 0m21s
2023-03-31 21:43:30.727800: Epoch: 9, Batch: 25, Loss: 0.4476, Elapsed: 0m28s
2023-03-31 21:44:06.995561: Epoch: 9, Batch: 26, Loss: 0.4680, Elapsed: 0m36s
2023-03-31 21:44:32.384435: Epoch: 9, Batch: 27, Loss: 0.4217, Elapsed: 0m25s
2023-03-31 21:45:02.402196: Epoch: 9, Batch: 28, Loss: 0.4576, Elapsed: 0m30s
2023-03-31 21:45:31.987191: Epoch: 9, Batch: 29, Loss: 0.4523, Elapsed: 0m29s
2023-03-31 21:46:00.945505: Epoch: 9, Batch: 30, Loss: 0.4444, Elapsed: 0m28s
2023-03-31 21:46:22.980089: Epoch: 9, Batch: 31, Loss: 0.4176, Elapsed: 0m22s
2023-03-31 21:47:00.553304: Epoch: 9, Batch: 32, Loss: 0.4660, Elapsed: 0m37s
2023-03-31 21:47:19.711185: Epoch: 9, Batch: 33, Loss: 0.4180, Elapsed: 0m19s
2023-03-31 21:47:49.594141: Epoch: 9, Batch: 34, Loss: 0.4625, Elapsed: 0m29s
2023-03-31 21:48:16.236119: Epoch: 9, Batch: 35, Loss: 0.4429, Elapsed: 0m26s
2023-03-31 21:48:43.322966: Epoch: 9, Batch: 36, Loss: 0.4516, Elapsed: 0m27s
2023-03-31 21:49:13.220871: Epoch: 9, Batch: 37, Loss: 0.4498, Elapsed: 0m29s
2023-03-31 21:49:37.793195: Epoch: 9, Batch: 38, Loss: 0.4409, Elapsed: 0m24s
2023-03-31 21:50:12.189178: Epoch: 9, Batch: 39, Loss: 0.4652, Elapsed: 0m34s
2023-03-31 21:50:33.389991: Epoch: 9, Batch: 40, Loss: 0.4302, Elapsed: 0m21s
2023-03-31 21:51:04.140724: Epoch: 9, Batch: 41, Loss: 0.4561, Elapsed: 0m30s
2023-03-31 21:51:36.072058: Epoch: 9, Batch: 42, Loss: 0.4539, Elapsed: 0m31s
2023-03-31 21:52:05.864470: Epoch: 9, Batch: 43, Loss: 0.4606, Elapsed: 0m29s
2023-03-31 21:52:39.148201: Epoch: 9, Batch: 44, Loss: 0.4588, Elapsed: 0m33s
2023-03-31 21:53:04.339380: Epoch: 9, Batch: 45, Loss: 0.4448, Elapsed: 0m25s
2023-03-31 21:53:36.479796: Epoch: 9, Batch: 46, Loss: 0.4592, Elapsed: 0m32s
2023-03-31 21:53:57.318469: Epoch: 9, Batch: 47, Loss: 0.4180, Elapsed: 0m20s
2023-03-31 21:54:12.307240: Epoch: 9, Batch: 48, Loss: 0.4056, Elapsed: 0m14s
2023-03-31 21:54:37.248860: Epoch: 9, Batch: 49, Loss: 0.4492, Elapsed: 0m24s
2023-03-31 21:55:15.408812: Epoch: 9, Batch: 50, Loss: 0.4589, Elapsed: 0m38s
2023-03-31 21:55:15.422430 Starting testing the valid set with 20 subgraphs!
2023-03-31 22:00:07.155011: validation Test:  Loss: 0.4531,  AUC: 0.8424, Acc: 76.7440,  Precision: 0.8526 -- Elapsed: 4m51s
2023-03-31 22:00:07.156529 Starting testing the train set with 20 subgraphs!
2023-03-31 22:19:25.371544: training Test:  Loss: 0.4513,  AUC: 0.8433, Acc: 77.0481,  Precision: 0.8643 -- Elapsed: 19m18s
2023-03-31 22:20:02.086697: Epoch: 9, Batch: 51, Loss: 0.4619, Elapsed: 0m36s
2023-03-31 22:20:24.432802: Epoch: 9, Batch: 52, Loss: 0.4382, Elapsed: 0m22s
2023-03-31 22:20:52.402122: Epoch: 9, Batch: 53, Loss: 0.4466, Elapsed: 0m27s
2023-03-31 22:21:12.371118: Epoch: 9, Batch: 54, Loss: 0.4360, Elapsed: 0m19s
2023-03-31 22:21:50.101453: Epoch: 9, Batch: 55, Loss: 0.4622, Elapsed: 0m37s
2023-03-31 22:22:16.734420: Epoch: 9, Batch: 56, Loss: 0.4520, Elapsed: 0m26s
2023-03-31 22:22:38.892644: Epoch: 9, Batch: 57, Loss: 0.4277, Elapsed: 0m22s
2023-03-31 22:23:08.315090: Epoch: 9, Batch: 58, Loss: 0.4614, Elapsed: 0m29s
2023-03-31 22:23:48.029139: Epoch: 9, Batch: 59, Loss: 0.4618, Elapsed: 0m39s
2023-03-31 22:24:14.241735: Epoch: 9, Batch: 60, Loss: 0.4450, Elapsed: 0m26s
2023-03-31 22:24:42.147548: Epoch: 9, Batch: 61, Loss: 0.4411, Elapsed: 0m27s
2023-03-31 22:25:12.394945: Epoch: 9, Batch: 62, Loss: 0.4562, Elapsed: 0m30s
2023-03-31 22:25:50.092227: Epoch: 9, Batch: 63, Loss: 0.4754, Elapsed: 0m37s
2023-03-31 22:26:29.961562: Epoch: 9, Batch: 64, Loss: 0.4654, Elapsed: 0m39s
2023-03-31 22:27:02.756931: Epoch: 9, Batch: 65, Loss: 0.4607, Elapsed: 0m32s
2023-03-31 22:27:35.787060: Epoch: 9, Batch: 66, Loss: 0.4642, Elapsed: 0m33s
2023-03-31 22:28:02.119827: Epoch: 9, Batch: 67, Loss: 0.4447, Elapsed: 0m26s
2023-03-31 22:28:52.317059: Epoch: 9, Batch: 68, Loss: 0.4751, Elapsed: 0m50s
2023-03-31 22:29:32.391690: Epoch: 9, Batch: 69, Loss: 0.4634, Elapsed: 0m40s
2023-03-31 22:30:02.069388: Epoch: 9, Batch: 70, Loss: 0.4551, Elapsed: 0m29s
2023-03-31 22:30:29.508604: Epoch: 9, Batch: 71, Loss: 0.4443, Elapsed: 0m27s
2023-03-31 22:31:02.942864: Epoch: 9, Batch: 72, Loss: 0.4566, Elapsed: 0m33s
2023-03-31 22:31:29.042918: Epoch: 9, Batch: 73, Loss: 0.4449, Elapsed: 0m26s
2023-03-31 22:31:57.743291: Epoch: 9, Batch: 74, Loss: 0.4375, Elapsed: 0m28s
2023-03-31 22:32:35.178837: Epoch: 9, Batch: 75, Loss: 0.4630, Elapsed: 0m37s
2023-03-31 22:33:22.208419: Epoch: 9, Batch: 76, Loss: 0.4663, Elapsed: 0m47s
2023-03-31 22:34:00.689341: Epoch: 9, Batch: 77, Loss: 0.4604, Elapsed: 0m38s
2023-03-31 22:34:24.712270: Epoch: 9, Batch: 78, Loss: 0.4361, Elapsed: 0m24s
2023-03-31 22:34:54.753263: Epoch: 9, Batch: 79, Loss: 0.4588, Elapsed: 0m30s
2023-03-31 22:35:22.303922: Epoch: 9, Batch: 80, Loss: 0.4477, Elapsed: 0m27s
2023-03-31 22:35:41.527228: Epoch: 10, Batch: 1, Loss: 0.4244, Elapsed: 0m19s
2023-03-31 22:35:59.668721: Epoch: 10, Batch: 2, Loss: 0.4069, Elapsed: 0m18s
2023-03-31 22:36:28.630567: Epoch: 10, Batch: 3, Loss: 0.4478, Elapsed: 0m28s
2023-03-31 22:37:08.330838: Epoch: 10, Batch: 4, Loss: 0.4614, Elapsed: 0m39s
2023-03-31 22:37:36.142635: Epoch: 10, Batch: 5, Loss: 0.4405, Elapsed: 0m27s
2023-03-31 22:38:04.992333: Epoch: 10, Batch: 6, Loss: 0.4490, Elapsed: 0m28s
2023-03-31 22:38:33.927302: Epoch: 10, Batch: 7, Loss: 0.4388, Elapsed: 0m28s
2023-03-31 22:38:53.967771: Epoch: 10, Batch: 8, Loss: 0.4290, Elapsed: 0m20s
2023-03-31 22:39:14.012183: Epoch: 10, Batch: 9, Loss: 0.4167, Elapsed: 0m20s
2023-03-31 22:39:52.665117: Epoch: 10, Batch: 10, Loss: 0.4591, Elapsed: 0m38s
2023-03-31 22:40:22.433492: Epoch: 10, Batch: 11, Loss: 0.4475, Elapsed: 0m29s
2023-03-31 22:40:48.638432: Epoch: 10, Batch: 12, Loss: 0.4564, Elapsed: 0m26s
2023-03-31 22:41:27.875010: Epoch: 10, Batch: 13, Loss: 0.4659, Elapsed: 0m39s
2023-03-31 22:41:55.028103: Epoch: 10, Batch: 14, Loss: 0.4590, Elapsed: 0m27s
2023-03-31 22:42:34.976090: Epoch: 10, Batch: 15, Loss: 0.4602, Elapsed: 0m39s
2023-03-31 22:42:49.828548: Epoch: 10, Batch: 16, Loss: 0.3898, Elapsed: 0m14s
2023-03-31 22:43:18.971172: Epoch: 10, Batch: 17, Loss: 0.4554, Elapsed: 0m29s
2023-03-31 22:43:52.015846: Epoch: 10, Batch: 18, Loss: 0.4650, Elapsed: 0m33s
2023-03-31 22:44:37.160207: Epoch: 10, Batch: 19, Loss: 0.4718, Elapsed: 0m45s
2023-03-31 22:45:07.697504: Epoch: 10, Batch: 20, Loss: 0.4453, Elapsed: 0m30s
2023-03-31 22:45:50.492733: Epoch: 10, Batch: 21, Loss: 0.4650, Elapsed: 0m42s
2023-03-31 22:46:11.838111: Epoch: 10, Batch: 22, Loss: 0.4136, Elapsed: 0m21s
2023-03-31 22:46:41.839025: Epoch: 10, Batch: 23, Loss: 0.4620, Elapsed: 0m29s
2023-03-31 22:47:18.304705: Epoch: 10, Batch: 24, Loss: 0.4633, Elapsed: 0m36s
2023-03-31 22:47:48.947409: Epoch: 10, Batch: 25, Loss: 0.4394, Elapsed: 0m30s
2023-03-31 22:48:13.455183: Epoch: 10, Batch: 26, Loss: 0.4377, Elapsed: 0m24s
2023-03-31 22:48:50.205706: Epoch: 10, Batch: 27, Loss: 0.4547, Elapsed: 0m36s
2023-03-31 22:49:22.464661: Epoch: 10, Batch: 28, Loss: 0.4554, Elapsed: 0m32s
2023-03-31 22:49:55.128218: Epoch: 10, Batch: 29, Loss: 0.4524, Elapsed: 0m32s
2023-03-31 22:50:21.349097: Epoch: 10, Batch: 30, Loss: 0.4454, Elapsed: 0m26s
2023-03-31 22:50:43.618930: Epoch: 10, Batch: 31, Loss: 0.4360, Elapsed: 0m22s
2023-03-31 22:51:09.658760: Epoch: 10, Batch: 32, Loss: 0.4446, Elapsed: 0m26s
2023-03-31 22:51:38.966937: Epoch: 10, Batch: 33, Loss: 0.4534, Elapsed: 0m29s
2023-03-31 22:52:03.667998: Epoch: 10, Batch: 34, Loss: 0.4328, Elapsed: 0m24s
2023-03-31 22:52:30.202113: Epoch: 10, Batch: 35, Loss: 0.4333, Elapsed: 0m26s
2023-03-31 22:52:53.632839: Epoch: 10, Batch: 36, Loss: 0.4280, Elapsed: 0m23s
2023-03-31 22:53:26.892550: Epoch: 10, Batch: 37, Loss: 0.4678, Elapsed: 0m33s
2023-03-31 22:54:13.025181: Epoch: 10, Batch: 38, Loss: 0.4736, Elapsed: 0m46s
2023-03-31 22:54:50.334388: Epoch: 10, Batch: 39, Loss: 0.4857, Elapsed: 0m37s
2023-03-31 22:55:22.906871: Epoch: 10, Batch: 40, Loss: 0.4445, Elapsed: 0m32s
2023-03-31 22:55:48.364990: Epoch: 10, Batch: 41, Loss: 0.4414, Elapsed: 0m25s
2023-03-31 22:56:13.997783: Epoch: 10, Batch: 42, Loss: 0.4351, Elapsed: 0m25s
2023-03-31 22:56:37.716201: Epoch: 10, Batch: 43, Loss: 0.4394, Elapsed: 0m23s
2023-03-31 22:57:04.412523: Epoch: 10, Batch: 44, Loss: 0.4429, Elapsed: 0m26s
2023-03-31 22:57:30.112290: Epoch: 10, Batch: 45, Loss: 0.4406, Elapsed: 0m25s
2023-03-31 22:57:55.596677: Epoch: 10, Batch: 46, Loss: 0.4198, Elapsed: 0m25s
2023-03-31 22:58:18.327357: Epoch: 10, Batch: 47, Loss: 0.4355, Elapsed: 0m22s
2023-03-31 22:58:41.132364: Epoch: 10, Batch: 48, Loss: 0.4324, Elapsed: 0m22s
2023-03-31 22:59:07.101004: Epoch: 10, Batch: 49, Loss: 0.4376, Elapsed: 0m25s
2023-03-31 22:59:37.075690: Epoch: 10, Batch: 50, Loss: 0.4492, Elapsed: 0m29s
2023-03-31 22:59:37.090457 Starting testing the valid set with 20 subgraphs!
2023-03-31 23:04:28.606214: validation Test:  Loss: 0.4493,  AUC: 0.8471, Acc: 77.5028,  Precision: 0.8438 -- Elapsed: 4m51s
2023-03-31 23:04:28.607382 Starting testing the train set with 20 subgraphs!
2023-03-31 23:23:47.374117: training Test:  Loss: 0.4464,  AUC: 0.8487, Acc: 77.7887,  Precision: 0.8503 -- Elapsed: 19m18s
2023-03-31 23:24:09.691852: Epoch: 10, Batch: 51, Loss: 0.4243, Elapsed: 0m22s
2023-03-31 23:24:36.176875: Epoch: 10, Batch: 52, Loss: 0.4531, Elapsed: 0m26s
2023-03-31 23:25:01.626354: Epoch: 10, Batch: 53, Loss: 0.4377, Elapsed: 0m25s
2023-03-31 23:25:35.791796: Epoch: 10, Batch: 54, Loss: 0.4523, Elapsed: 0m34s
2023-03-31 23:25:57.927385: Epoch: 10, Batch: 55, Loss: 0.4269, Elapsed: 0m22s
2023-03-31 23:26:37.790944: Epoch: 10, Batch: 56, Loss: 0.4592, Elapsed: 0m39s
2023-03-31 23:27:01.491680: Epoch: 10, Batch: 57, Loss: 0.4281, Elapsed: 0m23s
2023-03-31 23:27:42.084006: Epoch: 10, Batch: 58, Loss: 0.4621, Elapsed: 0m40s
2023-03-31 23:28:28.211928: Epoch: 10, Batch: 59, Loss: 0.4551, Elapsed: 0m46s
2023-03-31 23:28:55.313810: Epoch: 10, Batch: 60, Loss: 0.4409, Elapsed: 0m27s
2023-03-31 23:29:32.273226: Epoch: 10, Batch: 61, Loss: 0.4592, Elapsed: 0m36s
2023-03-31 23:30:02.687641: Epoch: 10, Batch: 62, Loss: 0.4388, Elapsed: 0m30s
2023-03-31 23:30:33.274064: Epoch: 10, Batch: 63, Loss: 0.4446, Elapsed: 0m30s
2023-03-31 23:31:15.896345: Epoch: 10, Batch: 64, Loss: 0.4601, Elapsed: 0m42s
2023-03-31 23:31:50.774060: Epoch: 10, Batch: 65, Loss: 0.4574, Elapsed: 0m34s
2023-03-31 23:32:21.174064: Epoch: 10, Batch: 66, Loss: 0.4516, Elapsed: 0m30s
2023-03-31 23:32:59.267259: Epoch: 10, Batch: 67, Loss: 0.4613, Elapsed: 0m38s
2023-03-31 23:33:35.341865: Epoch: 10, Batch: 68, Loss: 0.4674, Elapsed: 0m36s
2023-03-31 23:34:08.815637: Epoch: 10, Batch: 69, Loss: 0.4493, Elapsed: 0m33s
2023-03-31 23:34:30.022731: Epoch: 10, Batch: 70, Loss: 0.4229, Elapsed: 0m21s
2023-03-31 23:34:57.344442: Epoch: 10, Batch: 71, Loss: 0.4378, Elapsed: 0m27s
2023-03-31 23:35:26.673195: Epoch: 10, Batch: 72, Loss: 0.4359, Elapsed: 0m29s
2023-03-31 23:36:16.997989: Epoch: 10, Batch: 73, Loss: 0.4687, Elapsed: 0m50s
2023-03-31 23:36:45.142226: Epoch: 10, Batch: 74, Loss: 0.4467, Elapsed: 0m28s
2023-03-31 23:37:14.326338: Epoch: 10, Batch: 75, Loss: 0.4566, Elapsed: 0m29s
2023-03-31 23:37:39.369080: Epoch: 10, Batch: 76, Loss: 0.4308, Elapsed: 0m25s
2023-03-31 23:38:06.665800: Epoch: 10, Batch: 77, Loss: 0.4591, Elapsed: 0m27s
2023-03-31 23:38:42.289932: Epoch: 10, Batch: 78, Loss: 0.4637, Elapsed: 0m35s
2023-03-31 23:39:16.050423: Epoch: 10, Batch: 79, Loss: 0.4625, Elapsed: 0m33s
2023-03-31 23:39:36.886051: Epoch: 10, Batch: 80, Loss: 0.4130, Elapsed: 0m20s
2023-03-31 23:39:59.390210: Epoch: 11, Batch: 1, Loss: 0.4416, Elapsed: 0m22s
2023-03-31 23:40:18.587227: Epoch: 11, Batch: 2, Loss: 0.4285, Elapsed: 0m19s
2023-03-31 23:40:54.464889: Epoch: 11, Batch: 3, Loss: 0.4589, Elapsed: 0m35s
2023-03-31 23:41:28.778407: Epoch: 11, Batch: 4, Loss: 0.4597, Elapsed: 0m34s
2023-03-31 23:42:11.982498: Epoch: 11, Batch: 5, Loss: 0.4751, Elapsed: 0m43s
2023-03-31 23:42:36.667994: Epoch: 11, Batch: 6, Loss: 0.4315, Elapsed: 0m24s
2023-03-31 23:43:12.277184: Epoch: 11, Batch: 7, Loss: 0.4645, Elapsed: 0m35s
2023-03-31 23:43:34.053232: Epoch: 11, Batch: 8, Loss: 0.4091, Elapsed: 0m21s
2023-03-31 23:44:07.049231: Epoch: 11, Batch: 9, Loss: 0.4434, Elapsed: 0m32s
2023-03-31 23:44:35.341488: Epoch: 11, Batch: 10, Loss: 0.4361, Elapsed: 0m28s
2023-03-31 23:45:10.045520: Epoch: 11, Batch: 11, Loss: 0.4577, Elapsed: 0m34s
2023-03-31 23:45:48.956434: Epoch: 11, Batch: 12, Loss: 0.4638, Elapsed: 0m38s
2023-03-31 23:46:29.086036: Epoch: 11, Batch: 13, Loss: 0.4689, Elapsed: 0m40s
2023-03-31 23:46:58.574369: Epoch: 11, Batch: 14, Loss: 0.4451, Elapsed: 0m29s
2023-03-31 23:47:23.876420: Epoch: 11, Batch: 15, Loss: 0.4366, Elapsed: 0m25s
2023-03-31 23:47:51.807158: Epoch: 11, Batch: 16, Loss: 0.4562, Elapsed: 0m27s
2023-03-31 23:48:17.932219: Epoch: 11, Batch: 17, Loss: 0.4204, Elapsed: 0m26s
2023-03-31 23:48:44.755471: Epoch: 11, Batch: 18, Loss: 0.4829, Elapsed: 0m26s
2023-03-31 23:49:15.246132: Epoch: 11, Batch: 19, Loss: 0.4503, Elapsed: 0m30s
2023-03-31 23:49:51.178007: Epoch: 11, Batch: 20, Loss: 0.4564, Elapsed: 0m35s
2023-03-31 23:50:20.240262: Epoch: 11, Batch: 21, Loss: 0.4389, Elapsed: 0m29s
2023-03-31 23:50:58.690737: Epoch: 11, Batch: 22, Loss: 0.4647, Elapsed: 0m38s
2023-03-31 23:51:21.432193: Epoch: 11, Batch: 23, Loss: 0.4337, Elapsed: 0m22s
2023-03-31 23:51:47.620654: Epoch: 11, Batch: 24, Loss: 0.4491, Elapsed: 0m26s
2023-03-31 23:52:14.077692: Epoch: 11, Batch: 25, Loss: 0.4516, Elapsed: 0m26s
2023-03-31 23:52:46.568064: Epoch: 11, Batch: 26, Loss: 0.4936, Elapsed: 0m32s
2023-03-31 23:53:17.449152: Epoch: 11, Batch: 27, Loss: 0.4676, Elapsed: 0m30s
2023-03-31 23:53:42.137460: Epoch: 11, Batch: 28, Loss: 0.4328, Elapsed: 0m24s
2023-03-31 23:54:20.578019: Epoch: 11, Batch: 29, Loss: 0.5138, Elapsed: 0m38s
2023-03-31 23:54:47.989939: Epoch: 11, Batch: 30, Loss: 0.4683, Elapsed: 0m27s
2023-03-31 23:55:38.954955: Epoch: 11, Batch: 31, Loss: 0.4722, Elapsed: 0m50s
2023-03-31 23:56:17.583153: Epoch: 11, Batch: 32, Loss: 0.4848, Elapsed: 0m38s
2023-03-31 23:57:02.091210: Epoch: 11, Batch: 33, Loss: 0.4762, Elapsed: 0m44s
2023-03-31 23:57:50.013930: Epoch: 11, Batch: 34, Loss: 0.4950, Elapsed: 0m47s
2023-03-31 23:58:16.536294: Epoch: 11, Batch: 35, Loss: 0.4856, Elapsed: 0m26s
2023-03-31 23:58:42.446401: Epoch: 11, Batch: 36, Loss: 0.4998, Elapsed: 0m25s
2023-03-31 23:59:09.401397: Epoch: 11, Batch: 37, Loss: 0.4483, Elapsed: 0m26s
2023-03-31 23:59:36.752021: Epoch: 11, Batch: 38, Loss: 0.4681, Elapsed: 0m27s
2023-04-01 00:00:06.727298: Epoch: 11, Batch: 39, Loss: 0.4480, Elapsed: 0m29s
2023-04-01 00:00:40.127289: Epoch: 11, Batch: 40, Loss: 0.4762, Elapsed: 0m33s
2023-04-01 00:00:59.404622: Epoch: 11, Batch: 41, Loss: 0.4355, Elapsed: 0m19s
2023-04-01 00:01:19.653646: Epoch: 11, Batch: 42, Loss: 0.4353, Elapsed: 0m20s
2023-04-01 00:01:34.510618: Epoch: 11, Batch: 43, Loss: 0.4240, Elapsed: 0m14s
2023-04-01 00:02:08.826588: Epoch: 11, Batch: 44, Loss: 0.4680, Elapsed: 0m34s
2023-04-01 00:02:36.598354: Epoch: 11, Batch: 45, Loss: 0.4448, Elapsed: 0m27s
2023-04-01 00:02:53.777948: Epoch: 11, Batch: 46, Loss: 0.3920, Elapsed: 0m17s
2023-04-01 00:03:26.815102: Epoch: 11, Batch: 47, Loss: 0.4534, Elapsed: 0m33s
2023-04-01 00:04:11.520140: Epoch: 11, Batch: 48, Loss: 0.4737, Elapsed: 0m44s
2023-04-01 00:04:41.452950: Epoch: 11, Batch: 49, Loss: 0.4433, Elapsed: 0m29s
2023-04-01 00:05:13.646948: Epoch: 11, Batch: 50, Loss: 0.4658, Elapsed: 0m32s
2023-04-01 00:05:13.661043 Starting testing the valid set with 20 subgraphs!
2023-04-01 00:10:03.934946: validation Test:  Loss: 0.4552,  AUC: 0.8412, Acc: 76.7270,  Precision: 0.8520 -- Elapsed: 4m50s
2023-04-01 00:10:03.936227 Starting testing the train set with 20 subgraphs!
2023-04-01 00:29:22.467914: training Test:  Loss: 0.4562,  AUC: 0.8403, Acc: 76.6636,  Precision: 0.8514 -- Elapsed: 19m18s
2023-04-01 00:29:52.093025: Epoch: 11, Batch: 51, Loss: 0.4421, Elapsed: 0m29s
2023-04-01 00:30:16.016589: Epoch: 11, Batch: 52, Loss: 0.4571, Elapsed: 0m23s
2023-04-01 00:30:48.968267: Epoch: 11, Batch: 53, Loss: 0.4625, Elapsed: 0m32s
2023-04-01 00:31:12.846157: Epoch: 11, Batch: 54, Loss: 0.4416, Elapsed: 0m23s
2023-04-01 00:31:37.716177: Epoch: 11, Batch: 55, Loss: 0.4339, Elapsed: 0m24s
2023-04-01 00:32:07.144358: Epoch: 11, Batch: 56, Loss: 0.4582, Elapsed: 0m29s
2023-04-01 00:32:35.753086: Epoch: 11, Batch: 57, Loss: 0.4434, Elapsed: 0m28s
2023-04-01 00:33:05.795605: Epoch: 11, Batch: 58, Loss: 0.4540, Elapsed: 0m30s
2023-04-01 00:33:43.298814: Epoch: 11, Batch: 59, Loss: 0.4601, Elapsed: 0m37s
2023-04-01 00:34:09.200035: Epoch: 11, Batch: 60, Loss: 0.4453, Elapsed: 0m25s
2023-04-01 00:34:32.034659: Epoch: 11, Batch: 61, Loss: 0.4333, Elapsed: 0m22s
2023-04-01 00:35:11.450036: Epoch: 11, Batch: 62, Loss: 0.4609, Elapsed: 0m39s
2023-04-01 00:35:41.596306: Epoch: 11, Batch: 63, Loss: 0.4430, Elapsed: 0m30s
2023-04-01 00:36:12.188071: Epoch: 11, Batch: 64, Loss: 0.4542, Elapsed: 0m30s
2023-04-01 00:36:39.425286: Epoch: 11, Batch: 65, Loss: 0.4343, Elapsed: 0m27s
2023-04-01 00:37:21.203783: Epoch: 11, Batch: 66, Loss: 0.4573, Elapsed: 0m41s
2023-04-01 00:37:49.224390: Epoch: 11, Batch: 67, Loss: 0.4388, Elapsed: 0m28s
2023-04-01 00:38:11.097106: Epoch: 11, Batch: 68, Loss: 0.4346, Elapsed: 0m21s
2023-04-01 00:38:49.912416: Epoch: 11, Batch: 69, Loss: 0.4599, Elapsed: 0m38s
2023-04-01 00:39:15.331657: Epoch: 11, Batch: 70, Loss: 0.4398, Elapsed: 0m25s
2023-04-01 00:39:44.502847: Epoch: 11, Batch: 71, Loss: 0.4513, Elapsed: 0m29s
2023-04-01 00:40:05.509860: Epoch: 11, Batch: 72, Loss: 0.4078, Elapsed: 0m20s
2023-04-01 00:40:27.257155: Epoch: 11, Batch: 73, Loss: 0.4211, Elapsed: 0m21s
2023-04-01 00:40:56.279367: Epoch: 11, Batch: 74, Loss: 0.4336, Elapsed: 0m29s
2023-04-01 00:41:25.958599: Epoch: 11, Batch: 75, Loss: 0.4466, Elapsed: 0m29s
2023-04-01 00:41:46.600285: Epoch: 11, Batch: 76, Loss: 0.4272, Elapsed: 0m20s
2023-04-01 00:42:11.788334: Epoch: 11, Batch: 77, Loss: 0.4302, Elapsed: 0m25s
2023-04-01 00:42:50.832525: Epoch: 11, Batch: 78, Loss: 0.4528, Elapsed: 0m39s
2023-04-01 00:43:17.729731: Epoch: 11, Batch: 79, Loss: 0.4458, Elapsed: 0m26s
2023-04-01 00:43:53.792162: Epoch: 11, Batch: 80, Loss: 0.4661, Elapsed: 0m36s
2023-04-01 00:44:29.741496: Epoch: 12, Batch: 1, Loss: 0.4588, Elapsed: 0m35s
2023-04-01 00:44:48.847561: Epoch: 12, Batch: 2, Loss: 0.4211, Elapsed: 0m19s
2023-04-01 00:45:24.351452: Epoch: 12, Batch: 3, Loss: 0.4634, Elapsed: 0m35s
2023-04-01 00:45:49.800413: Epoch: 12, Batch: 4, Loss: 0.4418, Elapsed: 0m25s
2023-04-01 00:46:13.957181: Epoch: 12, Batch: 5, Loss: 0.4311, Elapsed: 0m24s
2023-04-01 00:46:44.818952: Epoch: 12, Batch: 6, Loss: 0.4369, Elapsed: 0m30s
2023-04-01 00:47:06.674578: Epoch: 12, Batch: 7, Loss: 0.4241, Elapsed: 0m21s
2023-04-01 00:47:42.920966: Epoch: 12, Batch: 8, Loss: 0.4579, Elapsed: 0m36s
2023-04-01 00:48:11.728066: Epoch: 12, Batch: 9, Loss: 0.4440, Elapsed: 0m28s
2023-04-01 00:48:35.343501: Epoch: 12, Batch: 10, Loss: 0.4220, Elapsed: 0m23s
2023-04-01 00:49:03.810542: Epoch: 12, Batch: 11, Loss: 0.4414, Elapsed: 0m28s
2023-04-01 00:49:34.777155: Epoch: 12, Batch: 12, Loss: 0.4468, Elapsed: 0m30s
2023-04-01 00:50:00.286593: Epoch: 12, Batch: 13, Loss: 0.4385, Elapsed: 0m25s
2023-04-01 00:50:28.995568: Epoch: 12, Batch: 14, Loss: 0.4343, Elapsed: 0m28s
2023-04-01 00:50:56.417053: Epoch: 12, Batch: 15, Loss: 0.4544, Elapsed: 0m27s
2023-04-01 00:51:30.026004: Epoch: 12, Batch: 16, Loss: 0.4540, Elapsed: 0m33s
2023-04-01 00:51:57.242194: Epoch: 12, Batch: 17, Loss: 0.4299, Elapsed: 0m27s
2023-04-01 00:52:19.579852: Epoch: 12, Batch: 18, Loss: 0.4324, Elapsed: 0m22s
2023-04-01 00:52:51.622507: Epoch: 12, Batch: 19, Loss: 0.4694, Elapsed: 0m32s
2023-04-01 00:53:15.120214: Epoch: 12, Batch: 20, Loss: 0.4506, Elapsed: 0m23s
2023-04-01 00:53:39.048586: Epoch: 12, Batch: 21, Loss: 0.4256, Elapsed: 0m23s
2023-04-01 00:54:00.020478: Epoch: 12, Batch: 22, Loss: 0.4154, Elapsed: 0m20s
2023-04-01 00:54:29.010670: Epoch: 12, Batch: 23, Loss: 0.4570, Elapsed: 0m28s
2023-04-01 00:54:58.346767: Epoch: 12, Batch: 24, Loss: 0.4495, Elapsed: 0m29s
2023-04-01 00:55:19.784766: Epoch: 12, Batch: 25, Loss: 0.4206, Elapsed: 0m21s
2023-04-01 00:55:42.191437: Epoch: 12, Batch: 26, Loss: 0.4260, Elapsed: 0m22s
2023-04-01 00:56:02.965911: Epoch: 12, Batch: 27, Loss: 0.4212, Elapsed: 0m20s
2023-04-01 00:56:30.353763: Epoch: 12, Batch: 28, Loss: 0.4394, Elapsed: 0m27s
2023-04-01 00:56:50.316034: Epoch: 12, Batch: 29, Loss: 0.4283, Elapsed: 0m19s
2023-04-01 00:57:28.241029: Epoch: 12, Batch: 30, Loss: 0.4813, Elapsed: 0m37s
2023-04-01 00:58:08.568507: Epoch: 12, Batch: 31, Loss: 0.4616, Elapsed: 0m40s
2023-04-01 00:58:34.788579: Epoch: 12, Batch: 32, Loss: 0.4451, Elapsed: 0m26s
2023-04-01 00:59:08.778050: Epoch: 12, Batch: 33, Loss: 0.4484, Elapsed: 0m33s
2023-04-01 00:59:47.575339: Epoch: 12, Batch: 34, Loss: 0.4602, Elapsed: 0m38s
2023-04-01 01:00:16.688742: Epoch: 12, Batch: 35, Loss: 0.4465, Elapsed: 0m29s
2023-04-01 01:00:54.653365: Epoch: 12, Batch: 36, Loss: 0.4601, Elapsed: 0m37s
2023-04-01 01:01:24.007281: Epoch: 12, Batch: 37, Loss: 0.4576, Elapsed: 0m29s
2023-04-01 01:01:49.682365: Epoch: 12, Batch: 38, Loss: 0.4197, Elapsed: 0m25s
2023-04-01 01:02:22.383543: Epoch: 12, Batch: 39, Loss: 0.4561, Elapsed: 0m32s
2023-04-01 01:02:47.184957: Epoch: 12, Batch: 40, Loss: 0.4285, Elapsed: 0m24s
2023-04-01 01:03:20.106046: Epoch: 12, Batch: 41, Loss: 0.4508, Elapsed: 0m32s
2023-04-01 01:04:04.380413: Epoch: 12, Batch: 42, Loss: 0.4473, Elapsed: 0m44s
2023-04-01 01:04:42.300111: Epoch: 12, Batch: 43, Loss: 0.4632, Elapsed: 0m37s
2023-04-01 01:05:08.329545: Epoch: 12, Batch: 44, Loss: 0.4377, Elapsed: 0m26s
2023-04-01 01:05:48.821696: Epoch: 12, Batch: 45, Loss: 0.4658, Elapsed: 0m40s
2023-04-01 01:06:30.706826: Epoch: 12, Batch: 46, Loss: 0.4533, Elapsed: 0m41s
2023-04-01 01:07:02.670995: Epoch: 12, Batch: 47, Loss: 0.4556, Elapsed: 0m31s
2023-04-01 01:07:31.756152: Epoch: 12, Batch: 48, Loss: 0.4333, Elapsed: 0m29s
2023-04-01 01:07:51.975078: Epoch: 12, Batch: 49, Loss: 0.4067, Elapsed: 0m20s
2023-04-01 01:08:31.971681: Epoch: 12, Batch: 50, Loss: 0.4490, Elapsed: 0m39s
2023-04-01 01:08:31.985834 Starting testing the valid set with 20 subgraphs!
2023-04-01 01:13:23.724311: validation Test:  Loss: 0.4415,  AUC: 0.8504, Acc: 77.9154,  Precision: 0.9095 -- Elapsed: 4m51s
2023-04-01 01:13:23.725493 Starting testing the train set with 20 subgraphs!
2023-04-01 01:32:40.389902: training Test:  Loss: 0.4407,  AUC: 0.8506, Acc: 77.9245,  Precision: 0.9139 -- Elapsed: 19m16s
2023-04-01 01:33:24.426847: Epoch: 12, Batch: 51, Loss: 0.4601, Elapsed: 0m44s
2023-04-01 01:33:52.673173: Epoch: 12, Batch: 52, Loss: 0.4510, Elapsed: 0m28s
2023-04-01 01:34:21.429884: Epoch: 12, Batch: 53, Loss: 0.4347, Elapsed: 0m28s
2023-04-01 01:35:00.763568: Epoch: 12, Batch: 54, Loss: 0.4569, Elapsed: 0m39s
2023-04-01 01:35:30.909768: Epoch: 12, Batch: 55, Loss: 0.4488, Elapsed: 0m30s
2023-04-01 01:35:57.348263: Epoch: 12, Batch: 56, Loss: 0.4324, Elapsed: 0m26s
2023-04-01 01:36:26.499581: Epoch: 12, Batch: 57, Loss: 0.4449, Elapsed: 0m29s
2023-04-01 01:36:48.778185: Epoch: 12, Batch: 58, Loss: 0.4254, Elapsed: 0m22s
2023-04-01 01:37:19.660886: Epoch: 12, Batch: 59, Loss: 0.4460, Elapsed: 0m30s
2023-04-01 01:37:46.491163: Epoch: 12, Batch: 60, Loss: 0.4474, Elapsed: 0m26s
2023-04-01 01:38:12.798449: Epoch: 12, Batch: 61, Loss: 0.4383, Elapsed: 0m26s
2023-04-01 01:38:42.296800: Epoch: 12, Batch: 62, Loss: 0.4307, Elapsed: 0m29s
2023-04-01 01:39:03.919208: Epoch: 12, Batch: 63, Loss: 0.4280, Elapsed: 0m21s
2023-04-01 01:39:41.003347: Epoch: 12, Batch: 64, Loss: 0.4540, Elapsed: 0m37s
2023-04-01 01:40:12.829606: Epoch: 12, Batch: 65, Loss: 0.4403, Elapsed: 0m31s
2023-04-01 01:40:46.410169: Epoch: 12, Batch: 66, Loss: 0.4500, Elapsed: 0m33s
2023-04-01 01:41:04.013385: Epoch: 12, Batch: 67, Loss: 0.3845, Elapsed: 0m17s
2023-04-01 01:41:33.153619: Epoch: 12, Batch: 68, Loss: 0.4361, Elapsed: 0m29s
2023-04-01 01:41:57.204010: Epoch: 12, Batch: 69, Loss: 0.4290, Elapsed: 0m24s
2023-04-01 01:42:40.613505: Epoch: 12, Batch: 70, Loss: 0.4568, Elapsed: 0m43s
2023-04-01 01:43:19.397724: Epoch: 12, Batch: 71, Loss: 0.4528, Elapsed: 0m38s
2023-04-01 01:43:45.308726: Epoch: 12, Batch: 72, Loss: 0.4410, Elapsed: 0m25s
2023-04-01 01:44:19.909170: Epoch: 12, Batch: 73, Loss: 0.4519, Elapsed: 0m34s
2023-04-01 01:44:35.297061: Epoch: 12, Batch: 74, Loss: 0.3873, Elapsed: 0m15s
2023-04-01 01:45:03.102925: Epoch: 12, Batch: 75, Loss: 0.4335, Elapsed: 0m27s
2023-04-01 01:45:28.862853: Epoch: 12, Batch: 76, Loss: 0.4337, Elapsed: 0m25s
2023-04-01 01:46:17.762408: Epoch: 12, Batch: 77, Loss: 0.4653, Elapsed: 0m48s
2023-04-01 01:46:46.823723: Epoch: 12, Batch: 78, Loss: 0.4274, Elapsed: 0m29s
2023-04-01 01:47:33.359002: Epoch: 12, Batch: 79, Loss: 0.4602, Elapsed: 0m46s
2023-04-01 01:48:00.455374: Epoch: 12, Batch: 80, Loss: 0.4433, Elapsed: 0m27s
2023-04-01 01:48:24.553361: Epoch: 13, Batch: 1, Loss: 0.4203, Elapsed: 0m24s
2023-04-01 01:48:49.825584: Epoch: 13, Batch: 2, Loss: 0.4158, Elapsed: 0m25s
2023-04-01 01:49:11.522803: Epoch: 13, Batch: 3, Loss: 0.4220, Elapsed: 0m21s
2023-04-01 01:49:49.571522: Epoch: 13, Batch: 4, Loss: 0.4528, Elapsed: 0m38s
2023-04-01 01:50:26.673926: Epoch: 13, Batch: 5, Loss: 0.4549, Elapsed: 0m37s
2023-04-01 01:51:00.174603: Epoch: 13, Batch: 6, Loss: 0.4391, Elapsed: 0m33s
2023-04-01 01:51:29.686887: Epoch: 13, Batch: 7, Loss: 0.4489, Elapsed: 0m29s
2023-04-01 01:51:58.760774: Epoch: 13, Batch: 8, Loss: 0.4322, Elapsed: 0m29s
2023-04-01 01:52:43.877084: Epoch: 13, Batch: 9, Loss: 0.4291, Elapsed: 0m45s
2023-04-01 01:53:12.357667: Epoch: 13, Batch: 10, Loss: 0.4295, Elapsed: 0m28s
2023-04-01 01:53:40.215956: Epoch: 13, Batch: 11, Loss: 0.4349, Elapsed: 0m27s
2023-04-01 01:54:15.381470: Epoch: 13, Batch: 12, Loss: 0.4529, Elapsed: 0m35s
2023-04-01 01:54:46.880458: Epoch: 13, Batch: 13, Loss: 0.4419, Elapsed: 0m31s
2023-04-01 01:55:10.215940: Epoch: 13, Batch: 14, Loss: 0.4328, Elapsed: 0m23s
2023-04-01 01:55:36.703686: Epoch: 13, Batch: 15, Loss: 0.4224, Elapsed: 0m26s
2023-04-01 01:56:15.912599: Epoch: 13, Batch: 16, Loss: 0.4534, Elapsed: 0m39s
2023-04-01 01:56:45.006266: Epoch: 13, Batch: 17, Loss: 0.4457, Elapsed: 0m29s
2023-04-01 01:57:14.455901: Epoch: 13, Batch: 18, Loss: 0.4450, Elapsed: 0m29s
2023-04-01 01:57:45.637209: Epoch: 13, Batch: 19, Loss: 0.4433, Elapsed: 0m31s
2023-04-01 01:58:11.484479: Epoch: 13, Batch: 20, Loss: 0.4333, Elapsed: 0m25s
2023-04-01 01:58:31.866905: Epoch: 13, Batch: 21, Loss: 0.4013, Elapsed: 0m20s
2023-04-01 01:59:01.726490: Epoch: 13, Batch: 22, Loss: 0.4295, Elapsed: 0m29s
2023-04-01 01:59:29.301108: Epoch: 13, Batch: 23, Loss: 0.4339, Elapsed: 0m27s
2023-04-01 02:00:03.392896: Epoch: 13, Batch: 24, Loss: 0.4488, Elapsed: 0m34s
2023-04-01 02:00:29.192067: Epoch: 13, Batch: 25, Loss: 0.4321, Elapsed: 0m25s
2023-04-01 02:00:58.890705: Epoch: 13, Batch: 26, Loss: 0.4302, Elapsed: 0m29s
2023-04-01 02:01:32.446811: Epoch: 13, Batch: 27, Loss: 0.4445, Elapsed: 0m33s
2023-04-01 02:01:48.352285: Epoch: 13, Batch: 28, Loss: 0.3802, Elapsed: 0m15s
2023-04-01 02:02:22.811002: Epoch: 13, Batch: 29, Loss: 0.4578, Elapsed: 0m34s
2023-04-01 02:02:45.187557: Epoch: 13, Batch: 30, Loss: 0.4326, Elapsed: 0m22s
2023-04-01 02:03:12.145044: Epoch: 13, Batch: 31, Loss: 0.4265, Elapsed: 0m26s
2023-04-01 02:03:38.102907: Epoch: 13, Batch: 32, Loss: 0.4351, Elapsed: 0m25s
2023-04-01 02:04:08.264855: Epoch: 13, Batch: 33, Loss: 0.4406, Elapsed: 0m30s
2023-04-01 02:04:34.773291: Epoch: 13, Batch: 34, Loss: 0.4298, Elapsed: 0m26s
2023-04-01 02:05:02.488609: Epoch: 13, Batch: 35, Loss: 0.4314, Elapsed: 0m27s
2023-04-01 02:05:23.870559: Epoch: 13, Batch: 36, Loss: 0.4235, Elapsed: 0m21s
2023-04-01 02:05:59.158231: Epoch: 13, Batch: 37, Loss: 0.4482, Elapsed: 0m35s
2023-04-01 02:06:39.302698: Epoch: 13, Batch: 38, Loss: 0.4592, Elapsed: 0m40s
2023-04-01 02:07:23.713276: Epoch: 13, Batch: 39, Loss: 0.4565, Elapsed: 0m44s
2023-04-01 02:08:01.791305: Epoch: 13, Batch: 40, Loss: 0.4456, Elapsed: 0m38s
2023-04-01 02:08:21.604356: Epoch: 13, Batch: 41, Loss: 0.4186, Elapsed: 0m19s
2023-04-01 02:08:44.517798: Epoch: 13, Batch: 42, Loss: 0.4338, Elapsed: 0m22s
2023-04-01 02:09:10.708381: Epoch: 13, Batch: 43, Loss: 0.4344, Elapsed: 0m26s
2023-04-01 02:09:41.655169: Epoch: 13, Batch: 44, Loss: 0.4444, Elapsed: 0m30s
2023-04-01 02:10:05.686692: Epoch: 13, Batch: 45, Loss: 0.4204, Elapsed: 0m24s
2023-04-01 02:10:32.218379: Epoch: 13, Batch: 46, Loss: 0.4366, Elapsed: 0m26s
2023-04-01 02:11:20.213153: Epoch: 13, Batch: 47, Loss: 0.4677, Elapsed: 0m47s
2023-04-01 02:11:48.455993: Epoch: 13, Batch: 48, Loss: 0.4342, Elapsed: 0m28s
2023-04-01 02:12:14.550792: Epoch: 13, Batch: 49, Loss: 0.4455, Elapsed: 0m26s
2023-04-01 02:12:44.565647: Epoch: 13, Batch: 50, Loss: 0.4356, Elapsed: 0m30s
2023-04-01 02:12:44.576976 Starting testing the valid set with 20 subgraphs!
2023-04-01 02:17:36.752563: validation Test:  Loss: 0.4439,  AUC: 0.8487, Acc: 77.8445,  Precision: 0.9394 -- Elapsed: 4m52s
2023-04-01 02:17:36.753749 Starting testing the train set with 20 subgraphs!
2023-04-01 02:36:56.753144: training Test:  Loss: 0.4421,  AUC: 0.8494, Acc: 77.9171,  Precision: 0.9427 -- Elapsed: 19m19s
2023-04-01 02:37:22.178563: Epoch: 13, Batch: 51, Loss: 0.4252, Elapsed: 0m25s
2023-04-01 02:37:52.314806: Epoch: 13, Batch: 52, Loss: 0.4283, Elapsed: 0m30s
2023-04-01 02:38:17.233425: Epoch: 13, Batch: 53, Loss: 0.4317, Elapsed: 0m24s
2023-04-01 02:38:41.205832: Epoch: 13, Batch: 54, Loss: 0.4294, Elapsed: 0m23s
2023-04-01 02:39:03.560369: Epoch: 13, Batch: 55, Loss: 0.4255, Elapsed: 0m22s
2023-04-01 02:39:32.604960: Epoch: 13, Batch: 56, Loss: 0.4468, Elapsed: 0m29s
2023-04-01 02:40:12.076189: Epoch: 13, Batch: 57, Loss: 0.4533, Elapsed: 0m39s
2023-04-01 02:40:49.695341: Epoch: 13, Batch: 58, Loss: 0.4782, Elapsed: 0m37s
2023-04-01 02:41:15.359299: Epoch: 13, Batch: 59, Loss: 0.4413, Elapsed: 0m25s
2023-04-01 02:41:51.704384: Epoch: 13, Batch: 60, Loss: 0.4535, Elapsed: 0m36s
2023-04-01 02:42:29.307355: Epoch: 13, Batch: 61, Loss: 0.4493, Elapsed: 0m37s
2023-04-01 02:42:51.796184: Epoch: 13, Batch: 62, Loss: 0.4166, Elapsed: 0m22s
2023-04-01 02:43:29.890648: Epoch: 13, Batch: 63, Loss: 0.4532, Elapsed: 0m38s
2023-04-01 02:43:50.956454: Epoch: 13, Batch: 64, Loss: 0.4049, Elapsed: 0m21s
2023-04-01 02:44:20.263624: Epoch: 13, Batch: 65, Loss: 0.4314, Elapsed: 0m29s
2023-04-01 02:44:38.449018: Epoch: 13, Batch: 66, Loss: 0.4016, Elapsed: 0m18s
2023-04-01 02:45:13.641480: Epoch: 13, Batch: 67, Loss: 0.4461, Elapsed: 0m35s
2023-04-01 02:46:01.779698: Epoch: 13, Batch: 68, Loss: 0.4621, Elapsed: 0m48s
2023-04-01 02:46:30.605042: Epoch: 13, Batch: 69, Loss: 0.4397, Elapsed: 0m28s
2023-04-01 02:47:09.992604: Epoch: 13, Batch: 70, Loss: 0.4506, Elapsed: 0m39s
2023-04-01 02:47:42.050883: Epoch: 13, Batch: 71, Loss: 0.4487, Elapsed: 0m32s
2023-04-01 02:48:12.170846: Epoch: 13, Batch: 72, Loss: 0.4412, Elapsed: 0m30s
2023-04-01 02:48:36.336145: Epoch: 13, Batch: 73, Loss: 0.4244, Elapsed: 0m24s
2023-04-01 02:49:08.694541: Epoch: 13, Batch: 74, Loss: 0.4421, Elapsed: 0m32s
2023-04-01 02:49:26.025086: Epoch: 13, Batch: 75, Loss: 0.4091, Elapsed: 0m17s
2023-04-01 02:50:09.093255: Epoch: 13, Batch: 76, Loss: 0.4547, Elapsed: 0m43s
2023-04-01 02:50:31.410137: Epoch: 13, Batch: 77, Loss: 0.4181, Elapsed: 0m22s
2023-04-01 02:51:05.698588: Epoch: 13, Batch: 78, Loss: 0.4516, Elapsed: 0m34s
2023-04-01 02:51:32.598281: Epoch: 13, Batch: 79, Loss: 0.4349, Elapsed: 0m26s
2023-04-01 02:52:13.887334: Epoch: 13, Batch: 80, Loss: 0.4526, Elapsed: 0m41s
2023-04-01 02:52:55.703198: Epoch: 14, Batch: 1, Loss: 0.4669, Elapsed: 0m41s
2023-04-01 02:53:32.030032: Epoch: 14, Batch: 2, Loss: 0.4610, Elapsed: 0m36s
2023-04-01 02:54:00.232399: Epoch: 14, Batch: 3, Loss: 0.4606, Elapsed: 0m28s
2023-04-01 02:54:24.372870: Epoch: 14, Batch: 4, Loss: 0.4191, Elapsed: 0m24s
2023-04-01 02:54:52.848009: Epoch: 14, Batch: 5, Loss: 0.4354, Elapsed: 0m28s
2023-04-01 02:55:27.173082: Epoch: 14, Batch: 6, Loss: 0.4444, Elapsed: 0m34s
2023-04-01 02:56:00.597488: Epoch: 14, Batch: 7, Loss: 0.4391, Elapsed: 0m33s
2023-04-01 02:56:20.307128: Epoch: 14, Batch: 8, Loss: 0.4045, Elapsed: 0m19s
2023-04-01 02:56:50.013190: Epoch: 14, Batch: 9, Loss: 0.4328, Elapsed: 0m29s
2023-04-01 02:57:18.112854: Epoch: 14, Batch: 10, Loss: 0.4306, Elapsed: 0m28s
2023-04-01 02:57:44.070656: Epoch: 14, Batch: 11, Loss: 0.4364, Elapsed: 0m25s
2023-04-01 02:58:18.082511: Epoch: 14, Batch: 12, Loss: 0.4485, Elapsed: 0m33s
2023-04-01 02:58:43.885818: Epoch: 14, Batch: 13, Loss: 0.4338, Elapsed: 0m25s
2023-04-01 02:59:19.367323: Epoch: 14, Batch: 14, Loss: 0.4706, Elapsed: 0m35s
2023-04-01 02:59:44.934476: Epoch: 14, Batch: 15, Loss: 0.4374, Elapsed: 0m25s
2023-04-01 03:00:13.578627: Epoch: 14, Batch: 16, Loss: 0.4468, Elapsed: 0m28s
2023-04-01 03:00:49.029047: Epoch: 14, Batch: 17, Loss: 0.4549, Elapsed: 0m35s
2023-04-01 03:01:16.999025: Epoch: 14, Batch: 18, Loss: 0.4439, Elapsed: 0m27s
2023-04-01 03:01:39.942550: Epoch: 14, Batch: 19, Loss: 0.4323, Elapsed: 0m22s
2023-04-01 03:02:05.937982: Epoch: 14, Batch: 20, Loss: 0.4390, Elapsed: 0m25s
2023-04-01 03:02:46.489345: Epoch: 14, Batch: 21, Loss: 0.4572, Elapsed: 0m40s
2023-04-01 03:03:12.416511: Epoch: 14, Batch: 22, Loss: 0.4282, Elapsed: 0m25s
2023-04-01 03:03:45.133917: Epoch: 14, Batch: 23, Loss: 0.4507, Elapsed: 0m32s
2023-04-01 03:04:23.799941: Epoch: 14, Batch: 24, Loss: 0.4511, Elapsed: 0m38s
2023-04-01 03:04:52.235013: Epoch: 14, Batch: 25, Loss: 0.4474, Elapsed: 0m28s
2023-04-01 03:05:21.714921: Epoch: 14, Batch: 26, Loss: 0.4409, Elapsed: 0m29s
2023-04-01 03:05:44.853047: Epoch: 14, Batch: 27, Loss: 0.4201, Elapsed: 0m23s
2023-04-01 03:06:04.744824: Epoch: 14, Batch: 28, Loss: 0.4216, Elapsed: 0m19s
2023-04-01 03:06:33.861724: Epoch: 14, Batch: 29, Loss: 0.4349, Elapsed: 0m29s
2023-04-01 03:06:54.599545: Epoch: 14, Batch: 30, Loss: 0.4146, Elapsed: 0m20s
2023-04-01 03:07:20.006251: Epoch: 14, Batch: 31, Loss: 0.4383, Elapsed: 0m25s
2023-04-01 03:07:58.243865: Epoch: 14, Batch: 32, Loss: 0.4507, Elapsed: 0m38s
2023-04-01 03:08:38.521202: Epoch: 14, Batch: 33, Loss: 0.4529, Elapsed: 0m40s
2023-04-01 03:09:02.380414: Epoch: 14, Batch: 34, Loss: 0.4243, Elapsed: 0m23s
2023-04-01 03:09:52.198325: Epoch: 14, Batch: 35, Loss: 0.4667, Elapsed: 0m49s
2023-04-01 03:10:23.780118: Epoch: 14, Batch: 36, Loss: 0.4480, Elapsed: 0m31s
2023-04-01 03:10:53.854843: Epoch: 14, Batch: 37, Loss: 0.4406, Elapsed: 0m29s
2023-04-01 03:11:30.456521: Epoch: 14, Batch: 38, Loss: 0.4535, Elapsed: 0m36s
2023-04-01 03:11:56.963909: Epoch: 14, Batch: 39, Loss: 0.4349, Elapsed: 0m26s
2023-04-01 03:12:19.122006: Epoch: 14, Batch: 40, Loss: 0.4161, Elapsed: 0m22s
2023-04-01 03:12:49.887002: Epoch: 14, Batch: 41, Loss: 0.4445, Elapsed: 0m30s
2023-04-01 03:13:10.831542: Epoch: 14, Batch: 42, Loss: 0.4032, Elapsed: 0m20s
2023-04-01 03:13:47.679263: Epoch: 14, Batch: 43, Loss: 0.4674, Elapsed: 0m36s
2023-04-01 03:14:17.105345: Epoch: 14, Batch: 44, Loss: 0.4434, Elapsed: 0m29s
2023-04-01 03:14:39.879122: Epoch: 14, Batch: 45, Loss: 0.4302, Elapsed: 0m22s
2023-04-01 03:15:06.937875: Epoch: 14, Batch: 46, Loss: 0.4363, Elapsed: 0m27s
2023-04-01 03:15:36.693505: Epoch: 14, Batch: 47, Loss: 0.4307, Elapsed: 0m29s
2023-04-01 03:16:14.408735: Epoch: 14, Batch: 48, Loss: 0.4573, Elapsed: 0m37s
2023-04-01 03:16:52.932075: Epoch: 14, Batch: 49, Loss: 0.4509, Elapsed: 0m38s
2023-04-01 03:17:14.685633: Epoch: 14, Batch: 50, Loss: 0.4049, Elapsed: 0m21s
2023-04-01 03:17:14.698963 Starting testing the valid set with 20 subgraphs!
2023-04-01 03:22:06.080135: validation Test:  Loss: 0.4394,  AUC: 0.8503, Acc: 78.1090,  Precision: 0.8836 -- Elapsed: 4m51s
2023-04-01 03:22:06.081783 Starting testing the train set with 20 subgraphs!
2023-04-01 03:41:23.738160: training Test:  Loss: 0.4379,  AUC: 0.8505, Acc: 78.2232,  Precision: 0.8957 -- Elapsed: 19m17s
2023-04-01 03:42:07.298705: Epoch: 14, Batch: 51, Loss: 0.4564, Elapsed: 0m43s
2023-04-01 03:42:34.720410: Epoch: 14, Batch: 52, Loss: 0.4240, Elapsed: 0m27s
2023-04-01 03:42:59.210869: Epoch: 14, Batch: 53, Loss: 0.4253, Elapsed: 0m24s
2023-04-01 03:43:22.058139: Epoch: 14, Batch: 54, Loss: 0.4196, Elapsed: 0m22s
2023-04-01 03:43:54.153763: Epoch: 14, Batch: 55, Loss: 0.4363, Elapsed: 0m32s
2023-04-01 03:44:23.265322: Epoch: 14, Batch: 56, Loss: 0.4260, Elapsed: 0m29s
2023-04-01 03:45:00.541157: Epoch: 14, Batch: 57, Loss: 0.4488, Elapsed: 0m37s
2023-04-01 03:45:28.469791: Epoch: 14, Batch: 58, Loss: 0.4197, Elapsed: 0m27s
2023-04-01 03:46:00.169800: Epoch: 14, Batch: 59, Loss: 0.4501, Elapsed: 0m31s
2023-04-01 03:46:30.363793: Epoch: 14, Batch: 60, Loss: 0.4530, Elapsed: 0m30s
2023-04-01 03:46:45.332411: Epoch: 14, Batch: 61, Loss: 0.3820, Elapsed: 0m14s
2023-04-01 03:47:15.420537: Epoch: 14, Batch: 62, Loss: 0.4303, Elapsed: 0m30s
2023-04-01 03:48:02.336483: Epoch: 14, Batch: 63, Loss: 0.4567, Elapsed: 0m46s
2023-04-01 03:48:24.909945: Epoch: 14, Batch: 64, Loss: 0.4282, Elapsed: 0m22s
2023-04-01 03:49:10.385217: Epoch: 14, Batch: 65, Loss: 0.4291, Elapsed: 0m45s
2023-04-01 03:49:40.565485: Epoch: 14, Batch: 66, Loss: 0.4261, Elapsed: 0m30s
2023-04-01 03:50:19.025311: Epoch: 14, Batch: 67, Loss: 0.4514, Elapsed: 0m38s
2023-04-01 03:50:52.849693: Epoch: 14, Batch: 68, Loss: 0.4479, Elapsed: 0m33s
2023-04-01 03:51:17.986121: Epoch: 14, Batch: 69, Loss: 0.4315, Elapsed: 0m25s
2023-04-01 03:51:46.119417: Epoch: 14, Batch: 70, Loss: 0.4348, Elapsed: 0m28s
2023-04-01 03:52:07.590691: Epoch: 14, Batch: 71, Loss: 0.4194, Elapsed: 0m21s
2023-04-01 03:52:34.763408: Epoch: 14, Batch: 72, Loss: 0.4451, Elapsed: 0m27s
2023-04-01 03:53:16.685024: Epoch: 14, Batch: 73, Loss: 0.4548, Elapsed: 0m41s
2023-04-01 03:53:33.765754: Epoch: 14, Batch: 74, Loss: 0.3852, Elapsed: 0m17s
2023-04-01 03:54:03.713876: Epoch: 14, Batch: 75, Loss: 0.4395, Elapsed: 0m29s
2023-04-01 03:54:22.520351: Epoch: 14, Batch: 76, Loss: 0.4056, Elapsed: 0m18s
2023-04-01 03:54:53.640680: Epoch: 14, Batch: 77, Loss: 0.4384, Elapsed: 0m31s
2023-04-01 03:55:19.163356: Epoch: 14, Batch: 78, Loss: 0.4196, Elapsed: 0m25s
2023-04-01 03:55:44.640170: Epoch: 14, Batch: 79, Loss: 0.4072, Elapsed: 0m25s
2023-04-01 03:56:30.896395: Epoch: 14, Batch: 80, Loss: 0.4617, Elapsed: 0m46s
2023-04-01 03:57:00.271563: Epoch: 15, Batch: 1, Loss: 0.4251, Elapsed: 0m29s
2023-04-01 03:57:51.002960: Epoch: 15, Batch: 2, Loss: 0.4617, Elapsed: 0m50s
2023-04-01 03:58:20.538616: Epoch: 15, Batch: 3, Loss: 0.4388, Elapsed: 0m29s
2023-04-01 03:58:42.556836: Epoch: 15, Batch: 4, Loss: 0.4203, Elapsed: 0m22s
2023-04-01 03:59:08.842431: Epoch: 15, Batch: 5, Loss: 0.4278, Elapsed: 0m26s
2023-04-01 03:59:41.806240: Epoch: 15, Batch: 6, Loss: 0.4467, Elapsed: 0m32s
2023-04-01 04:00:14.732723: Epoch: 15, Batch: 7, Loss: 0.4464, Elapsed: 0m32s
2023-04-01 04:00:40.147560: Epoch: 15, Batch: 8, Loss: 0.4182, Elapsed: 0m25s
2023-04-01 04:01:09.173514: Epoch: 15, Batch: 9, Loss: 0.4264, Elapsed: 0m29s
2023-04-01 04:01:49.767356: Epoch: 15, Batch: 10, Loss: 0.4514, Elapsed: 0m40s
2023-04-01 04:02:11.486034: Epoch: 15, Batch: 11, Loss: 0.4180, Elapsed: 0m21s
2023-04-01 04:02:40.282700: Epoch: 15, Batch: 12, Loss: 0.4351, Elapsed: 0m28s
2023-04-01 04:02:59.508390: Epoch: 15, Batch: 13, Loss: 0.4010, Elapsed: 0m19s
2023-04-01 04:03:26.756203: Epoch: 15, Batch: 14, Loss: 0.4326, Elapsed: 0m27s
2023-04-01 04:03:42.435629: Epoch: 15, Batch: 15, Loss: 0.3788, Elapsed: 0m15s
2023-04-01 04:04:19.746855: Epoch: 15, Batch: 16, Loss: 0.4596, Elapsed: 0m37s
2023-04-01 04:04:59.114250: Epoch: 15, Batch: 17, Loss: 0.4422, Elapsed: 0m39s
2023-04-01 04:05:35.679977: Epoch: 15, Batch: 18, Loss: 0.4455, Elapsed: 0m36s
2023-04-01 04:06:10.357581: Epoch: 15, Batch: 19, Loss: 0.4476, Elapsed: 0m34s
2023-04-01 04:06:37.611574: Epoch: 15, Batch: 20, Loss: 0.4266, Elapsed: 0m27s
2023-04-01 04:07:00.612387: Epoch: 15, Batch: 21, Loss: 0.4180, Elapsed: 0m22s
2023-04-01 04:07:30.098571: Epoch: 15, Batch: 22, Loss: 0.4399, Elapsed: 0m29s
2023-04-01 04:07:55.159850: Epoch: 15, Batch: 23, Loss: 0.4222, Elapsed: 0m25s
2023-04-01 04:08:33.900379: Epoch: 15, Batch: 24, Loss: 0.4553, Elapsed: 0m38s
2023-04-01 04:09:06.327734: Epoch: 15, Batch: 25, Loss: 0.4338, Elapsed: 0m32s
2023-04-01 04:09:51.512172: Epoch: 15, Batch: 26, Loss: 0.4296, Elapsed: 0m45s
2023-04-01 04:10:25.432567: Epoch: 15, Batch: 27, Loss: 0.4479, Elapsed: 0m33s
2023-04-01 04:11:06.701787: Epoch: 15, Batch: 28, Loss: 0.4488, Elapsed: 0m41s
2023-04-01 04:11:32.790984: Epoch: 15, Batch: 29, Loss: 0.4249, Elapsed: 0m26s
2023-04-01 04:11:57.861011: Epoch: 15, Batch: 30, Loss: 0.4055, Elapsed: 0m25s
2023-04-01 04:12:34.928168: Epoch: 15, Batch: 31, Loss: 0.4463, Elapsed: 0m37s
2023-04-01 04:13:04.302603: Epoch: 15, Batch: 32, Loss: 0.4281, Elapsed: 0m29s
2023-04-01 04:13:29.198373: Epoch: 15, Batch: 33, Loss: 0.4203, Elapsed: 0m24s
2023-04-01 04:13:56.314197: Epoch: 15, Batch: 34, Loss: 0.4474, Elapsed: 0m27s
2023-04-01 04:14:27.327329: Epoch: 15, Batch: 35, Loss: 0.4427, Elapsed: 0m30s
2023-04-01 04:15:06.940444: Epoch: 15, Batch: 36, Loss: 0.4499, Elapsed: 0m39s
2023-04-01 04:15:38.418487: Epoch: 15, Batch: 37, Loss: 0.4393, Elapsed: 0m31s
2023-04-01 04:16:15.748191: Epoch: 15, Batch: 38, Loss: 0.4475, Elapsed: 0m37s
2023-04-01 04:16:42.508637: Epoch: 15, Batch: 39, Loss: 0.4304, Elapsed: 0m26s
2023-04-01 04:16:59.763466: Epoch: 15, Batch: 40, Loss: 0.4080, Elapsed: 0m17s
2023-04-01 04:17:27.242139: Epoch: 15, Batch: 41, Loss: 0.4375, Elapsed: 0m27s
2023-04-01 04:17:48.751618: Epoch: 15, Batch: 42, Loss: 0.4210, Elapsed: 0m21s
2023-04-01 04:18:23.097268: Epoch: 15, Batch: 43, Loss: 0.4492, Elapsed: 0m34s
2023-04-01 04:18:49.365437: Epoch: 15, Batch: 44, Loss: 0.4299, Elapsed: 0m26s
2023-04-01 04:19:28.360806: Epoch: 15, Batch: 45, Loss: 0.4539, Elapsed: 0m38s
2023-04-01 04:20:05.685103: Epoch: 15, Batch: 46, Loss: 0.4527, Elapsed: 0m37s
2023-04-01 04:20:32.981735: Epoch: 15, Batch: 47, Loss: 0.4361, Elapsed: 0m27s
2023-04-01 04:20:59.082483: Epoch: 15, Batch: 48, Loss: 0.4322, Elapsed: 0m26s
2023-04-01 04:21:42.620275: Epoch: 15, Batch: 49, Loss: 0.4609, Elapsed: 0m43s
2023-04-01 04:22:06.957439: Epoch: 15, Batch: 50, Loss: 0.4161, Elapsed: 0m24s
2023-04-01 04:22:06.971700 Starting testing the valid set with 20 subgraphs!
2023-04-01 04:26:57.467725: validation Test:  Loss: 0.4435,  AUC: 0.8509, Acc: 77.8241,  Precision: 0.8354 -- Elapsed: 4m50s
2023-04-01 04:26:57.468895 Starting testing the train set with 20 subgraphs!
2023-04-01 04:46:18.460924: training Test:  Loss: 0.4411,  AUC: 0.8518, Acc: 77.9807,  Precision: 0.8396 -- Elapsed: 19m20s
2023-04-01 04:46:48.592549: Epoch: 15, Batch: 51, Loss: 0.4413, Elapsed: 0m30s
2023-04-01 04:47:09.097109: Epoch: 15, Batch: 52, Loss: 0.4194, Elapsed: 0m20s
2023-04-01 04:47:42.298889: Epoch: 15, Batch: 53, Loss: 0.4435, Elapsed: 0m33s
2023-04-01 04:48:29.451229: Epoch: 15, Batch: 54, Loss: 0.4623, Elapsed: 0m47s
2023-04-01 04:48:59.623551: Epoch: 15, Batch: 55, Loss: 0.4435, Elapsed: 0m30s
2023-04-01 04:49:25.447937: Epoch: 15, Batch: 56, Loss: 0.4193, Elapsed: 0m25s
2023-04-01 04:50:00.936116: Epoch: 15, Batch: 57, Loss: 0.4568, Elapsed: 0m35s
2023-04-01 04:50:31.151649: Epoch: 15, Batch: 58, Loss: 0.4316, Elapsed: 0m30s
2023-04-01 04:51:16.341135: Epoch: 15, Batch: 59, Loss: 0.4539, Elapsed: 0m45s
2023-04-01 04:51:36.747563: Epoch: 15, Batch: 60, Loss: 0.4028, Elapsed: 0m20s
2023-04-01 04:52:11.906212: Epoch: 15, Batch: 61, Loss: 0.4507, Elapsed: 0m35s
2023-04-01 04:52:43.150676: Epoch: 15, Batch: 62, Loss: 0.4402, Elapsed: 0m31s
2023-04-01 04:53:17.416363: Epoch: 15, Batch: 63, Loss: 0.4419, Elapsed: 0m34s
2023-04-01 04:53:47.194382: Epoch: 15, Batch: 64, Loss: 0.4269, Elapsed: 0m29s
2023-04-01 04:54:14.282607: Epoch: 15, Batch: 65, Loss: 0.4291, Elapsed: 0m27s
2023-04-01 04:54:40.760850: Epoch: 15, Batch: 66, Loss: 0.4350, Elapsed: 0m26s
2023-04-01 04:55:03.143043: Epoch: 15, Batch: 67, Loss: 0.4325, Elapsed: 0m22s
2023-04-01 04:55:23.199707: Epoch: 15, Batch: 68, Loss: 0.4014, Elapsed: 0m20s
2023-04-01 04:55:45.455771: Epoch: 15, Batch: 69, Loss: 0.4125, Elapsed: 0m22s
2023-04-01 04:56:22.104043: Epoch: 15, Batch: 70, Loss: 0.4515, Elapsed: 0m36s
2023-04-01 04:56:45.281747: Epoch: 15, Batch: 71, Loss: 0.4288, Elapsed: 0m23s
2023-04-01 04:57:14.587469: Epoch: 15, Batch: 72, Loss: 0.4424, Elapsed: 0m29s
2023-04-01 04:57:43.465967: Epoch: 15, Batch: 73, Loss: 0.4271, Elapsed: 0m28s
2023-04-01 04:58:07.275952: Epoch: 15, Batch: 74, Loss: 0.4140, Elapsed: 0m23s
2023-04-01 04:58:34.266618: Epoch: 15, Batch: 75, Loss: 0.4368, Elapsed: 0m26s
2023-04-01 04:59:02.740099: Epoch: 15, Batch: 76, Loss: 0.4354, Elapsed: 0m28s
2023-04-01 04:59:28.409380: Epoch: 15, Batch: 77, Loss: 0.4303, Elapsed: 0m25s
2023-04-01 04:59:50.644814: Epoch: 15, Batch: 78, Loss: 0.4084, Elapsed: 0m22s
2023-04-01 05:00:21.041449: Epoch: 15, Batch: 79, Loss: 0.4418, Elapsed: 0m30s
2023-04-01 05:00:48.744479: Epoch: 15, Batch: 80, Loss: 0.4248, Elapsed: 0m27s
2023-04-01 05:01:15.547691: Epoch: 16, Batch: 1, Loss: 0.4245, Elapsed: 0m26s
2023-04-01 05:01:45.830345: Epoch: 16, Batch: 2, Loss: 0.4427, Elapsed: 0m30s
2023-04-01 05:02:08.248204: Epoch: 16, Batch: 3, Loss: 0.4115, Elapsed: 0m22s
2023-04-01 05:02:35.075519: Epoch: 16, Batch: 4, Loss: 0.4267, Elapsed: 0m26s
2023-04-01 05:03:02.114181: Epoch: 16, Batch: 5, Loss: 0.4341, Elapsed: 0m27s
2023-04-01 05:03:26.561818: Epoch: 16, Batch: 6, Loss: 0.4209, Elapsed: 0m24s
2023-04-01 05:03:55.286002: Epoch: 16, Batch: 7, Loss: 0.4268, Elapsed: 0m28s
2023-04-01 05:04:16.148865: Epoch: 16, Batch: 8, Loss: 0.4015, Elapsed: 0m20s
2023-04-01 05:04:57.260812: Epoch: 16, Batch: 9, Loss: 0.4477, Elapsed: 0m41s
2023-04-01 05:05:15.208711: Epoch: 16, Batch: 10, Loss: 0.3775, Elapsed: 0m17s
2023-04-01 05:05:44.128860: Epoch: 16, Batch: 11, Loss: 0.4289, Elapsed: 0m28s
2023-04-01 05:06:23.174835: Epoch: 16, Batch: 12, Loss: 0.4467, Elapsed: 0m39s
2023-04-01 05:07:02.399739: Epoch: 16, Batch: 13, Loss: 0.4583, Elapsed: 0m39s
2023-04-01 05:07:29.522672: Epoch: 16, Batch: 14, Loss: 0.4167, Elapsed: 0m27s
2023-04-01 05:07:57.985571: Epoch: 16, Batch: 15, Loss: 0.4348, Elapsed: 0m28s
2023-04-01 05:08:43.271645: Epoch: 16, Batch: 16, Loss: 0.4283, Elapsed: 0m45s
2023-04-01 05:09:23.788590: Epoch: 16, Batch: 17, Loss: 0.4485, Elapsed: 0m40s
2023-04-01 05:10:02.562892: Epoch: 16, Batch: 18, Loss: 0.4479, Elapsed: 0m38s
2023-04-01 05:10:31.170284: Epoch: 16, Batch: 19, Loss: 0.4306, Elapsed: 0m28s
2023-04-01 05:10:53.675267: Epoch: 16, Batch: 20, Loss: 0.4191, Elapsed: 0m22s
2023-04-01 05:11:20.939750: Epoch: 16, Batch: 21, Loss: 0.4305, Elapsed: 0m27s
2023-04-01 05:11:57.373997: Epoch: 16, Batch: 22, Loss: 0.4493, Elapsed: 0m36s
2023-04-01 05:12:19.880197: Epoch: 16, Batch: 23, Loss: 0.4250, Elapsed: 0m22s
2023-04-01 05:12:46.257604: Epoch: 16, Batch: 24, Loss: 0.4299, Elapsed: 0m26s
2023-04-01 05:13:21.136991: Epoch: 16, Batch: 25, Loss: 0.4533, Elapsed: 0m34s
2023-04-01 05:13:54.065371: Epoch: 16, Batch: 26, Loss: 0.4399, Elapsed: 0m32s
2023-04-01 05:14:26.039233: Epoch: 16, Batch: 27, Loss: 0.4365, Elapsed: 0m31s
2023-04-01 05:14:54.501007: Epoch: 16, Batch: 28, Loss: 0.4467, Elapsed: 0m28s
2023-04-01 05:15:19.092533: Epoch: 16, Batch: 29, Loss: 0.4194, Elapsed: 0m24s
2023-04-01 05:16:08.134438: Epoch: 16, Batch: 30, Loss: 0.4626, Elapsed: 0m49s
2023-04-01 05:16:34.080099: Epoch: 16, Batch: 31, Loss: 0.4249, Elapsed: 0m25s
2023-04-01 05:17:04.089593: Epoch: 16, Batch: 32, Loss: 0.4274, Elapsed: 0m29s
2023-04-01 05:17:18.697822: Epoch: 16, Batch: 33, Loss: 0.3783, Elapsed: 0m14s
2023-04-01 05:17:45.732100: Epoch: 16, Batch: 34, Loss: 0.4251, Elapsed: 0m27s
2023-04-01 05:18:22.485199: Epoch: 16, Batch: 35, Loss: 0.4477, Elapsed: 0m36s
2023-04-01 05:18:56.284936: Epoch: 16, Batch: 36, Loss: 0.4416, Elapsed: 0m33s
2023-04-01 05:19:40.538276: Epoch: 16, Batch: 37, Loss: 0.4540, Elapsed: 0m44s
2023-04-01 05:20:05.104584: Epoch: 16, Batch: 38, Loss: 0.4180, Elapsed: 0m24s
2023-04-01 05:20:42.059315: Epoch: 16, Batch: 39, Loss: 0.4612, Elapsed: 0m36s
2023-04-01 05:21:06.794410: Epoch: 16, Batch: 40, Loss: 0.4030, Elapsed: 0m24s
2023-04-01 05:21:44.240587: Epoch: 16, Batch: 41, Loss: 0.4437, Elapsed: 0m37s
2023-04-01 05:22:06.637086: Epoch: 16, Batch: 42, Loss: 0.4314, Elapsed: 0m22s
2023-04-01 05:22:35.052706: Epoch: 16, Batch: 43, Loss: 0.4441, Elapsed: 0m28s
2023-04-01 05:23:00.371570: Epoch: 16, Batch: 44, Loss: 0.4290, Elapsed: 0m25s
2023-04-01 05:23:39.045224: Epoch: 16, Batch: 45, Loss: 0.4515, Elapsed: 0m38s
2023-04-01 05:24:04.888395: Epoch: 16, Batch: 46, Loss: 0.4287, Elapsed: 0m25s
2023-04-01 05:24:39.582319: Epoch: 16, Batch: 47, Loss: 0.4454, Elapsed: 0m34s
2023-04-01 05:25:04.850627: Epoch: 16, Batch: 48, Loss: 0.4260, Elapsed: 0m25s
2023-04-01 05:25:37.624275: Epoch: 16, Batch: 49, Loss: 0.4398, Elapsed: 0m32s
2023-04-01 05:26:04.752884: Epoch: 16, Batch: 50, Loss: 0.4312, Elapsed: 0m27s
2023-04-01 05:26:04.767890 Starting testing the valid set with 20 subgraphs!
2023-04-01 05:30:55.529575: validation Test:  Loss: 0.4347,  AUC: 0.8526, Acc: 78.3394,  Precision: 0.8920 -- Elapsed: 4m50s
2023-04-01 05:30:55.531022 Starting testing the train set with 20 subgraphs!
2023-04-01 05:50:10.020312: training Test:  Loss: 0.4335,  AUC: 0.8531, Acc: 78.4888,  Precision: 0.9031 -- Elapsed: 19m14s
2023-04-01 05:50:44.736583: Epoch: 16, Batch: 51, Loss: 0.4444, Elapsed: 0m34s
2023-04-01 05:51:14.128110: Epoch: 16, Batch: 52, Loss: 0.4362, Elapsed: 0m29s
2023-04-01 05:51:57.745813: Epoch: 16, Batch: 53, Loss: 0.4526, Elapsed: 0m43s
2023-04-01 05:52:32.221046: Epoch: 16, Batch: 54, Loss: 0.4438, Elapsed: 0m34s
2023-04-01 05:53:02.782457: Epoch: 16, Batch: 55, Loss: 0.4382, Elapsed: 0m30s
2023-04-01 05:53:38.807610: Epoch: 16, Batch: 56, Loss: 0.4458, Elapsed: 0m36s
2023-04-01 05:54:04.276014: Epoch: 16, Batch: 57, Loss: 0.4312, Elapsed: 0m25s
2023-04-01 05:54:35.197383: Epoch: 16, Batch: 58, Loss: 0.4281, Elapsed: 0m30s
2023-04-01 05:54:56.625556: Epoch: 16, Batch: 59, Loss: 0.4214, Elapsed: 0m21s
2023-04-01 05:55:35.875426: Epoch: 16, Batch: 60, Loss: 0.4507, Elapsed: 0m39s
2023-04-01 05:56:03.835178: Epoch: 16, Batch: 61, Loss: 0.4274, Elapsed: 0m27s
2023-04-01 05:56:31.296542: Epoch: 16, Batch: 62, Loss: 0.4368, Elapsed: 0m27s
2023-04-01 05:57:18.070660: Epoch: 16, Batch: 63, Loss: 0.4593, Elapsed: 0m46s
2023-04-01 05:57:41.376374: Epoch: 16, Batch: 64, Loss: 0.4237, Elapsed: 0m23s
2023-04-01 05:58:14.199223: Epoch: 16, Batch: 65, Loss: 0.4484, Elapsed: 0m32s
2023-04-01 05:58:43.308217: Epoch: 16, Batch: 66, Loss: 0.4250, Elapsed: 0m29s
2023-04-01 05:59:10.545612: Epoch: 16, Batch: 67, Loss: 0.4286, Elapsed: 0m27s
2023-04-01 05:59:40.334183: Epoch: 16, Batch: 68, Loss: 0.4369, Elapsed: 0m29s
2023-04-01 06:00:05.184880: Epoch: 16, Batch: 69, Loss: 0.4099, Elapsed: 0m24s
2023-04-01 06:00:25.220583: Epoch: 16, Batch: 70, Loss: 0.4169, Elapsed: 0m20s
2023-04-01 06:00:46.900722: Epoch: 16, Batch: 71, Loss: 0.3985, Elapsed: 0m21s
2023-04-01 06:01:06.716793: Epoch: 16, Batch: 72, Loss: 0.3980, Elapsed: 0m19s
2023-04-01 06:01:36.893106: Epoch: 16, Batch: 73, Loss: 0.4310, Elapsed: 0m30s
2023-04-01 06:02:00.303165: Epoch: 16, Batch: 74, Loss: 0.4097, Elapsed: 0m23s
2023-04-01 06:02:21.206378: Epoch: 16, Batch: 75, Loss: 0.3963, Elapsed: 0m20s
2023-04-01 06:02:59.560616: Epoch: 16, Batch: 76, Loss: 0.4488, Elapsed: 0m38s
2023-04-01 06:03:28.970353: Epoch: 16, Batch: 77, Loss: 0.4410, Elapsed: 0m29s
2023-04-01 06:03:49.615292: Epoch: 16, Batch: 78, Loss: 0.4055, Elapsed: 0m20s
2023-04-01 06:04:22.470129: Epoch: 16, Batch: 79, Loss: 0.4444, Elapsed: 0m32s
2023-04-01 06:04:51.925695: Epoch: 16, Batch: 80, Loss: 0.4295, Elapsed: 0m29s
2023-04-01 06:05:16.201534: Epoch: 17, Batch: 1, Loss: 0.4177, Elapsed: 0m24s
2023-04-01 06:05:36.037021: Epoch: 17, Batch: 2, Loss: 0.3954, Elapsed: 0m19s
2023-04-01 06:06:03.063528: Epoch: 17, Batch: 3, Loss: 0.4298, Elapsed: 0m27s
2023-04-01 06:06:34.031990: Epoch: 17, Batch: 4, Loss: 0.4365, Elapsed: 0m30s
2023-04-01 06:07:00.622709: Epoch: 17, Batch: 5, Loss: 0.4260, Elapsed: 0m26s
2023-04-01 06:07:37.170632: Epoch: 17, Batch: 6, Loss: 0.4472, Elapsed: 0m36s
2023-04-01 06:07:57.180580: Epoch: 17, Batch: 7, Loss: 0.3982, Elapsed: 0m19s
2023-04-01 06:08:22.495021: Epoch: 17, Batch: 8, Loss: 0.4033, Elapsed: 0m25s
2023-04-01 06:09:07.164902: Epoch: 17, Batch: 9, Loss: 0.4514, Elapsed: 0m44s
2023-04-01 06:09:38.152329: Epoch: 17, Batch: 10, Loss: 0.4363, Elapsed: 0m30s
2023-04-01 06:10:15.133068: Epoch: 17, Batch: 11, Loss: 0.4480, Elapsed: 0m36s
2023-04-01 06:10:45.224457: Epoch: 17, Batch: 12, Loss: 0.4318, Elapsed: 0m30s
2023-04-01 06:11:31.596374: Epoch: 17, Batch: 13, Loss: 0.4572, Elapsed: 0m46s
2023-04-01 06:12:20.255631: Epoch: 17, Batch: 14, Loss: 0.4584, Elapsed: 0m48s
2023-04-01 06:12:56.952771: Epoch: 17, Batch: 15, Loss: 0.4513, Elapsed: 0m36s
2023-04-01 06:13:28.428594: Epoch: 17, Batch: 16, Loss: 0.4330, Elapsed: 0m31s
2023-04-01 06:14:05.437182: Epoch: 17, Batch: 17, Loss: 0.4465, Elapsed: 0m36s
2023-04-01 06:14:34.303386: Epoch: 17, Batch: 18, Loss: 0.4245, Elapsed: 0m28s
2023-04-01 06:14:56.759513: Epoch: 17, Batch: 19, Loss: 0.4160, Elapsed: 0m22s
2023-04-01 06:15:33.206429: Epoch: 17, Batch: 20, Loss: 0.4439, Elapsed: 0m36s
2023-04-01 06:16:08.103713: Epoch: 17, Batch: 21, Loss: 0.4421, Elapsed: 0m34s
2023-04-01 06:16:45.726818: Epoch: 17, Batch: 22, Loss: 0.4483, Elapsed: 0m37s
2023-04-01 06:17:13.404832: Epoch: 17, Batch: 23, Loss: 0.4281, Elapsed: 0m27s
2023-04-01 06:17:36.310472: Epoch: 17, Batch: 24, Loss: 0.4157, Elapsed: 0m22s
2023-04-01 06:18:21.308671: Epoch: 17, Batch: 25, Loss: 0.4309, Elapsed: 0m44s
2023-04-01 06:18:43.989173: Epoch: 17, Batch: 26, Loss: 0.4217, Elapsed: 0m22s
2023-04-01 06:19:05.144683: Epoch: 17, Batch: 27, Loss: 0.4067, Elapsed: 0m21s
2023-04-01 06:19:45.319178: Epoch: 17, Batch: 28, Loss: 0.4514, Elapsed: 0m40s
2023-04-01 06:20:16.564740: Epoch: 17, Batch: 29, Loss: 0.4440, Elapsed: 0m31s
2023-04-01 06:20:46.224572: Epoch: 17, Batch: 30, Loss: 0.4321, Elapsed: 0m29s
2023-04-01 06:21:12.505322: Epoch: 17, Batch: 31, Loss: 0.4297, Elapsed: 0m26s
2023-04-01 06:21:40.565016: Epoch: 17, Batch: 32, Loss: 0.4390, Elapsed: 0m28s
2023-04-01 06:22:13.084313: Epoch: 17, Batch: 33, Loss: 0.4379, Elapsed: 0m32s
2023-04-01 06:22:43.417451: Epoch: 17, Batch: 34, Loss: 0.4400, Elapsed: 0m30s
2023-04-01 06:23:07.779725: Epoch: 17, Batch: 35, Loss: 0.4084, Elapsed: 0m24s
2023-04-01 06:23:34.692945: Epoch: 17, Batch: 36, Loss: 0.4210, Elapsed: 0m26s
2023-04-01 06:24:12.579348: Epoch: 17, Batch: 37, Loss: 0.4435, Elapsed: 0m37s
2023-04-01 06:24:40.325590: Epoch: 17, Batch: 38, Loss: 0.4433, Elapsed: 0m27s
2023-04-01 06:25:06.101227: Epoch: 17, Batch: 39, Loss: 0.4158, Elapsed: 0m25s
2023-04-01 06:25:31.500070: Epoch: 17, Batch: 40, Loss: 0.4184, Elapsed: 0m25s
2023-04-01 06:25:58.615244: Epoch: 17, Batch: 41, Loss: 0.4293, Elapsed: 0m27s
2023-04-01 06:26:37.041103: Epoch: 17, Batch: 42, Loss: 0.4490, Elapsed: 0m38s
2023-04-01 06:27:00.504108: Epoch: 17, Batch: 43, Loss: 0.4104, Elapsed: 0m23s
2023-04-01 06:27:38.501479: Epoch: 17, Batch: 44, Loss: 0.4496, Elapsed: 0m37s
2023-04-01 06:28:11.733679: Epoch: 17, Batch: 45, Loss: 0.4385, Elapsed: 0m33s
2023-04-01 06:28:33.222225: Epoch: 17, Batch: 46, Loss: 0.3969, Elapsed: 0m21s
2023-04-01 06:29:01.532175: Epoch: 17, Batch: 47, Loss: 0.4275, Elapsed: 0m28s
2023-04-01 06:29:31.212307: Epoch: 17, Batch: 48, Loss: 0.4228, Elapsed: 0m29s
2023-04-01 06:30:02.936210: Epoch: 17, Batch: 49, Loss: 0.4433, Elapsed: 0m31s
2023-04-01 06:30:22.724657: Epoch: 17, Batch: 50, Loss: 0.3939, Elapsed: 0m19s
2023-04-01 06:30:22.736957 Starting testing the valid set with 20 subgraphs!
2023-04-01 06:35:15.680170: validation Test:  Loss: 0.4338,  AUC: 0.8544, Acc: 78.3536,  Precision: 0.8618 -- Elapsed: 4m52s
2023-04-01 06:35:15.681657 Starting testing the train set with 20 subgraphs!
2023-04-01 06:54:36.198973: training Test:  Loss: 0.4320,  AUC: 0.8549, Acc: 78.5544,  Precision: 0.8659 -- Elapsed: 19m20s
2023-04-01 06:55:06.440012: Epoch: 17, Batch: 51, Loss: 0.4392, Elapsed: 0m30s
2023-04-01 06:55:30.875919: Epoch: 17, Batch: 52, Loss: 0.4229, Elapsed: 0m24s
2023-04-01 06:56:01.803571: Epoch: 17, Batch: 53, Loss: 0.4231, Elapsed: 0m30s
2023-04-01 06:56:31.374601: Epoch: 17, Batch: 54, Loss: 0.4380, Elapsed: 0m29s
2023-04-01 06:56:56.637458: Epoch: 17, Batch: 55, Loss: 0.4221, Elapsed: 0m25s
2023-04-01 06:57:42.087887: Epoch: 17, Batch: 56, Loss: 0.4523, Elapsed: 0m45s
2023-04-01 06:58:09.455091: Epoch: 17, Batch: 57, Loss: 0.4176, Elapsed: 0m27s
2023-04-01 06:58:36.707990: Epoch: 17, Batch: 58, Loss: 0.4279, Elapsed: 0m27s
2023-04-01 06:59:12.365538: Epoch: 17, Batch: 59, Loss: 0.4465, Elapsed: 0m35s
2023-04-01 06:59:38.525558: Epoch: 17, Batch: 60, Loss: 0.4220, Elapsed: 0m26s
2023-04-01 07:00:16.293178: Epoch: 17, Batch: 61, Loss: 0.4467, Elapsed: 0m37s
2023-04-01 07:00:42.523869: Epoch: 17, Batch: 62, Loss: 0.4248, Elapsed: 0m26s
2023-04-01 07:01:09.807010: Epoch: 17, Batch: 63, Loss: 0.4271, Elapsed: 0m27s
2023-04-01 07:01:47.478669: Epoch: 17, Batch: 64, Loss: 0.4595, Elapsed: 0m37s
2023-04-01 07:02:29.120921: Epoch: 17, Batch: 65, Loss: 0.4483, Elapsed: 0m41s
2023-04-01 07:02:58.841261: Epoch: 17, Batch: 66, Loss: 0.4287, Elapsed: 0m29s
2023-04-01 07:03:24.000987: Epoch: 17, Batch: 67, Loss: 0.4273, Elapsed: 0m25s
2023-04-01 07:03:57.056793: Epoch: 17, Batch: 68, Loss: 0.4307, Elapsed: 0m33s
2023-04-01 07:04:18.853614: Epoch: 17, Batch: 69, Loss: 0.4138, Elapsed: 0m21s
2023-04-01 07:04:48.292769: Epoch: 17, Batch: 70, Loss: 0.4234, Elapsed: 0m29s
2023-04-01 07:05:10.715195: Epoch: 17, Batch: 71, Loss: 0.4229, Elapsed: 0m22s
2023-04-01 07:05:43.970716: Epoch: 17, Batch: 72, Loss: 0.4430, Elapsed: 0m33s
2023-04-01 07:06:13.343687: Epoch: 17, Batch: 73, Loss: 0.4395, Elapsed: 0m29s
2023-04-01 07:06:37.372005: Epoch: 17, Batch: 74, Loss: 0.4194, Elapsed: 0m24s
2023-04-01 07:07:19.289558: Epoch: 17, Batch: 75, Loss: 0.4439, Elapsed: 0m41s
2023-04-01 07:07:41.153174: Epoch: 17, Batch: 76, Loss: 0.4069, Elapsed: 0m21s
2023-04-01 07:07:58.975978: Epoch: 17, Batch: 77, Loss: 0.3728, Elapsed: 0m17s
2023-04-01 07:08:18.689503: Epoch: 17, Batch: 78, Loss: 0.4103, Elapsed: 0m19s
2023-04-01 07:08:47.839182: Epoch: 17, Batch: 79, Loss: 0.4277, Elapsed: 0m29s
2023-04-01 07:09:02.846535: Epoch: 17, Batch: 80, Loss: 0.3776, Elapsed: 0m14s
2023-04-01 07:09:31.895698: Epoch: 18, Batch: 1, Loss: 0.4232, Elapsed: 0m29s
2023-04-01 07:09:52.871549: Epoch: 18, Batch: 2, Loss: 0.4103, Elapsed: 0m20s
2023-04-01 07:10:17.104871: Epoch: 18, Batch: 3, Loss: 0.4255, Elapsed: 0m24s
2023-04-01 07:10:34.646576: Epoch: 18, Batch: 4, Loss: 0.3746, Elapsed: 0m17s
2023-04-01 07:11:04.751832: Epoch: 18, Batch: 5, Loss: 0.4322, Elapsed: 0m30s
2023-04-01 07:11:34.777868: Epoch: 18, Batch: 6, Loss: 0.4393, Elapsed: 0m30s
2023-04-01 07:12:04.149521: Epoch: 18, Batch: 7, Loss: 0.4360, Elapsed: 0m29s
2023-04-01 07:12:41.011350: Epoch: 18, Batch: 8, Loss: 0.4488, Elapsed: 0m36s
2023-04-01 07:13:11.478989: Epoch: 18, Batch: 9, Loss: 0.4318, Elapsed: 0m30s
2023-04-01 07:13:39.834029: Epoch: 18, Batch: 10, Loss: 0.4276, Elapsed: 0m28s
2023-04-01 07:14:07.715730: Epoch: 18, Batch: 11, Loss: 0.4239, Elapsed: 0m27s
2023-04-01 07:14:40.987527: Epoch: 18, Batch: 12, Loss: 0.4393, Elapsed: 0m33s
2023-04-01 07:15:17.335878: Epoch: 18, Batch: 13, Loss: 0.4488, Elapsed: 0m36s
2023-04-01 07:15:55.139871: Epoch: 18, Batch: 14, Loss: 0.4447, Elapsed: 0m37s
2023-04-01 07:16:33.775612: Epoch: 18, Batch: 15, Loss: 0.4463, Elapsed: 0m38s
2023-04-01 07:16:57.865186: Epoch: 18, Batch: 16, Loss: 0.4200, Elapsed: 0m24s
2023-04-01 07:17:27.695215: Epoch: 18, Batch: 17, Loss: 0.4298, Elapsed: 0m29s
2023-04-01 07:17:54.000100: Epoch: 18, Batch: 18, Loss: 0.4229, Elapsed: 0m26s
2023-04-01 07:18:20.012602: Epoch: 18, Batch: 19, Loss: 0.4256, Elapsed: 0m25s
2023-04-01 07:18:45.564203: Epoch: 18, Batch: 20, Loss: 0.4209, Elapsed: 0m25s
2023-04-01 07:19:10.989339: Epoch: 18, Batch: 21, Loss: 0.4310, Elapsed: 0m25s
2023-04-01 07:19:51.542283: Epoch: 18, Batch: 22, Loss: 0.4567, Elapsed: 0m40s
2023-04-01 07:20:32.930246: Epoch: 18, Batch: 23, Loss: 0.4626, Elapsed: 0m41s
2023-04-01 07:20:53.263850: Epoch: 18, Batch: 24, Loss: 0.4157, Elapsed: 0m20s
2023-04-01 07:21:29.576338: Epoch: 18, Batch: 25, Loss: 0.4470, Elapsed: 0m36s
2023-04-01 07:22:09.680641: Epoch: 18, Batch: 26, Loss: 0.4578, Elapsed: 0m40s
2023-04-01 07:22:24.682286: Epoch: 18, Batch: 27, Loss: 0.3837, Elapsed: 0m14s
2023-04-01 07:23:10.180789: Epoch: 18, Batch: 28, Loss: 0.4372, Elapsed: 0m45s
2023-04-01 07:23:35.395454: Epoch: 18, Batch: 29, Loss: 0.4123, Elapsed: 0m25s
2023-04-01 07:23:54.612247: Epoch: 18, Batch: 30, Loss: 0.4046, Elapsed: 0m19s
2023-04-01 07:24:22.017822: Epoch: 18, Batch: 31, Loss: 0.4310, Elapsed: 0m27s
2023-04-01 07:24:59.677756: Epoch: 18, Batch: 32, Loss: 0.5008, Elapsed: 0m37s
2023-04-01 07:25:19.895654: Epoch: 18, Batch: 33, Loss: 0.4006, Elapsed: 0m20s
2023-04-01 07:25:49.047495: Epoch: 18, Batch: 34, Loss: 0.4270, Elapsed: 0m29s
2023-04-01 07:26:21.879645: Epoch: 18, Batch: 35, Loss: 0.4449, Elapsed: 0m32s
2023-04-01 07:26:52.987326: Epoch: 18, Batch: 36, Loss: 0.4381, Elapsed: 0m31s
2023-04-01 07:27:22.664164: Epoch: 18, Batch: 37, Loss: 0.4397, Elapsed: 0m29s
2023-04-01 07:27:50.197120: Epoch: 18, Batch: 38, Loss: 0.4344, Elapsed: 0m27s
2023-04-01 07:28:25.274987: Epoch: 18, Batch: 39, Loss: 0.4416, Elapsed: 0m35s
2023-04-01 07:28:46.817528: Epoch: 18, Batch: 40, Loss: 0.4158, Elapsed: 0m21s
2023-04-01 07:29:09.625924: Epoch: 18, Batch: 41, Loss: 0.4335, Elapsed: 0m22s
2023-04-01 07:29:38.076722: Epoch: 18, Batch: 42, Loss: 0.4394, Elapsed: 0m28s
2023-04-01 07:30:03.766835: Epoch: 18, Batch: 43, Loss: 0.4307, Elapsed: 0m25s
2023-04-01 07:30:54.528516: Epoch: 18, Batch: 44, Loss: 0.4595, Elapsed: 0m50s
2023-04-01 07:31:30.932543: Epoch: 18, Batch: 45, Loss: 0.4463, Elapsed: 0m36s
2023-04-01 07:31:57.414251: Epoch: 18, Batch: 46, Loss: 0.4233, Elapsed: 0m26s
2023-04-01 07:32:17.313695: Epoch: 18, Batch: 47, Loss: 0.4004, Elapsed: 0m19s
2023-04-01 07:32:50.770176: Epoch: 18, Batch: 48, Loss: 0.4422, Elapsed: 0m33s
2023-04-01 07:33:27.404049: Epoch: 18, Batch: 49, Loss: 0.4478, Elapsed: 0m36s
2023-04-01 07:33:57.681575: Epoch: 18, Batch: 50, Loss: 0.4387, Elapsed: 0m30s
2023-04-01 07:33:57.692355 Starting testing the valid set with 20 subgraphs!
2023-04-01 07:38:46.940292: validation Test:  Loss: 0.4377,  AUC: 0.8522, Acc: 78.1606,  Precision: 0.9189 -- Elapsed: 4m49s
2023-04-01 07:38:46.941476 Starting testing the train set with 20 subgraphs!
2023-04-01 07:58:09.154526: training Test:  Loss: 0.4354,  AUC: 0.8529, Acc: 78.3144,  Precision: 0.9061 -- Elapsed: 19m22s
2023-04-01 07:58:37.963022: Epoch: 18, Batch: 51, Loss: 0.4330, Elapsed: 0m28s
2023-04-01 07:59:08.551651: Epoch: 18, Batch: 52, Loss: 0.4389, Elapsed: 0m30s
2023-04-01 07:59:30.915208: Epoch: 18, Batch: 53, Loss: 0.4213, Elapsed: 0m22s
2023-04-01 07:59:57.718068: Epoch: 18, Batch: 54, Loss: 0.4256, Elapsed: 0m26s
2023-04-01 08:00:21.356706: Epoch: 18, Batch: 55, Loss: 0.4116, Elapsed: 0m23s
2023-04-01 08:00:44.969579: Epoch: 18, Batch: 56, Loss: 0.4185, Elapsed: 0m23s
2023-04-01 08:01:27.654037: Epoch: 18, Batch: 57, Loss: 0.4482, Elapsed: 0m42s
2023-04-01 08:01:54.391942: Epoch: 18, Batch: 58, Loss: 0.4312, Elapsed: 0m26s
2023-04-01 08:02:23.679031: Epoch: 18, Batch: 59, Loss: 0.4446, Elapsed: 0m29s
2023-04-01 08:02:44.893572: Epoch: 18, Batch: 60, Loss: 0.3983, Elapsed: 0m21s
2023-04-01 08:03:13.403463: Epoch: 18, Batch: 61, Loss: 0.4387, Elapsed: 0m28s
2023-04-01 08:03:39.964412: Epoch: 18, Batch: 62, Loss: 0.4273, Elapsed: 0m26s
2023-04-01 08:04:09.065624: Epoch: 18, Batch: 63, Loss: 0.4232, Elapsed: 0m29s
2023-04-01 08:04:40.857517: Epoch: 18, Batch: 64, Loss: 0.4463, Elapsed: 0m31s
2023-04-01 08:05:25.342550: Epoch: 18, Batch: 65, Loss: 0.4547, Elapsed: 0m44s
2023-04-01 08:06:04.906368: Epoch: 18, Batch: 66, Loss: 0.4479, Elapsed: 0m39s
2023-04-01 08:06:29.114062: Epoch: 18, Batch: 67, Loss: 0.4190, Elapsed: 0m24s
2023-04-01 08:07:02.485927: Epoch: 18, Batch: 68, Loss: 0.4432, Elapsed: 0m33s
2023-04-01 08:07:48.576625: Epoch: 18, Batch: 69, Loss: 0.4585, Elapsed: 0m46s
2023-04-01 08:08:21.093729: Epoch: 18, Batch: 70, Loss: 0.4455, Elapsed: 0m32s
2023-04-01 08:08:48.208696: Epoch: 18, Batch: 71, Loss: 0.4306, Elapsed: 0m27s
2023-04-01 08:09:13.956119: Epoch: 18, Batch: 72, Loss: 0.4299, Elapsed: 0m25s
2023-04-01 08:09:54.132308: Epoch: 18, Batch: 73, Loss: 0.4463, Elapsed: 0m40s
2023-04-01 08:10:30.450688: Epoch: 18, Batch: 74, Loss: 0.4456, Elapsed: 0m36s
2023-04-01 08:10:53.416640: Epoch: 18, Batch: 75, Loss: 0.4090, Elapsed: 0m22s
2023-04-01 08:11:17.088284: Epoch: 18, Batch: 76, Loss: 0.4167, Elapsed: 0m23s
2023-04-01 08:11:45.969762: Epoch: 18, Batch: 77, Loss: 0.4445, Elapsed: 0m28s
2023-04-01 08:12:15.006337: Epoch: 18, Batch: 78, Loss: 0.4299, Elapsed: 0m29s
2023-04-01 08:12:37.803722: Epoch: 18, Batch: 79, Loss: 0.4228, Elapsed: 0m22s
2023-04-01 08:13:13.361604: Epoch: 18, Batch: 80, Loss: 0.4479, Elapsed: 0m35s
2023-04-01 08:13:43.793226: Epoch: 19, Batch: 1, Loss: 0.4280, Elapsed: 0m30s
2023-04-01 08:14:21.222091: Epoch: 19, Batch: 2, Loss: 0.4529, Elapsed: 0m37s
2023-04-01 08:14:50.393887: Epoch: 19, Batch: 3, Loss: 0.4298, Elapsed: 0m29s
2023-04-01 08:15:26.575452: Epoch: 19, Batch: 4, Loss: 0.4459, Elapsed: 0m36s
2023-04-01 08:15:45.965495: Epoch: 19, Batch: 5, Loss: 0.3963, Elapsed: 0m19s
2023-04-01 08:16:11.542161: Epoch: 19, Batch: 6, Loss: 0.4284, Elapsed: 0m25s
2023-04-01 08:16:49.443447: Epoch: 19, Batch: 7, Loss: 0.4485, Elapsed: 0m37s
2023-04-01 08:17:23.182219: Epoch: 19, Batch: 8, Loss: 0.4410, Elapsed: 0m33s
2023-04-01 08:18:00.759329: Epoch: 19, Batch: 9, Loss: 0.4619, Elapsed: 0m37s
2023-04-01 08:18:29.022900: Epoch: 19, Batch: 10, Loss: 0.4434, Elapsed: 0m28s
2023-04-01 08:19:04.941517: Epoch: 19, Batch: 11, Loss: 0.4459, Elapsed: 0m35s
2023-04-01 08:19:42.430626: Epoch: 19, Batch: 12, Loss: 0.4542, Elapsed: 0m37s
2023-04-01 08:20:07.557111: Epoch: 19, Batch: 13, Loss: 0.4108, Elapsed: 0m25s
2023-04-01 08:20:30.065837: Epoch: 19, Batch: 14, Loss: 0.4238, Elapsed: 0m22s
2023-04-01 08:20:54.652488: Epoch: 19, Batch: 15, Loss: 0.4222, Elapsed: 0m24s
2023-04-01 08:21:21.354235: Epoch: 19, Batch: 16, Loss: 0.4288, Elapsed: 0m26s
2023-04-01 08:21:46.224467: Epoch: 19, Batch: 17, Loss: 0.4031, Elapsed: 0m24s
2023-04-01 08:22:01.116387: Epoch: 19, Batch: 18, Loss: 0.3801, Elapsed: 0m14s
2023-04-01 08:22:27.669860: Epoch: 19, Batch: 19, Loss: 0.4301, Elapsed: 0m26s
2023-04-01 08:22:55.054175: Epoch: 19, Batch: 20, Loss: 0.4266, Elapsed: 0m27s
2023-04-01 08:23:16.475757: Epoch: 19, Batch: 21, Loss: 0.4092, Elapsed: 0m21s
2023-04-01 08:23:46.237846: Epoch: 19, Batch: 22, Loss: 0.4282, Elapsed: 0m29s
2023-04-01 08:24:09.822478: Epoch: 19, Batch: 23, Loss: 0.4140, Elapsed: 0m23s
2023-04-01 08:24:36.388879: Epoch: 19, Batch: 24, Loss: 0.4250, Elapsed: 0m26s
2023-04-01 08:24:58.261436: Epoch: 19, Batch: 25, Loss: 0.4094, Elapsed: 0m21s
2023-04-01 08:25:24.053301: Epoch: 19, Batch: 26, Loss: 0.4182, Elapsed: 0m25s
2023-04-01 08:25:53.275965: Epoch: 19, Batch: 27, Loss: 0.4332, Elapsed: 0m29s
2023-04-01 08:26:32.907748: Epoch: 19, Batch: 28, Loss: 0.4449, Elapsed: 0m39s
2023-04-01 08:27:05.206008: Epoch: 19, Batch: 29, Loss: 0.4363, Elapsed: 0m32s
2023-04-01 08:27:34.638942: Epoch: 19, Batch: 30, Loss: 0.4263, Elapsed: 0m29s
2023-04-01 08:28:20.309542: Epoch: 19, Batch: 31, Loss: 0.4376, Elapsed: 0m45s
2023-04-01 08:28:51.335761: Epoch: 19, Batch: 32, Loss: 0.4361, Elapsed: 0m31s
2023-04-01 08:29:30.235620: Epoch: 19, Batch: 33, Loss: 0.4498, Elapsed: 0m38s
2023-04-01 08:29:52.514218: Epoch: 19, Batch: 34, Loss: 0.4153, Elapsed: 0m22s
2023-04-01 08:30:12.532013: Epoch: 19, Batch: 35, Loss: 0.3992, Elapsed: 0m20s
2023-04-01 08:30:38.781649: Epoch: 19, Batch: 36, Loss: 0.4135, Elapsed: 0m26s
2023-04-01 08:31:14.590759: Epoch: 19, Batch: 37, Loss: 0.4456, Elapsed: 0m35s
2023-04-01 08:31:44.724866: Epoch: 19, Batch: 38, Loss: 0.4308, Elapsed: 0m30s
2023-04-01 08:32:09.502855: Epoch: 19, Batch: 39, Loss: 0.4212, Elapsed: 0m24s
2023-04-01 08:32:31.460348: Epoch: 19, Batch: 40, Loss: 0.4173, Elapsed: 0m21s
2023-04-01 08:33:21.859687: Epoch: 19, Batch: 41, Loss: 0.4566, Elapsed: 0m50s
2023-04-01 08:33:42.061704: Epoch: 19, Batch: 42, Loss: 0.4120, Elapsed: 0m20s
2023-04-01 08:34:27.335782: Epoch: 19, Batch: 43, Loss: 0.4562, Elapsed: 0m45s
2023-04-01 08:35:09.504854: Epoch: 19, Batch: 44, Loss: 0.4497, Elapsed: 0m42s
2023-04-01 08:35:36.492226: Epoch: 19, Batch: 45, Loss: 0.4393, Elapsed: 0m26s
2023-04-01 08:36:02.723947: Epoch: 19, Batch: 46, Loss: 0.4230, Elapsed: 0m26s
2023-04-01 08:36:29.899568: Epoch: 19, Batch: 47, Loss: 0.4496, Elapsed: 0m27s
2023-04-01 08:37:05.629249: Epoch: 19, Batch: 48, Loss: 0.4538, Elapsed: 0m35s
2023-04-01 08:37:34.591067: Epoch: 19, Batch: 49, Loss: 0.4293, Elapsed: 0m28s
2023-04-01 08:37:57.753830: Epoch: 19, Batch: 50, Loss: 0.4219, Elapsed: 0m23s
2023-04-01 08:37:57.767157 Starting testing the valid set with 20 subgraphs!
2023-04-01 08:42:47.827956: validation Test:  Loss: 0.4342,  AUC: 0.8546, Acc: 78.4449,  Precision: 0.8943 -- Elapsed: 4m50s
2023-04-01 08:42:47.829188 Starting testing the train set with 20 subgraphs!
2023-04-01 09:02:10.291350: training Test:  Loss: 0.4324,  AUC: 0.8553, Acc: 78.5215,  Precision: 0.9035 -- Elapsed: 19m22s
2023-04-01 09:02:38.266274: Epoch: 19, Batch: 51, Loss: 0.4236, Elapsed: 0m27s
2023-04-01 09:03:10.524699: Epoch: 19, Batch: 52, Loss: 0.4433, Elapsed: 0m32s
2023-04-01 09:03:40.541939: Epoch: 19, Batch: 53, Loss: 0.4429, Elapsed: 0m30s
2023-04-01 09:04:02.023048: Epoch: 19, Batch: 54, Loss: 0.3959, Elapsed: 0m21s
2023-04-01 09:04:35.057382: Epoch: 19, Batch: 55, Loss: 0.4330, Elapsed: 0m33s
2023-04-01 09:04:52.631971: Epoch: 19, Batch: 56, Loss: 0.3803, Elapsed: 0m17s
2023-04-01 09:05:14.400411: Epoch: 19, Batch: 57, Loss: 0.4203, Elapsed: 0m21s
2023-04-01 09:05:44.121893: Epoch: 19, Batch: 58, Loss: 0.4337, Elapsed: 0m29s
2023-04-01 09:06:16.833546: Epoch: 19, Batch: 59, Loss: 0.4318, Elapsed: 0m32s
2023-04-01 09:06:55.554377: Epoch: 19, Batch: 60, Loss: 0.4476, Elapsed: 0m38s
2023-04-01 09:07:40.096968: Epoch: 19, Batch: 61, Loss: 0.4552, Elapsed: 0m44s
2023-04-01 09:08:12.666298: Epoch: 19, Batch: 62, Loss: 0.4478, Elapsed: 0m32s
2023-04-01 09:08:42.026269: Epoch: 19, Batch: 63, Loss: 0.4351, Elapsed: 0m29s
2023-04-01 09:09:10.780078: Epoch: 19, Batch: 64, Loss: 0.4338, Elapsed: 0m28s
2023-04-01 09:09:45.791092: Epoch: 19, Batch: 65, Loss: 0.4406, Elapsed: 0m34s
2023-04-01 09:10:18.765098: Epoch: 19, Batch: 66, Loss: 0.4413, Elapsed: 0m32s
2023-04-01 09:10:47.954085: Epoch: 19, Batch: 67, Loss: 0.4276, Elapsed: 0m29s
2023-04-01 09:11:18.482866: Epoch: 19, Batch: 68, Loss: 0.4371, Elapsed: 0m30s
2023-04-01 09:11:43.138219: Epoch: 19, Batch: 69, Loss: 0.4195, Elapsed: 0m24s
2023-04-01 09:12:18.862961: Epoch: 19, Batch: 70, Loss: 0.4408, Elapsed: 0m35s
2023-04-01 09:12:44.940813: Epoch: 19, Batch: 71, Loss: 0.4208, Elapsed: 0m26s
2023-04-01 09:13:09.917026: Epoch: 19, Batch: 72, Loss: 0.4164, Elapsed: 0m24s
2023-04-01 09:13:48.946864: Epoch: 19, Batch: 73, Loss: 0.4407, Elapsed: 0m39s
2023-04-01 09:14:27.530557: Epoch: 19, Batch: 74, Loss: 0.4421, Elapsed: 0m38s
2023-04-01 09:14:47.661762: Epoch: 19, Batch: 75, Loss: 0.3984, Elapsed: 0m20s
2023-04-01 09:15:19.053794: Epoch: 19, Batch: 76, Loss: 0.4236, Elapsed: 0m31s
2023-04-01 09:15:48.025439: Epoch: 19, Batch: 77, Loss: 0.4236, Elapsed: 0m28s
2023-04-01 09:16:14.784767: Epoch: 19, Batch: 78, Loss: 0.4228, Elapsed: 0m26s
2023-04-01 09:16:56.028355: Epoch: 19, Batch: 79, Loss: 0.4424, Elapsed: 0m41s
2023-04-01 09:17:20.424516: Epoch: 19, Batch: 80, Loss: 0.4142, Elapsed: 0m24s
2023-04-01 09:18:00.255678: Epoch: 20, Batch: 1, Loss: 0.4424, Elapsed: 0m39s
2023-04-01 09:18:20.974477: Epoch: 20, Batch: 2, Loss: 0.4113, Elapsed: 0m20s
2023-04-01 09:18:46.260878: Epoch: 20, Batch: 3, Loss: 0.4239, Elapsed: 0m25s
2023-04-01 09:19:14.867867: Epoch: 20, Batch: 4, Loss: 0.4227, Elapsed: 0m28s
2023-04-01 09:19:42.816787: Epoch: 20, Batch: 5, Loss: 0.4278, Elapsed: 0m27s
2023-04-01 09:20:19.433945: Epoch: 20, Batch: 6, Loss: 0.4477, Elapsed: 0m36s
2023-04-01 09:20:44.030575: Epoch: 20, Batch: 7, Loss: 0.4163, Elapsed: 0m24s
2023-04-01 09:21:11.354630: Epoch: 20, Batch: 8, Loss: 0.4414, Elapsed: 0m27s
2023-04-01 09:21:40.554981: Epoch: 20, Batch: 9, Loss: 0.4214, Elapsed: 0m29s
2023-04-01 09:22:09.856568: Epoch: 20, Batch: 10, Loss: 0.4383, Elapsed: 0m29s
2023-04-01 09:22:33.908060: Epoch: 20, Batch: 11, Loss: 0.4149, Elapsed: 0m24s
2023-04-01 09:23:03.010698: Epoch: 20, Batch: 12, Loss: 0.4236, Elapsed: 0m29s
2023-04-01 09:23:32.127967: Epoch: 20, Batch: 13, Loss: 0.4231, Elapsed: 0m29s
2023-04-01 09:23:57.315624: Epoch: 20, Batch: 14, Loss: 0.4253, Elapsed: 0m25s
2023-04-01 09:24:30.188750: Epoch: 20, Batch: 15, Loss: 0.4393, Elapsed: 0m32s
2023-04-01 09:25:03.612901: Epoch: 20, Batch: 16, Loss: 0.4301, Elapsed: 0m33s
2023-04-01 09:25:35.541205: Epoch: 20, Batch: 17, Loss: 0.4419, Elapsed: 0m31s
2023-04-01 09:26:01.969568: Epoch: 20, Batch: 18, Loss: 0.4288, Elapsed: 0m26s
2023-04-01 09:26:37.606643: Epoch: 20, Batch: 19, Loss: 0.4417, Elapsed: 0m35s
2023-04-01 09:27:04.862011: Epoch: 20, Batch: 20, Loss: 0.4234, Elapsed: 0m27s
2023-04-01 09:27:45.939492: Epoch: 20, Batch: 21, Loss: 0.4479, Elapsed: 0m41s
2023-04-01 09:28:25.439896: Epoch: 20, Batch: 22, Loss: 0.4452, Elapsed: 0m39s
2023-04-01 09:28:47.815205: Epoch: 20, Batch: 23, Loss: 0.4098, Elapsed: 0m22s
2023-04-01 09:29:22.871541: Epoch: 20, Batch: 24, Loss: 0.4525, Elapsed: 0m35s
2023-04-01 09:29:45.693001: Epoch: 20, Batch: 25, Loss: 0.4187, Elapsed: 0m22s
2023-04-01 09:30:18.821563: Epoch: 20, Batch: 26, Loss: 0.4371, Elapsed: 0m33s
2023-04-01 09:30:43.556169: Epoch: 20, Batch: 27, Loss: 0.4282, Elapsed: 0m24s
2023-04-01 09:31:14.581013: Epoch: 20, Batch: 28, Loss: 0.4359, Elapsed: 0m31s
2023-04-01 09:31:47.741084: Epoch: 20, Batch: 29, Loss: 0.4438, Elapsed: 0m33s
2023-04-01 09:32:15.384216: Epoch: 20, Batch: 30, Loss: 0.4242, Elapsed: 0m27s
2023-04-01 09:32:38.408000: Epoch: 20, Batch: 31, Loss: 0.4283, Elapsed: 0m23s
2023-04-01 09:33:08.370539: Epoch: 20, Batch: 32, Loss: 0.4361, Elapsed: 0m29s
2023-04-01 09:33:45.967852: Epoch: 20, Batch: 33, Loss: 0.4466, Elapsed: 0m37s
2023-04-01 09:34:11.428196: Epoch: 20, Batch: 34, Loss: 0.4270, Elapsed: 0m25s
2023-04-01 09:34:50.603084: Epoch: 20, Batch: 35, Loss: 0.4481, Elapsed: 0m39s
2023-04-01 09:35:16.925072: Epoch: 20, Batch: 36, Loss: 0.4337, Elapsed: 0m26s
2023-04-01 09:35:43.390284: Epoch: 20, Batch: 37, Loss: 0.4278, Elapsed: 0m26s
2023-04-01 09:36:17.084080: Epoch: 20, Batch: 38, Loss: 0.4407, Elapsed: 0m33s
2023-04-01 09:36:37.573757: Epoch: 20, Batch: 39, Loss: 0.4102, Elapsed: 0m20s
2023-04-01 09:37:16.051076: Epoch: 20, Batch: 40, Loss: 0.4471, Elapsed: 0m38s
2023-04-01 09:37:39.203089: Epoch: 20, Batch: 41, Loss: 0.4149, Elapsed: 0m23s
2023-04-01 09:38:05.938600: Epoch: 20, Batch: 42, Loss: 0.4350, Elapsed: 0m26s
2023-04-01 09:38:32.342477: Epoch: 20, Batch: 43, Loss: 0.4266, Elapsed: 0m26s
2023-04-01 09:38:52.148186: Epoch: 20, Batch: 44, Loss: 0.4064, Elapsed: 0m19s
2023-04-01 09:39:30.748634: Epoch: 20, Batch: 45, Loss: 0.4445, Elapsed: 0m38s
2023-04-01 09:40:04.231519: Epoch: 20, Batch: 46, Loss: 0.4323, Elapsed: 0m33s
2023-04-01 09:40:25.936528: Epoch: 20, Batch: 47, Loss: 0.3985, Elapsed: 0m21s
2023-04-01 09:40:55.585939: Epoch: 20, Batch: 48, Loss: 0.4399, Elapsed: 0m29s
2023-04-01 09:41:36.211908: Epoch: 20, Batch: 49, Loss: 0.4560, Elapsed: 0m40s
2023-04-01 09:41:58.066820: Epoch: 20, Batch: 50, Loss: 0.4196, Elapsed: 0m21s
2023-04-01 09:41:58.079904 Starting testing the valid set with 20 subgraphs!
2023-04-01 09:46:51.465620: validation Test:  Loss: 0.4401,  AUC: 0.8515, Acc: 77.6470,  Precision: 0.8171 -- Elapsed: 4m53s
2023-04-01 09:46:51.467420 Starting testing the train set with 20 subgraphs!
2023-04-01 10:06:11.219404: training Test:  Loss: 0.4369,  AUC: 0.8522, Acc: 78.0362,  Precision: 0.8338 -- Elapsed: 19m19s
2023-04-01 10:06:45.507521: Epoch: 20, Batch: 51, Loss: 0.4478, Elapsed: 0m34s
2023-04-01 10:07:10.099988: Epoch: 20, Batch: 52, Loss: 0.4109, Elapsed: 0m24s
2023-04-01 10:07:34.114343: Epoch: 20, Batch: 53, Loss: 0.4085, Elapsed: 0m23s
2023-04-01 10:08:03.847214: Epoch: 20, Batch: 54, Loss: 0.4356, Elapsed: 0m29s
2023-04-01 10:08:29.152105: Epoch: 20, Batch: 55, Loss: 0.4044, Elapsed: 0m25s
2023-04-01 10:08:53.063769: Epoch: 20, Batch: 56, Loss: 0.4196, Elapsed: 0m23s
2023-04-01 10:09:18.203098: Epoch: 20, Batch: 57, Loss: 0.4182, Elapsed: 0m25s
2023-04-01 10:10:03.182310: Epoch: 20, Batch: 58, Loss: 0.4525, Elapsed: 0m44s
2023-04-01 10:10:20.897435: Epoch: 20, Batch: 59, Loss: 0.3748, Elapsed: 0m17s
2023-04-01 10:11:05.889626: Epoch: 20, Batch: 60, Loss: 0.4550, Elapsed: 0m44s
2023-04-01 10:11:50.596029: Epoch: 20, Batch: 61, Loss: 0.4432, Elapsed: 0m44s
2023-04-01 10:12:29.279362: Epoch: 20, Batch: 62, Loss: 0.4419, Elapsed: 0m38s
2023-04-01 10:12:55.547036: Epoch: 20, Batch: 63, Loss: 0.4126, Elapsed: 0m26s
2023-04-01 10:13:14.833749: Epoch: 20, Batch: 64, Loss: 0.3905, Elapsed: 0m19s
2023-04-01 10:13:34.545831: Epoch: 20, Batch: 65, Loss: 0.3945, Elapsed: 0m19s
2023-04-01 10:14:03.001724: Epoch: 20, Batch: 66, Loss: 0.4255, Elapsed: 0m28s
2023-04-01 10:14:32.713208: Epoch: 20, Batch: 67, Loss: 0.4228, Elapsed: 0m29s
2023-04-01 10:14:55.796034: Epoch: 20, Batch: 68, Loss: 0.4206, Elapsed: 0m23s
2023-04-01 10:15:24.819061: Epoch: 20, Batch: 69, Loss: 0.4284, Elapsed: 0m29s
2023-04-01 10:15:52.110898: Epoch: 20, Batch: 70, Loss: 0.4415, Elapsed: 0m27s
2023-04-01 10:16:19.956600: Epoch: 20, Batch: 71, Loss: 0.4310, Elapsed: 0m27s
2023-04-01 10:17:07.852433: Epoch: 20, Batch: 72, Loss: 0.4580, Elapsed: 0m47s
2023-04-01 10:17:44.368207: Epoch: 20, Batch: 73, Loss: 0.4558, Elapsed: 0m36s
2023-04-01 10:18:13.518072: Epoch: 20, Batch: 74, Loss: 0.4390, Elapsed: 0m29s
2023-04-01 10:18:50.484476: Epoch: 20, Batch: 75, Loss: 0.4410, Elapsed: 0m36s
2023-04-01 10:19:27.229114: Epoch: 20, Batch: 76, Loss: 0.4441, Elapsed: 0m36s
2023-04-01 10:20:02.847284: Epoch: 20, Batch: 77, Loss: 0.4477, Elapsed: 0m35s
2023-04-01 10:20:30.726598: Epoch: 20, Batch: 78, Loss: 0.4312, Elapsed: 0m27s
2023-04-01 10:21:01.143108: Epoch: 20, Batch: 79, Loss: 0.4311, Elapsed: 0m30s
2023-04-01 10:21:15.681073: Epoch: 20, Batch: 80, Loss: 0.3771, Elapsed: 0m14s
2023-04-01 10:21:15.694013: Training completed!
