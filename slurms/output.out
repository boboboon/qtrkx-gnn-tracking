Printing configs: 
train_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_new
valid_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/valid_new
dataset: TuysuzPaper
log_dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/
run_type: new_run
gpu: -1
n_files: 100
n_valid: 20
n_train: 80
batch_size: 1
lr_c: 0.01
n_iters: 3
n_epoch: 20
TEST_every: 50
hid_dim: 4
network: QGNN
optimizer: Adam
loss_func: BinaryCrossentropy
n_thread: 4
log_verbosity: 2
EN_qc: {'PQC_id': '10', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}
NN_qc: {'PQC_id': '10', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}
Log dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/
Training data input dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_new
Validation data input dir: /home/xzcaplcu/repo/qtrkx-gnn-tracking/data_personal/train_new
2023-01-22 20:59:29.104197 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_validation.csv
2023-01-22 20:59:29.104382 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_training.csv
2023-01-22 20:59:29.104577 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/summary.csv
2023-01-22 20:59:29.104768 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_0.csv
2023-01-22 20:59:29.104955 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_1.csv
2023-01-22 20:59:29.105172 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_2.csv
2023-01-22 20:59:29.105339 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_3.csv
2023-01-22 20:59:29.105531 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_4.csv
2023-01-22 20:59:29.105720 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_5.csv
2023-01-22 20:59:29.105904 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_6.csv
2023-01-22 20:59:29.106092 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_7.csv
2023-01-22 20:59:29.106271 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_8.csv
2023-01-22 20:59:29.106487 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_9.csv
2023-01-22 20:59:29.106680 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_10.csv
2023-01-22 20:59:29.106865 Deleted old log: /home/xzcaplcu/repo/qtrkx-gnn-tracking/logs/test_QGNN/run 1/log_parameters_11.csv
Model: "GNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
InputNet (Dense)             multiple                  16        
_________________________________________________________________
EdgeNet (EdgeNet)            multiple                  81        
_________________________________________________________________
NodeNet (NodeNet)            multiple                  124       
=================================================================
Total params: 221
Trainable params: 221
Non-trainable params: 0
_________________________________________________________________
None
2023-01-22 20:59:31.916076 Starting testing the valid set with 20 subgraphs!
2023-01-22 20:59:49.879258: validation Test:  Loss: 0.7831,  AUC: 0.5263, Acc: 45.7074,  Precision: 0.6135 -- Elapsed: 0m17s
2023-01-22 20:59:49.879305 Starting testing the train set with 20 subgraphs!
2023-01-22 21:01:28.639742: training Test:  Loss: 0.7504,  AUC: 0.5274, Acc: 50.0749,  Precision: 0.5547 -- Elapsed: 1m38s
2023-01-22 21:01:28.641511: Training is starting!
2023-01-22 21:01:32.696782: Epoch: 1, Batch: 1, Loss: 0.7438, Elapsed: 0m4s
2023-01-22 21:01:34.559268: Epoch: 1, Batch: 2, Loss: 0.7635, Elapsed: 0m1s
2023-01-22 21:01:36.678198: Epoch: 1, Batch: 3, Loss: 0.7252, Elapsed: 0m2s
2023-01-22 21:01:37.980744: Epoch: 1, Batch: 4, Loss: 0.7658, Elapsed: 0m1s
2023-01-22 21:01:40.073991: Epoch: 1, Batch: 5, Loss: 0.6896, Elapsed: 0m2s
2023-01-22 21:01:42.256488: Epoch: 1, Batch: 6, Loss: 0.6814, Elapsed: 0m2s
2023-01-22 21:01:45.211274: Epoch: 1, Batch: 7, Loss: 0.6639, Elapsed: 0m2s
2023-01-22 21:01:48.209672: Epoch: 1, Batch: 8, Loss: 0.6853, Elapsed: 0m2s
2023-01-22 21:01:52.791880: Epoch: 1, Batch: 9, Loss: 0.6695, Elapsed: 0m4s
2023-01-22 21:01:55.769865: Epoch: 1, Batch: 10, Loss: 0.6628, Elapsed: 0m2s
2023-01-22 21:02:00.185789: Epoch: 1, Batch: 11, Loss: 0.6688, Elapsed: 0m4s
2023-01-22 21:02:03.032261: Epoch: 1, Batch: 12, Loss: 0.6778, Elapsed: 0m2s
2023-01-22 21:02:08.496087: Epoch: 1, Batch: 13, Loss: 0.6607, Elapsed: 0m5s
2023-01-22 21:02:10.653612: Epoch: 1, Batch: 14, Loss: 0.6678, Elapsed: 0m2s
2023-01-22 21:02:14.147411: Epoch: 1, Batch: 15, Loss: 0.6755, Elapsed: 0m3s
2023-01-22 21:02:16.634922: Epoch: 1, Batch: 16, Loss: 0.6377, Elapsed: 0m2s
2023-01-22 21:02:18.019359: Epoch: 1, Batch: 17, Loss: 0.6478, Elapsed: 0m1s
2023-01-22 21:02:19.952283: Epoch: 1, Batch: 18, Loss: 0.6452, Elapsed: 0m1s
2023-01-22 21:02:21.464394: Epoch: 1, Batch: 19, Loss: 0.6697, Elapsed: 0m1s
2023-01-22 21:02:23.307947: Epoch: 1, Batch: 20, Loss: 0.6438, Elapsed: 0m1s
2023-01-22 21:02:24.990650: Epoch: 1, Batch: 21, Loss: 0.6529, Elapsed: 0m1s
2023-01-22 21:02:26.804574: Epoch: 1, Batch: 22, Loss: 0.6281, Elapsed: 0m1s
2023-01-22 21:02:28.205228: Epoch: 1, Batch: 23, Loss: 0.6349, Elapsed: 0m1s
2023-01-22 21:02:29.244651: Epoch: 1, Batch: 24, Loss: 0.6000, Elapsed: 0m1s
2023-01-22 21:02:31.207461: Epoch: 1, Batch: 25, Loss: 0.6428, Elapsed: 0m1s
2023-01-22 21:02:33.239552: Epoch: 1, Batch: 26, Loss: 0.6503, Elapsed: 0m2s
2023-01-22 21:02:35.898163: Epoch: 1, Batch: 27, Loss: 0.6438, Elapsed: 0m2s
2023-01-22 21:02:38.359434: Epoch: 1, Batch: 28, Loss: 0.6310, Elapsed: 0m2s
2023-01-22 21:02:39.267026: Epoch: 1, Batch: 29, Loss: 0.6227, Elapsed: 0m0s
2023-01-22 21:02:42.773138: Epoch: 1, Batch: 30, Loss: 0.6527, Elapsed: 0m3s
2023-01-22 21:02:44.617901: Epoch: 1, Batch: 31, Loss: 0.6224, Elapsed: 0m1s
2023-01-22 21:02:46.214357: Epoch: 1, Batch: 32, Loss: 0.6005, Elapsed: 0m1s
2023-01-22 21:02:48.993939: Epoch: 1, Batch: 33, Loss: 0.6520, Elapsed: 0m2s
2023-01-22 21:02:54.062994: Epoch: 1, Batch: 34, Loss: 0.6855, Elapsed: 0m5s
2023-01-22 21:02:56.743798: Epoch: 1, Batch: 35, Loss: 0.6441, Elapsed: 0m2s
2023-01-22 21:02:58.543103: Epoch: 1, Batch: 36, Loss: 0.6338, Elapsed: 0m1s
2023-01-22 21:03:00.012004: Epoch: 1, Batch: 37, Loss: 0.5988, Elapsed: 0m1s
2023-01-22 21:03:02.987936: Epoch: 1, Batch: 38, Loss: 0.6382, Elapsed: 0m2s
2023-01-22 21:03:05.666241: Epoch: 1, Batch: 39, Loss: 0.6640, Elapsed: 0m2s
2023-01-22 21:03:06.945212: Epoch: 1, Batch: 40, Loss: 0.6600, Elapsed: 0m1s
2023-01-22 21:03:08.457275: Epoch: 1, Batch: 41, Loss: 0.6010, Elapsed: 0m1s
2023-01-22 21:03:10.130559: Epoch: 1, Batch: 42, Loss: 0.5863, Elapsed: 0m1s
2023-01-22 21:03:13.337765: Epoch: 1, Batch: 43, Loss: 0.6378, Elapsed: 0m3s
2023-01-22 21:03:16.390528: Epoch: 1, Batch: 44, Loss: 0.6279, Elapsed: 0m3s
2023-01-22 21:03:18.695889: Epoch: 1, Batch: 45, Loss: 0.5998, Elapsed: 0m2s
2023-01-22 21:03:21.506865: Epoch: 1, Batch: 46, Loss: 0.6667, Elapsed: 0m2s
2023-01-22 21:03:26.218403: Epoch: 1, Batch: 47, Loss: 0.6298, Elapsed: 0m4s
2023-01-22 21:03:27.931324: Epoch: 1, Batch: 48, Loss: 0.6132, Elapsed: 0m1s
2023-01-22 21:03:30.880657: Epoch: 1, Batch: 49, Loss: 0.6189, Elapsed: 0m2s
2023-01-22 21:03:33.173806: Epoch: 1, Batch: 50, Loss: 0.6157, Elapsed: 0m2s
2023-01-22 21:03:33.186713 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:03:50.612527: validation Test:  Loss: 0.6171,  AUC: 0.7266, Acc: 66.7822,  Precision: 0.7730 -- Elapsed: 0m17s
2023-01-22 21:03:50.612565 Starting testing the train set with 20 subgraphs!
2023-01-22 21:05:26.703992: training Test:  Loss: 0.6276,  AUC: 0.7128, Acc: 66.4747,  Precision: 0.7081 -- Elapsed: 1m36s
2023-01-22 21:05:30.316158: Epoch: 1, Batch: 51, Loss: 0.6318, Elapsed: 0m3s
2023-01-22 21:05:31.195760: Epoch: 1, Batch: 52, Loss: 0.6149, Elapsed: 0m0s
2023-01-22 21:05:34.072838: Epoch: 1, Batch: 53, Loss: 0.6310, Elapsed: 0m2s
2023-01-22 21:05:35.445510: Epoch: 1, Batch: 54, Loss: 0.6074, Elapsed: 0m1s
2023-01-22 21:05:38.762622: Epoch: 1, Batch: 55, Loss: 0.6556, Elapsed: 0m3s
2023-01-22 21:05:40.775690: Epoch: 1, Batch: 56, Loss: 0.6437, Elapsed: 0m2s
2023-01-22 21:05:43.592518: Epoch: 1, Batch: 57, Loss: 0.6273, Elapsed: 0m2s
2023-01-22 21:05:47.433542: Epoch: 1, Batch: 58, Loss: 0.6745, Elapsed: 0m3s
2023-01-22 21:05:49.917693: Epoch: 1, Batch: 59, Loss: 0.6070, Elapsed: 0m2s
2023-01-22 21:05:51.201920: Epoch: 1, Batch: 60, Loss: 0.5899, Elapsed: 0m1s
2023-01-22 21:05:52.336136: Epoch: 1, Batch: 61, Loss: 0.5982, Elapsed: 0m1s
2023-01-22 21:05:55.074281: Epoch: 1, Batch: 62, Loss: 0.6239, Elapsed: 0m2s
2023-01-22 21:05:56.534080: Epoch: 1, Batch: 63, Loss: 0.5934, Elapsed: 0m1s
2023-01-22 21:05:59.719269: Epoch: 1, Batch: 64, Loss: 0.6355, Elapsed: 0m3s
2023-01-22 21:06:03.763569: Epoch: 1, Batch: 65, Loss: 0.6498, Elapsed: 0m4s
2023-01-22 21:06:07.150757: Epoch: 1, Batch: 66, Loss: 0.6299, Elapsed: 0m3s
2023-01-22 21:06:10.030154: Epoch: 1, Batch: 67, Loss: 0.6432, Elapsed: 0m2s
2023-01-22 21:06:12.522513: Epoch: 1, Batch: 68, Loss: 0.6142, Elapsed: 0m2s
2023-01-22 21:06:15.201832: Epoch: 1, Batch: 69, Loss: 0.5981, Elapsed: 0m2s
2023-01-22 21:06:17.680735: Epoch: 1, Batch: 70, Loss: 0.5930, Elapsed: 0m2s
2023-01-22 21:06:21.815923: Epoch: 1, Batch: 71, Loss: 0.6203, Elapsed: 0m4s
2023-01-22 21:06:23.846432: Epoch: 1, Batch: 72, Loss: 0.6220, Elapsed: 0m2s
2023-01-22 21:06:26.417835: Epoch: 1, Batch: 73, Loss: 0.5913, Elapsed: 0m2s
2023-01-22 21:06:28.805673: Epoch: 1, Batch: 74, Loss: 0.6512, Elapsed: 0m2s
2023-01-22 21:06:30.907202: Epoch: 1, Batch: 75, Loss: 0.5700, Elapsed: 0m2s
2023-01-22 21:06:33.954992: Epoch: 1, Batch: 76, Loss: 0.6526, Elapsed: 0m3s
2023-01-22 21:06:37.729775: Epoch: 1, Batch: 77, Loss: 0.7718, Elapsed: 0m3s
2023-01-22 21:06:40.214397: Epoch: 1, Batch: 78, Loss: 0.6050, Elapsed: 0m2s
2023-01-22 21:06:41.644495: Epoch: 1, Batch: 79, Loss: 0.6213, Elapsed: 0m1s
2023-01-22 21:06:44.376085: Epoch: 1, Batch: 80, Loss: 0.6328, Elapsed: 0m2s
2023-01-22 21:06:47.022299: Epoch: 2, Batch: 1, Loss: 0.6296, Elapsed: 0m2s
2023-01-22 21:06:49.999641: Epoch: 2, Batch: 2, Loss: 0.6106, Elapsed: 0m2s
2023-01-22 21:06:52.781182: Epoch: 2, Batch: 3, Loss: 0.6331, Elapsed: 0m2s
2023-01-22 21:06:55.835516: Epoch: 2, Batch: 4, Loss: 0.6034, Elapsed: 0m3s
2023-01-22 21:06:56.714019: Epoch: 2, Batch: 5, Loss: 0.5977, Elapsed: 0m0s
2023-01-22 21:06:58.390278: Epoch: 2, Batch: 6, Loss: 0.5734, Elapsed: 0m1s
2023-01-22 21:07:00.548869: Epoch: 2, Batch: 7, Loss: 0.6133, Elapsed: 0m2s
2023-01-22 21:07:01.446336: Epoch: 2, Batch: 8, Loss: 0.5601, Elapsed: 0m0s
2023-01-22 21:07:05.403955: Epoch: 2, Batch: 9, Loss: 0.7623, Elapsed: 0m3s
2023-01-22 21:07:07.945295: Epoch: 2, Batch: 10, Loss: 0.5902, Elapsed: 0m2s
2023-01-22 21:07:09.770804: Epoch: 2, Batch: 11, Loss: 0.5874, Elapsed: 0m1s
2023-01-22 21:07:14.372473: Epoch: 2, Batch: 12, Loss: 0.6887, Elapsed: 0m4s
2023-01-22 21:07:15.652123: Epoch: 2, Batch: 13, Loss: 0.5870, Elapsed: 0m1s
2023-01-22 21:07:17.667963: Epoch: 2, Batch: 14, Loss: 0.6250, Elapsed: 0m2s
2023-01-22 21:07:22.419957: Epoch: 2, Batch: 15, Loss: 0.6784, Elapsed: 0m4s
2023-01-22 21:07:23.928553: Epoch: 2, Batch: 16, Loss: 0.5994, Elapsed: 0m1s
2023-01-22 21:07:25.379865: Epoch: 2, Batch: 17, Loss: 0.6379, Elapsed: 0m1s
2023-01-22 21:07:26.878933: Epoch: 2, Batch: 18, Loss: 0.5680, Elapsed: 0m1s
2023-01-22 21:07:30.145238: Epoch: 2, Batch: 19, Loss: 0.6152, Elapsed: 0m3s
2023-01-22 21:07:33.141081: Epoch: 2, Batch: 20, Loss: 0.6506, Elapsed: 0m2s
2023-01-22 21:07:36.708472: Epoch: 2, Batch: 21, Loss: 0.6200, Elapsed: 0m3s
2023-01-22 21:07:38.989438: Epoch: 2, Batch: 22, Loss: 0.5773, Elapsed: 0m2s
2023-01-22 21:07:42.180767: Epoch: 2, Batch: 23, Loss: 0.6174, Elapsed: 0m3s
2023-01-22 21:07:44.976281: Epoch: 2, Batch: 24, Loss: 0.5991, Elapsed: 0m2s
2023-01-22 21:07:48.072847: Epoch: 2, Batch: 25, Loss: 0.6119, Elapsed: 0m3s
2023-01-22 21:07:50.964099: Epoch: 2, Batch: 26, Loss: 0.5774, Elapsed: 0m2s
2023-01-22 21:07:52.353792: Epoch: 2, Batch: 27, Loss: 0.6331, Elapsed: 0m1s
2023-01-22 21:07:54.386440: Epoch: 2, Batch: 28, Loss: 0.6131, Elapsed: 0m2s
2023-01-22 21:07:55.663120: Epoch: 2, Batch: 29, Loss: 0.5806, Elapsed: 0m1s
2023-01-22 21:07:58.393187: Epoch: 2, Batch: 30, Loss: 0.6072, Elapsed: 0m2s
2023-01-22 21:07:59.758601: Epoch: 2, Batch: 31, Loss: 0.5909, Elapsed: 0m1s
2023-01-22 21:08:04.186113: Epoch: 2, Batch: 32, Loss: 0.6716, Elapsed: 0m4s
2023-01-22 21:08:05.463400: Epoch: 2, Batch: 33, Loss: 0.5887, Elapsed: 0m1s
2023-01-22 21:08:07.838296: Epoch: 2, Batch: 34, Loss: 0.6355, Elapsed: 0m2s
2023-01-22 21:08:12.953948: Epoch: 2, Batch: 35, Loss: 0.6308, Elapsed: 0m5s
2023-01-22 21:08:16.471897: Epoch: 2, Batch: 36, Loss: 0.6346, Elapsed: 0m3s
2023-01-22 21:08:18.556174: Epoch: 2, Batch: 37, Loss: 0.5485, Elapsed: 0m2s
2023-01-22 21:08:21.198686: Epoch: 2, Batch: 38, Loss: 0.5655, Elapsed: 0m2s
2023-01-22 21:08:23.661358: Epoch: 2, Batch: 39, Loss: 0.5864, Elapsed: 0m2s
2023-01-22 21:08:26.662843: Epoch: 2, Batch: 40, Loss: 0.6027, Elapsed: 0m2s
2023-01-22 21:08:29.339833: Epoch: 2, Batch: 41, Loss: 0.5567, Elapsed: 0m2s
2023-01-22 21:08:31.294021: Epoch: 2, Batch: 42, Loss: 0.6055, Elapsed: 0m1s
2023-01-22 21:08:33.752409: Epoch: 2, Batch: 43, Loss: 0.5808, Elapsed: 0m2s
2023-01-22 21:08:36.623647: Epoch: 2, Batch: 44, Loss: 0.6157, Elapsed: 0m2s
2023-01-22 21:08:39.084033: Epoch: 2, Batch: 45, Loss: 0.5906, Elapsed: 0m2s
2023-01-22 21:08:40.356011: Epoch: 2, Batch: 46, Loss: 0.5698, Elapsed: 0m1s
2023-01-22 21:08:43.274016: Epoch: 2, Batch: 47, Loss: 0.5861, Elapsed: 0m2s
2023-01-22 21:08:45.735761: Epoch: 2, Batch: 48, Loss: 0.5549, Elapsed: 0m2s
2023-01-22 21:08:48.948653: Epoch: 2, Batch: 49, Loss: 0.6081, Elapsed: 0m3s
2023-01-22 21:08:51.244341: Epoch: 2, Batch: 50, Loss: 0.6071, Elapsed: 0m2s
2023-01-22 21:08:51.256535 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:09:09.111702: validation Test:  Loss: 0.5909,  AUC: 0.7508, Acc: 68.1608,  Precision: 0.7519 -- Elapsed: 0m17s
2023-01-22 21:09:09.111744 Starting testing the train set with 20 subgraphs!
2023-01-22 21:10:45.244477: training Test:  Loss: 0.5907,  AUC: 0.7523, Acc: 68.7124,  Precision: 0.7318 -- Elapsed: 1m36s
2023-01-22 21:10:49.436044: Epoch: 2, Batch: 51, Loss: 0.6077, Elapsed: 0m4s
2023-01-22 21:10:53.220964: Epoch: 2, Batch: 52, Loss: 0.6640, Elapsed: 0m3s
2023-01-22 21:10:54.812334: Epoch: 2, Batch: 53, Loss: 0.5409, Elapsed: 0m1s
2023-01-22 21:10:56.141249: Epoch: 2, Batch: 54, Loss: 0.5573, Elapsed: 0m1s
2023-01-22 21:11:01.816336: Epoch: 2, Batch: 55, Loss: 0.5731, Elapsed: 0m5s
2023-01-22 21:11:04.905163: Epoch: 2, Batch: 56, Loss: 0.6330, Elapsed: 0m3s
2023-01-22 21:11:06.739706: Epoch: 2, Batch: 57, Loss: 0.5619, Elapsed: 0m1s
2023-01-22 21:11:09.421784: Epoch: 2, Batch: 58, Loss: 0.6003, Elapsed: 0m2s
2023-01-22 21:11:10.443292: Epoch: 2, Batch: 59, Loss: 0.5734, Elapsed: 0m1s
2023-01-22 21:11:12.033328: Epoch: 2, Batch: 60, Loss: 0.5389, Elapsed: 0m1s
2023-01-22 21:11:13.488747: Epoch: 2, Batch: 61, Loss: 0.5579, Elapsed: 0m1s
2023-01-22 21:11:17.027735: Epoch: 2, Batch: 62, Loss: 0.5580, Elapsed: 0m3s
2023-01-22 21:11:19.516975: Epoch: 2, Batch: 63, Loss: 0.5745, Elapsed: 0m2s
2023-01-22 21:11:21.821843: Epoch: 2, Batch: 64, Loss: 0.5809, Elapsed: 0m2s
2023-01-22 21:11:25.850586: Epoch: 2, Batch: 65, Loss: 0.6127, Elapsed: 0m4s
2023-01-22 21:11:26.993132: Epoch: 2, Batch: 66, Loss: 0.5356, Elapsed: 0m1s
2023-01-22 21:11:29.943090: Epoch: 2, Batch: 67, Loss: 0.5979, Elapsed: 0m2s
2023-01-22 21:11:31.457384: Epoch: 2, Batch: 68, Loss: 0.5358, Elapsed: 0m1s
2023-01-22 21:11:33.925168: Epoch: 2, Batch: 69, Loss: 0.6816, Elapsed: 0m2s
2023-01-22 21:11:35.758995: Epoch: 2, Batch: 70, Loss: 0.5818, Elapsed: 0m1s
2023-01-22 21:11:39.631890: Epoch: 2, Batch: 71, Loss: 0.7100, Elapsed: 0m3s
2023-01-22 21:11:41.542367: Epoch: 2, Batch: 72, Loss: 0.5397, Elapsed: 0m1s
2023-01-22 21:11:44.418520: Epoch: 2, Batch: 73, Loss: 0.6320, Elapsed: 0m2s
2023-01-22 21:11:45.865806: Epoch: 2, Batch: 74, Loss: 0.5458, Elapsed: 0m1s
2023-01-22 21:11:48.691263: Epoch: 2, Batch: 75, Loss: 0.6245, Elapsed: 0m2s
2023-01-22 21:11:50.469025: Epoch: 2, Batch: 76, Loss: 0.5970, Elapsed: 0m1s
2023-01-22 21:11:52.491735: Epoch: 2, Batch: 77, Loss: 0.6041, Elapsed: 0m2s
2023-01-22 21:11:55.492561: Epoch: 2, Batch: 78, Loss: 0.6316, Elapsed: 0m2s
2023-01-22 21:11:57.694612: Epoch: 2, Batch: 79, Loss: 0.5617, Elapsed: 0m2s
2023-01-22 21:11:59.846489: Epoch: 2, Batch: 80, Loss: 0.6289, Elapsed: 0m2s
2023-01-22 21:12:02.148149: Epoch: 3, Batch: 1, Loss: 0.6225, Elapsed: 0m2s
2023-01-22 21:12:04.099378: Epoch: 3, Batch: 2, Loss: 0.6122, Elapsed: 0m1s
2023-01-22 21:12:05.819018: Epoch: 3, Batch: 3, Loss: 0.6025, Elapsed: 0m1s
2023-01-22 21:12:10.104791: Epoch: 3, Batch: 4, Loss: 0.7428, Elapsed: 0m4s
2023-01-22 21:12:11.852254: Epoch: 3, Batch: 5, Loss: 0.5507, Elapsed: 0m1s
2023-01-22 21:12:13.871283: Epoch: 3, Batch: 6, Loss: 0.5851, Elapsed: 0m2s
2023-01-22 21:12:15.796291: Epoch: 3, Batch: 7, Loss: 0.5603, Elapsed: 0m1s
2023-01-22 21:12:17.248371: Epoch: 3, Batch: 8, Loss: 0.5980, Elapsed: 0m1s
2023-01-22 21:12:19.924981: Epoch: 3, Batch: 9, Loss: 0.6492, Elapsed: 0m2s
2023-01-22 21:12:21.367580: Epoch: 3, Batch: 10, Loss: 0.5531, Elapsed: 0m1s
2023-01-22 21:12:24.275915: Epoch: 3, Batch: 11, Loss: 0.5884, Elapsed: 0m2s
2023-01-22 21:12:27.792657: Epoch: 3, Batch: 12, Loss: 0.6274, Elapsed: 0m3s
2023-01-22 21:12:30.676478: Epoch: 3, Batch: 13, Loss: 0.6073, Elapsed: 0m2s
2023-01-22 21:12:33.133985: Epoch: 3, Batch: 14, Loss: 0.6014, Elapsed: 0m2s
2023-01-22 21:12:36.168526: Epoch: 3, Batch: 15, Loss: 0.6478, Elapsed: 0m3s
2023-01-22 21:12:38.975894: Epoch: 3, Batch: 16, Loss: 0.6262, Elapsed: 0m2s
2023-01-22 21:12:43.688193: Epoch: 3, Batch: 17, Loss: 0.5908, Elapsed: 0m4s
2023-01-22 21:12:45.762011: Epoch: 3, Batch: 18, Loss: 0.5386, Elapsed: 0m2s
2023-01-22 21:12:47.048886: Epoch: 3, Batch: 19, Loss: 0.6167, Elapsed: 0m1s
2023-01-22 21:12:49.214074: Epoch: 3, Batch: 20, Loss: 0.6055, Elapsed: 0m2s
2023-01-22 21:12:53.063897: Epoch: 3, Batch: 21, Loss: 0.6125, Elapsed: 0m3s
2023-01-22 21:12:55.476432: Epoch: 3, Batch: 22, Loss: 0.5661, Elapsed: 0m2s
2023-01-22 21:12:58.156835: Epoch: 3, Batch: 23, Loss: 0.5986, Elapsed: 0m2s
2023-01-22 21:13:01.330947: Epoch: 3, Batch: 24, Loss: 0.5969, Elapsed: 0m3s
2023-01-22 21:13:02.902465: Epoch: 3, Batch: 25, Loss: 0.5458, Elapsed: 0m1s
2023-01-22 21:13:04.269691: Epoch: 3, Batch: 26, Loss: 0.5757, Elapsed: 0m1s
2023-01-22 21:13:07.236216: Epoch: 3, Batch: 27, Loss: 0.5819, Elapsed: 0m2s
2023-01-22 21:13:10.341797: Epoch: 3, Batch: 28, Loss: 0.5860, Elapsed: 0m3s
2023-01-22 21:13:13.134543: Epoch: 3, Batch: 29, Loss: 0.5758, Elapsed: 0m2s
2023-01-22 21:13:14.156971: Epoch: 3, Batch: 30, Loss: 0.4674, Elapsed: 0m1s
2023-01-22 21:13:16.633185: Epoch: 3, Batch: 31, Loss: 0.5581, Elapsed: 0m2s
2023-01-22 21:13:19.871626: Epoch: 3, Batch: 32, Loss: 0.5863, Elapsed: 0m3s
2023-01-22 21:13:23.922398: Epoch: 3, Batch: 33, Loss: 0.5860, Elapsed: 0m4s
2023-01-22 21:13:26.736376: Epoch: 3, Batch: 34, Loss: 0.5852, Elapsed: 0m2s
2023-01-22 21:13:28.612694: Epoch: 3, Batch: 35, Loss: 0.5771, Elapsed: 0m1s
2023-01-22 21:13:31.108285: Epoch: 3, Batch: 36, Loss: 0.5651, Elapsed: 0m2s
2023-01-22 21:13:32.378042: Epoch: 3, Batch: 37, Loss: 0.5639, Elapsed: 0m1s
2023-01-22 21:13:35.219518: Epoch: 3, Batch: 38, Loss: 0.6205, Elapsed: 0m2s
2023-01-22 21:13:38.817947: Epoch: 3, Batch: 39, Loss: 0.6077, Elapsed: 0m3s
2023-01-22 21:13:41.259335: Epoch: 3, Batch: 40, Loss: 0.5638, Elapsed: 0m2s
2023-01-22 21:13:43.738457: Epoch: 3, Batch: 41, Loss: 0.5786, Elapsed: 0m2s
2023-01-22 21:13:46.382446: Epoch: 3, Batch: 42, Loss: 0.5678, Elapsed: 0m2s
2023-01-22 21:13:47.512573: Epoch: 3, Batch: 43, Loss: 0.5087, Elapsed: 0m1s
2023-01-22 21:13:49.639440: Epoch: 3, Batch: 44, Loss: 0.5859, Elapsed: 0m2s
2023-01-22 21:13:53.162194: Epoch: 3, Batch: 45, Loss: 0.5843, Elapsed: 0m3s
2023-01-22 21:13:56.215257: Epoch: 3, Batch: 46, Loss: 0.5651, Elapsed: 0m3s
2023-01-22 21:13:58.692735: Epoch: 3, Batch: 47, Loss: 0.5682, Elapsed: 0m2s
2023-01-22 21:14:02.564477: Epoch: 3, Batch: 48, Loss: 0.5708, Elapsed: 0m3s
2023-01-22 21:14:04.002739: Epoch: 3, Batch: 49, Loss: 0.5372, Elapsed: 0m1s
2023-01-22 21:14:08.625035: Epoch: 3, Batch: 50, Loss: 0.5623, Elapsed: 0m4s
2023-01-22 21:14:08.637409 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:14:26.759165: validation Test:  Loss: 0.5493,  AUC: 0.7945, Acc: 71.7269,  Precision: 0.7787 -- Elapsed: 0m18s
2023-01-22 21:14:26.759204 Starting testing the train set with 20 subgraphs!
2023-01-22 21:16:03.114464: training Test:  Loss: 0.5656,  AUC: 0.7763, Acc: 71.0235,  Precision: 0.7557 -- Elapsed: 1m36s
2023-01-22 21:16:05.797549: Epoch: 3, Batch: 51, Loss: 0.5580, Elapsed: 0m2s
2023-01-22 21:16:07.811393: Epoch: 3, Batch: 52, Loss: 0.5753, Elapsed: 0m2s
2023-01-22 21:16:10.355807: Epoch: 3, Batch: 53, Loss: 0.5538, Elapsed: 0m2s
2023-01-22 21:16:11.864685: Epoch: 3, Batch: 54, Loss: 0.5333, Elapsed: 0m1s
2023-01-22 21:16:13.422999: Epoch: 3, Batch: 55, Loss: 0.5160, Elapsed: 0m1s
2023-01-22 21:16:15.893869: Epoch: 3, Batch: 56, Loss: 0.5381, Elapsed: 0m2s
2023-01-22 21:16:17.225748: Epoch: 3, Batch: 57, Loss: 0.5217, Elapsed: 0m1s
2023-01-22 21:16:20.043839: Epoch: 3, Batch: 58, Loss: 0.5530, Elapsed: 0m2s
2023-01-22 21:16:21.650093: Epoch: 3, Batch: 59, Loss: 0.5474, Elapsed: 0m1s
2023-01-22 21:16:26.488913: Epoch: 3, Batch: 60, Loss: 0.5627, Elapsed: 0m4s
2023-01-22 21:16:27.796128: Epoch: 3, Batch: 61, Loss: 0.5308, Elapsed: 0m1s
2023-01-22 21:16:29.190623: Epoch: 3, Batch: 62, Loss: 0.5130, Elapsed: 0m1s
2023-01-22 21:16:30.067457: Epoch: 3, Batch: 63, Loss: 0.4950, Elapsed: 0m0s
2023-01-22 21:16:35.173072: Epoch: 3, Batch: 64, Loss: 0.5736, Elapsed: 0m5s
2023-01-22 21:16:40.617602: Epoch: 3, Batch: 65, Loss: 0.5582, Elapsed: 0m5s
2023-01-22 21:16:42.475793: Epoch: 3, Batch: 66, Loss: 0.5452, Elapsed: 0m1s
2023-01-22 21:16:46.623602: Epoch: 3, Batch: 67, Loss: 0.5792, Elapsed: 0m4s
2023-01-22 21:16:48.706986: Epoch: 3, Batch: 68, Loss: 0.5564, Elapsed: 0m2s
2023-01-22 21:16:50.002786: Epoch: 3, Batch: 69, Loss: 0.5201, Elapsed: 0m1s
2023-01-22 21:16:52.375458: Epoch: 3, Batch: 70, Loss: 0.5706, Elapsed: 0m2s
2023-01-22 21:16:55.352704: Epoch: 3, Batch: 71, Loss: 0.5941, Elapsed: 0m2s
2023-01-22 21:16:58.343722: Epoch: 3, Batch: 72, Loss: 0.5853, Elapsed: 0m2s
2023-01-22 21:17:00.172495: Epoch: 3, Batch: 73, Loss: 0.5565, Elapsed: 0m1s
2023-01-22 21:17:01.053135: Epoch: 3, Batch: 74, Loss: 0.4420, Elapsed: 0m0s
2023-01-22 21:17:04.251728: Epoch: 3, Batch: 75, Loss: 0.5313, Elapsed: 0m3s
2023-01-22 21:17:06.249624: Epoch: 3, Batch: 76, Loss: 0.5336, Elapsed: 0m1s
2023-01-22 21:17:09.053532: Epoch: 3, Batch: 77, Loss: 0.5797, Elapsed: 0m2s
2023-01-22 21:17:10.545954: Epoch: 3, Batch: 78, Loss: 0.5487, Elapsed: 0m1s
2023-01-22 21:17:12.914419: Epoch: 3, Batch: 79, Loss: 0.5738, Elapsed: 0m2s
2023-01-22 21:17:15.002422: Epoch: 3, Batch: 80, Loss: 0.5218, Elapsed: 0m2s
2023-01-22 21:17:18.066043: Epoch: 4, Batch: 1, Loss: 0.5519, Elapsed: 0m3s
2023-01-22 21:17:19.886514: Epoch: 4, Batch: 2, Loss: 0.5442, Elapsed: 0m1s
2023-01-22 21:17:23.139040: Epoch: 4, Batch: 3, Loss: 0.5564, Elapsed: 0m3s
2023-01-22 21:17:24.414523: Epoch: 4, Batch: 4, Loss: 0.5187, Elapsed: 0m1s
2023-01-22 21:17:28.131250: Epoch: 4, Batch: 5, Loss: 0.6029, Elapsed: 0m3s
2023-01-22 21:17:30.963606: Epoch: 4, Batch: 6, Loss: 0.5244, Elapsed: 0m2s
2023-01-22 21:17:34.180261: Epoch: 4, Batch: 7, Loss: 0.5821, Elapsed: 0m3s
2023-01-22 21:17:37.103769: Epoch: 4, Batch: 8, Loss: 0.5966, Elapsed: 0m2s
2023-01-22 21:17:39.665674: Epoch: 4, Batch: 9, Loss: 0.5409, Elapsed: 0m2s
2023-01-22 21:17:42.671580: Epoch: 4, Batch: 10, Loss: 0.5636, Elapsed: 0m2s
2023-01-22 21:17:43.552775: Epoch: 4, Batch: 11, Loss: 0.4359, Elapsed: 0m0s
2023-01-22 21:17:46.272996: Epoch: 4, Batch: 12, Loss: 0.5432, Elapsed: 0m2s
2023-01-22 21:17:48.208937: Epoch: 4, Batch: 13, Loss: 0.5671, Elapsed: 0m1s
2023-01-22 21:17:50.508457: Epoch: 4, Batch: 14, Loss: 0.5378, Elapsed: 0m2s
2023-01-22 21:17:54.580775: Epoch: 4, Batch: 15, Loss: 0.5573, Elapsed: 0m4s
2023-01-22 21:17:58.105044: Epoch: 4, Batch: 16, Loss: 0.5828, Elapsed: 0m3s
2023-01-22 21:18:00.570599: Epoch: 4, Batch: 17, Loss: 0.5545, Elapsed: 0m2s
2023-01-22 21:18:03.241819: Epoch: 4, Batch: 18, Loss: 0.5513, Elapsed: 0m2s
2023-01-22 21:18:05.242275: Epoch: 4, Batch: 19, Loss: 0.5235, Elapsed: 0m1s
2023-01-22 21:18:06.621730: Epoch: 4, Batch: 20, Loss: 0.5132, Elapsed: 0m1s
2023-01-22 21:18:08.443317: Epoch: 4, Batch: 21, Loss: 0.5317, Elapsed: 0m1s
2023-01-22 21:18:09.903400: Epoch: 4, Batch: 22, Loss: 0.5155, Elapsed: 0m1s
2023-01-22 21:18:13.773526: Epoch: 4, Batch: 23, Loss: 0.5498, Elapsed: 0m3s
2023-01-22 21:18:16.558369: Epoch: 4, Batch: 24, Loss: 0.5804, Elapsed: 0m2s
2023-01-22 21:18:18.573525: Epoch: 4, Batch: 25, Loss: 0.5473, Elapsed: 0m2s
2023-01-22 21:18:21.533479: Epoch: 4, Batch: 26, Loss: 0.5673, Elapsed: 0m2s
2023-01-22 21:18:25.005773: Epoch: 4, Batch: 27, Loss: 0.5703, Elapsed: 0m3s
2023-01-22 21:18:26.444016: Epoch: 4, Batch: 28, Loss: 0.5544, Elapsed: 0m1s
2023-01-22 21:18:29.354648: Epoch: 4, Batch: 29, Loss: 0.5433, Elapsed: 0m2s
2023-01-22 21:18:30.940337: Epoch: 4, Batch: 30, Loss: 0.4837, Elapsed: 0m1s
2023-01-22 21:18:37.057683: Epoch: 4, Batch: 31, Loss: 0.5477, Elapsed: 0m6s
2023-01-22 21:18:39.921502: Epoch: 4, Batch: 32, Loss: 0.5401, Elapsed: 0m2s
2023-01-22 21:18:42.047742: Epoch: 4, Batch: 33, Loss: 0.5648, Elapsed: 0m2s
2023-01-22 21:18:44.875429: Epoch: 4, Batch: 34, Loss: 0.5414, Elapsed: 0m2s
2023-01-22 21:18:46.786518: Epoch: 4, Batch: 35, Loss: 0.5255, Elapsed: 0m1s
2023-01-22 21:18:51.875882: Epoch: 4, Batch: 36, Loss: 0.5589, Elapsed: 0m5s
2023-01-22 21:18:53.993208: Epoch: 4, Batch: 37, Loss: 0.5020, Elapsed: 0m2s
2023-01-22 21:18:58.403072: Epoch: 4, Batch: 38, Loss: 0.5529, Elapsed: 0m4s
2023-01-22 21:18:59.439826: Epoch: 4, Batch: 39, Loss: 0.4043, Elapsed: 0m1s
2023-01-22 21:19:01.803936: Epoch: 4, Batch: 40, Loss: 0.5746, Elapsed: 0m2s
2023-01-22 21:19:06.396436: Epoch: 4, Batch: 41, Loss: 0.5448, Elapsed: 0m4s
2023-01-22 21:19:09.238157: Epoch: 4, Batch: 42, Loss: 0.5713, Elapsed: 0m2s
2023-01-22 21:19:10.738721: Epoch: 4, Batch: 43, Loss: 0.4794, Elapsed: 0m1s
2023-01-22 21:19:12.030564: Epoch: 4, Batch: 44, Loss: 0.4959, Elapsed: 0m1s
2023-01-22 21:19:14.405790: Epoch: 4, Batch: 45, Loss: 0.5625, Elapsed: 0m2s
2023-01-22 21:19:17.375450: Epoch: 4, Batch: 46, Loss: 0.5411, Elapsed: 0m2s
2023-01-22 21:19:19.843996: Epoch: 4, Batch: 47, Loss: 0.5203, Elapsed: 0m2s
2023-01-22 21:19:22.716017: Epoch: 4, Batch: 48, Loss: 0.5669, Elapsed: 0m2s
2023-01-22 21:19:24.208851: Epoch: 4, Batch: 49, Loss: 0.4870, Elapsed: 0m1s
2023-01-22 21:19:25.890422: Epoch: 4, Batch: 50, Loss: 0.4925, Elapsed: 0m1s
2023-01-22 21:19:25.901431 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:19:43.600229: validation Test:  Loss: 0.5200,  AUC: 0.8063, Acc: 72.7588,  Precision: 0.8150 -- Elapsed: 0m17s
2023-01-22 21:19:43.600271 Starting testing the train set with 20 subgraphs!
2023-01-22 21:21:19.355881: training Test:  Loss: 0.5424,  AUC: 0.7850, Acc: 71.6611,  Precision: 0.7878 -- Elapsed: 1m35s
2023-01-22 21:21:21.174184: Epoch: 4, Batch: 51, Loss: 0.5288, Elapsed: 0m1s
2023-01-22 21:21:22.442774: Epoch: 4, Batch: 52, Loss: 0.5325, Elapsed: 0m1s
2023-01-22 21:21:26.298282: Epoch: 4, Batch: 53, Loss: 0.5781, Elapsed: 0m3s
2023-01-22 21:21:31.032598: Epoch: 4, Batch: 54, Loss: 0.5557, Elapsed: 0m4s
2023-01-22 21:21:34.225701: Epoch: 4, Batch: 55, Loss: 0.5267, Elapsed: 0m3s
2023-01-22 21:21:36.701200: Epoch: 4, Batch: 56, Loss: 0.5101, Elapsed: 0m2s
2023-01-22 21:21:38.086769: Epoch: 4, Batch: 57, Loss: 0.4903, Elapsed: 0m1s
2023-01-22 21:21:40.558852: Epoch: 4, Batch: 58, Loss: 0.5591, Elapsed: 0m2s
2023-01-22 21:21:43.242373: Epoch: 4, Batch: 59, Loss: 0.5225, Elapsed: 0m2s
2023-01-22 21:21:44.564657: Epoch: 4, Batch: 60, Loss: 0.4884, Elapsed: 0m1s
2023-01-22 21:21:47.133310: Epoch: 4, Batch: 61, Loss: 0.5393, Elapsed: 0m2s
2023-01-22 21:21:48.429982: Epoch: 4, Batch: 62, Loss: 0.4466, Elapsed: 0m1s
2023-01-22 21:21:51.688837: Epoch: 4, Batch: 63, Loss: 0.5810, Elapsed: 0m3s
2023-01-22 21:21:53.178460: Epoch: 4, Batch: 64, Loss: 0.5403, Elapsed: 0m1s
2023-01-22 21:21:55.646970: Epoch: 4, Batch: 65, Loss: 0.5395, Elapsed: 0m2s
2023-01-22 21:21:59.425082: Epoch: 4, Batch: 66, Loss: 0.6194, Elapsed: 0m3s
2023-01-22 21:22:01.592306: Epoch: 4, Batch: 67, Loss: 0.5417, Elapsed: 0m2s
2023-01-22 21:22:03.686816: Epoch: 4, Batch: 68, Loss: 0.5516, Elapsed: 0m2s
2023-01-22 21:22:04.556737: Epoch: 4, Batch: 69, Loss: 0.4348, Elapsed: 0m0s
2023-01-22 21:22:06.006340: Epoch: 4, Batch: 70, Loss: 0.4892, Elapsed: 0m1s
2023-01-22 21:22:08.683880: Epoch: 4, Batch: 71, Loss: 0.5961, Elapsed: 0m2s
2023-01-22 21:22:10.704025: Epoch: 4, Batch: 72, Loss: 0.5559, Elapsed: 0m2s
2023-01-22 21:22:11.987811: Epoch: 4, Batch: 73, Loss: 0.5107, Elapsed: 0m1s
2023-01-22 21:22:16.121602: Epoch: 4, Batch: 74, Loss: 0.5749, Elapsed: 0m4s
2023-01-22 21:22:17.722910: Epoch: 4, Batch: 75, Loss: 0.5460, Elapsed: 0m1s
2023-01-22 21:22:19.305412: Epoch: 4, Batch: 76, Loss: 0.4926, Elapsed: 0m1s
2023-01-22 21:22:22.496087: Epoch: 4, Batch: 77, Loss: 0.5825, Elapsed: 0m3s
2023-01-22 21:22:24.550968: Epoch: 4, Batch: 78, Loss: 0.4888, Elapsed: 0m2s
2023-01-22 21:22:26.855975: Epoch: 4, Batch: 79, Loss: 0.5223, Elapsed: 0m2s
2023-01-22 21:22:29.841456: Epoch: 4, Batch: 80, Loss: 0.5722, Elapsed: 0m2s
2023-01-22 21:22:31.295256: Epoch: 5, Batch: 1, Loss: 0.5170, Elapsed: 0m1s
2023-01-22 21:22:32.792147: Epoch: 5, Batch: 2, Loss: 0.5336, Elapsed: 0m1s
2023-01-22 21:22:35.101888: Epoch: 5, Batch: 3, Loss: 0.5292, Elapsed: 0m2s
2023-01-22 21:22:37.570482: Epoch: 5, Batch: 4, Loss: 0.5524, Elapsed: 0m2s
2023-01-22 21:22:41.364587: Epoch: 5, Batch: 5, Loss: 0.5671, Elapsed: 0m3s
2023-01-22 21:22:44.258771: Epoch: 5, Batch: 6, Loss: 0.5443, Elapsed: 0m2s
2023-01-22 21:22:46.746900: Epoch: 5, Batch: 7, Loss: 0.5228, Elapsed: 0m2s
2023-01-22 21:22:49.564774: Epoch: 5, Batch: 8, Loss: 0.5704, Elapsed: 0m2s
2023-01-22 21:22:53.373476: Epoch: 5, Batch: 9, Loss: 0.5810, Elapsed: 0m3s
2023-01-22 21:22:55.789965: Epoch: 5, Batch: 10, Loss: 0.5481, Elapsed: 0m2s
2023-01-22 21:22:59.990426: Epoch: 5, Batch: 11, Loss: 0.5468, Elapsed: 0m4s
2023-01-22 21:23:01.388073: Epoch: 5, Batch: 12, Loss: 0.5631, Elapsed: 0m1s
2023-01-22 21:23:02.664475: Epoch: 5, Batch: 13, Loss: 0.5272, Elapsed: 0m1s
2023-01-22 21:23:03.944601: Epoch: 5, Batch: 14, Loss: 0.5454, Elapsed: 0m1s
2023-01-22 21:23:06.946702: Epoch: 5, Batch: 15, Loss: 0.5702, Elapsed: 0m2s
2023-01-22 21:23:08.960260: Epoch: 5, Batch: 16, Loss: 0.5131, Elapsed: 0m2s
2023-01-22 21:23:10.400144: Epoch: 5, Batch: 17, Loss: 0.4861, Elapsed: 0m1s
2023-01-22 21:23:11.895251: Epoch: 5, Batch: 18, Loss: 0.4793, Elapsed: 0m1s
2023-01-22 21:23:13.840242: Epoch: 5, Batch: 19, Loss: 0.5553, Elapsed: 0m1s
2023-01-22 21:23:17.008905: Epoch: 5, Batch: 20, Loss: 0.5716, Elapsed: 0m3s
2023-01-22 21:23:19.491707: Epoch: 5, Batch: 21, Loss: 0.5075, Elapsed: 0m2s
2023-01-22 21:23:22.544307: Epoch: 5, Batch: 22, Loss: 0.5556, Elapsed: 0m3s
2023-01-22 21:23:24.559546: Epoch: 5, Batch: 23, Loss: 0.5351, Elapsed: 0m2s
2023-01-22 21:23:28.090725: Epoch: 5, Batch: 24, Loss: 0.5546, Elapsed: 0m3s
2023-01-22 21:23:32.515930: Epoch: 5, Batch: 25, Loss: 0.5738, Elapsed: 0m4s
2023-01-22 21:23:34.972780: Epoch: 5, Batch: 26, Loss: 0.5327, Elapsed: 0m2s
2023-01-22 21:23:36.803623: Epoch: 5, Batch: 27, Loss: 0.5210, Elapsed: 0m1s
2023-01-22 21:23:37.689715: Epoch: 5, Batch: 28, Loss: 0.4018, Elapsed: 0m0s
2023-01-22 21:23:39.188010: Epoch: 5, Batch: 29, Loss: 0.4848, Elapsed: 0m1s
2023-01-22 21:23:42.044230: Epoch: 5, Batch: 30, Loss: 0.6037, Elapsed: 0m2s
2023-01-22 21:23:43.420039: Epoch: 5, Batch: 31, Loss: 0.5035, Elapsed: 0m1s
2023-01-22 21:23:47.268596: Epoch: 5, Batch: 32, Loss: 0.5905, Elapsed: 0m3s
2023-01-22 21:23:48.142880: Epoch: 5, Batch: 33, Loss: 0.4555, Elapsed: 0m0s
2023-01-22 21:23:50.279575: Epoch: 5, Batch: 34, Loss: 0.5608, Elapsed: 0m2s
2023-01-22 21:23:53.012969: Epoch: 5, Batch: 35, Loss: 0.5428, Elapsed: 0m2s
2023-01-22 21:23:56.312212: Epoch: 5, Batch: 36, Loss: 0.5157, Elapsed: 0m3s
2023-01-22 21:23:58.125693: Epoch: 5, Batch: 37, Loss: 0.4808, Elapsed: 0m1s
2023-01-22 21:24:00.523004: Epoch: 5, Batch: 38, Loss: 0.5529, Elapsed: 0m2s
2023-01-22 21:24:04.383826: Epoch: 5, Batch: 39, Loss: 0.5960, Elapsed: 0m3s
2023-01-22 21:24:05.830856: Epoch: 5, Batch: 40, Loss: 0.5565, Elapsed: 0m1s
2023-01-22 21:24:08.648010: Epoch: 5, Batch: 41, Loss: 0.5576, Elapsed: 0m2s
2023-01-22 21:24:09.758980: Epoch: 5, Batch: 42, Loss: 0.4569, Elapsed: 0m1s
2023-01-22 21:24:11.590723: Epoch: 5, Batch: 43, Loss: 0.5269, Elapsed: 0m1s
2023-01-22 21:24:14.556982: Epoch: 5, Batch: 44, Loss: 0.5410, Elapsed: 0m2s
2023-01-22 21:24:16.727739: Epoch: 5, Batch: 45, Loss: 0.5274, Elapsed: 0m2s
2023-01-22 21:24:19.206581: Epoch: 5, Batch: 46, Loss: 0.5149, Elapsed: 0m2s
2023-01-22 21:24:21.875484: Epoch: 5, Batch: 47, Loss: 0.5284, Elapsed: 0m2s
2023-01-22 21:24:23.896158: Epoch: 5, Batch: 48, Loss: 0.5684, Elapsed: 0m2s
2023-01-22 21:24:26.561495: Epoch: 5, Batch: 49, Loss: 0.5693, Elapsed: 0m2s
2023-01-22 21:24:28.182314: Epoch: 5, Batch: 50, Loss: 0.5178, Elapsed: 0m1s
2023-01-22 21:24:28.195323 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:24:45.588597: validation Test:  Loss: 0.5160,  AUC: 0.8057, Acc: 72.8579,  Precision: 0.8469 -- Elapsed: 0m17s
2023-01-22 21:24:45.588703 Starting testing the train set with 20 subgraphs!
2023-01-22 21:26:22.385754: training Test:  Loss: 0.5419,  AUC: 0.7845, Acc: 71.8477,  Precision: 0.8173 -- Elapsed: 1m36s
2023-01-22 21:26:27.848847: Epoch: 5, Batch: 51, Loss: 0.5600, Elapsed: 0m5s
2023-01-22 21:26:30.543176: Epoch: 5, Batch: 52, Loss: 0.5428, Elapsed: 0m2s
2023-01-22 21:26:32.903394: Epoch: 5, Batch: 53, Loss: 0.5662, Elapsed: 0m2s
2023-01-22 21:26:34.226778: Epoch: 5, Batch: 54, Loss: 0.4691, Elapsed: 0m1s
2023-01-22 21:26:39.325916: Epoch: 5, Batch: 55, Loss: 0.5728, Elapsed: 0m5s
2023-01-22 21:26:41.726413: Epoch: 5, Batch: 56, Loss: 0.5608, Elapsed: 0m2s
2023-01-22 21:26:42.745135: Epoch: 5, Batch: 57, Loss: 0.3814, Elapsed: 0m1s
2023-01-22 21:26:46.782891: Epoch: 5, Batch: 58, Loss: 0.5566, Elapsed: 0m4s
2023-01-22 21:26:49.086812: Epoch: 5, Batch: 59, Loss: 0.5160, Elapsed: 0m2s
2023-01-22 21:26:50.476053: Epoch: 5, Batch: 60, Loss: 0.4674, Elapsed: 0m1s
2023-01-22 21:26:52.943346: Epoch: 5, Batch: 61, Loss: 0.5265, Elapsed: 0m2s
2023-01-22 21:26:54.616139: Epoch: 5, Batch: 62, Loss: 0.4858, Elapsed: 0m1s
2023-01-22 21:26:57.623066: Epoch: 5, Batch: 63, Loss: 0.5443, Elapsed: 0m2s
2023-01-22 21:26:59.682403: Epoch: 5, Batch: 64, Loss: 0.4690, Elapsed: 0m2s
2023-01-22 21:27:02.913474: Epoch: 5, Batch: 65, Loss: 0.5455, Elapsed: 0m3s
2023-01-22 21:27:05.534449: Epoch: 5, Batch: 66, Loss: 0.5400, Elapsed: 0m2s
2023-01-22 21:27:08.509583: Epoch: 5, Batch: 67, Loss: 0.5747, Elapsed: 0m2s
2023-01-22 21:27:11.580759: Epoch: 5, Batch: 68, Loss: 0.5449, Elapsed: 0m3s
2023-01-22 21:27:13.775221: Epoch: 5, Batch: 69, Loss: 0.5102, Elapsed: 0m2s
2023-01-22 21:27:18.471316: Epoch: 5, Batch: 70, Loss: 0.5403, Elapsed: 0m4s
2023-01-22 21:27:23.207769: Epoch: 5, Batch: 71, Loss: 0.5566, Elapsed: 0m4s
2023-01-22 21:27:25.006035: Epoch: 5, Batch: 72, Loss: 0.5222, Elapsed: 0m1s
2023-01-22 21:27:27.787700: Epoch: 5, Batch: 73, Loss: 0.5682, Elapsed: 0m2s
2023-01-22 21:27:30.265419: Epoch: 5, Batch: 74, Loss: 0.5510, Elapsed: 0m2s
2023-01-22 21:27:31.538080: Epoch: 5, Batch: 75, Loss: 0.4654, Elapsed: 0m1s
2023-01-22 21:27:35.683264: Epoch: 5, Batch: 76, Loss: 0.5724, Elapsed: 0m4s
2023-01-22 21:27:37.240550: Epoch: 5, Batch: 77, Loss: 0.4749, Elapsed: 0m1s
2023-01-22 21:27:40.103651: Epoch: 5, Batch: 78, Loss: 0.5574, Elapsed: 0m2s
2023-01-22 21:27:43.037368: Epoch: 5, Batch: 79, Loss: 0.5494, Elapsed: 0m2s
2023-01-22 21:27:45.602246: Epoch: 5, Batch: 80, Loss: 0.5096, Elapsed: 0m2s
2023-01-22 21:27:48.077756: Epoch: 6, Batch: 1, Loss: 0.5157, Elapsed: 0m2s
2023-01-22 21:27:50.716341: Epoch: 6, Batch: 2, Loss: 0.5347, Elapsed: 0m2s
2023-01-22 21:27:53.541479: Epoch: 6, Batch: 3, Loss: 0.5420, Elapsed: 0m2s
2023-01-22 21:27:55.631246: Epoch: 6, Batch: 4, Loss: 0.4986, Elapsed: 0m2s
2023-01-22 21:27:57.138256: Epoch: 6, Batch: 5, Loss: 0.4766, Elapsed: 0m1s
2023-01-22 21:27:59.967454: Epoch: 6, Batch: 6, Loss: 0.5594, Elapsed: 0m2s
2023-01-22 21:28:02.424577: Epoch: 6, Batch: 7, Loss: 0.5489, Elapsed: 0m2s
2023-01-22 21:28:05.106553: Epoch: 6, Batch: 8, Loss: 0.5423, Elapsed: 0m2s
2023-01-22 21:28:06.893684: Epoch: 6, Batch: 9, Loss: 0.5141, Elapsed: 0m1s
2023-01-22 21:28:09.381843: Epoch: 6, Batch: 10, Loss: 0.5038, Elapsed: 0m2s
2023-01-22 21:28:10.880242: Epoch: 6, Batch: 11, Loss: 0.5134, Elapsed: 0m1s
2023-01-22 21:28:13.346862: Epoch: 6, Batch: 12, Loss: 0.5400, Elapsed: 0m2s
2023-01-22 21:28:19.010938: Epoch: 6, Batch: 13, Loss: 0.5601, Elapsed: 0m5s
2023-01-22 21:28:21.049225: Epoch: 6, Batch: 14, Loss: 0.5347, Elapsed: 0m2s
2023-01-22 21:28:23.709233: Epoch: 6, Batch: 15, Loss: 0.5143, Elapsed: 0m2s
2023-01-22 21:28:25.152438: Epoch: 6, Batch: 16, Loss: 0.5093, Elapsed: 0m1s
2023-01-22 21:28:29.009458: Epoch: 6, Batch: 17, Loss: 0.5618, Elapsed: 0m3s
2023-01-22 21:28:34.435922: Epoch: 6, Batch: 18, Loss: 0.5434, Elapsed: 0m5s
2023-01-22 21:28:36.746960: Epoch: 6, Batch: 19, Loss: 0.5118, Elapsed: 0m2s
2023-01-22 21:28:39.558050: Epoch: 6, Batch: 20, Loss: 0.5436, Elapsed: 0m2s
2023-01-22 21:28:40.831359: Epoch: 6, Batch: 21, Loss: 0.4990, Elapsed: 0m1s
2023-01-22 21:28:43.416945: Epoch: 6, Batch: 22, Loss: 0.5108, Elapsed: 0m2s
2023-01-22 21:28:46.276669: Epoch: 6, Batch: 23, Loss: 0.5236, Elapsed: 0m2s
2023-01-22 21:28:49.222265: Epoch: 6, Batch: 24, Loss: 0.5535, Elapsed: 0m2s
2023-01-22 21:28:51.595236: Epoch: 6, Batch: 25, Loss: 0.5729, Elapsed: 0m2s
2023-01-22 21:28:52.878596: Epoch: 6, Batch: 26, Loss: 0.4946, Elapsed: 0m1s
2023-01-22 21:28:57.284668: Epoch: 6, Batch: 27, Loss: 0.5379, Elapsed: 0m4s
2023-01-22 21:29:01.173954: Epoch: 6, Batch: 28, Loss: 0.5391, Elapsed: 0m3s
2023-01-22 21:29:02.741045: Epoch: 6, Batch: 29, Loss: 0.4827, Elapsed: 0m1s
2023-01-22 21:29:06.251668: Epoch: 6, Batch: 30, Loss: 0.5707, Elapsed: 0m3s
2023-01-22 21:29:10.296509: Epoch: 6, Batch: 31, Loss: 0.5559, Elapsed: 0m4s
2023-01-22 21:29:13.165219: Epoch: 6, Batch: 32, Loss: 0.5520, Elapsed: 0m2s
2023-01-22 21:29:15.627117: Epoch: 6, Batch: 33, Loss: 0.5252, Elapsed: 0m2s
2023-01-22 21:29:18.534776: Epoch: 6, Batch: 34, Loss: 0.5330, Elapsed: 0m2s
2023-01-22 21:29:20.571065: Epoch: 6, Batch: 35, Loss: 0.5243, Elapsed: 0m2s
2023-01-22 21:29:22.302338: Epoch: 6, Batch: 36, Loss: 0.4630, Elapsed: 0m1s
2023-01-22 21:29:27.554585: Epoch: 6, Batch: 37, Loss: 0.5597, Elapsed: 0m5s
2023-01-22 21:29:28.999012: Epoch: 6, Batch: 38, Loss: 0.5461, Elapsed: 0m1s
2023-01-22 21:29:32.058509: Epoch: 6, Batch: 39, Loss: 0.5363, Elapsed: 0m3s
2023-01-22 21:29:35.307899: Epoch: 6, Batch: 40, Loss: 0.5444, Elapsed: 0m3s
2023-01-22 21:29:38.271700: Epoch: 6, Batch: 41, Loss: 0.5302, Elapsed: 0m2s
2023-01-22 21:29:40.744664: Epoch: 6, Batch: 42, Loss: 0.5130, Elapsed: 0m2s
2023-01-22 21:29:42.111206: Epoch: 6, Batch: 43, Loss: 0.4943, Elapsed: 0m1s
2023-01-22 21:29:43.700581: Epoch: 6, Batch: 44, Loss: 0.4692, Elapsed: 0m1s
2023-01-22 21:29:44.824791: Epoch: 6, Batch: 45, Loss: 0.4532, Elapsed: 0m1s
2023-01-22 21:29:45.704636: Epoch: 6, Batch: 46, Loss: 0.3857, Elapsed: 0m0s
2023-01-22 21:29:50.276308: Epoch: 6, Batch: 47, Loss: 0.5374, Elapsed: 0m4s
2023-01-22 21:29:53.070587: Epoch: 6, Batch: 48, Loss: 0.5707, Elapsed: 0m2s
2023-01-22 21:29:55.137339: Epoch: 6, Batch: 49, Loss: 0.4732, Elapsed: 0m2s
2023-01-22 21:29:58.110699: Epoch: 6, Batch: 50, Loss: 0.5741, Elapsed: 0m2s
2023-01-22 21:29:58.122349 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:30:15.668706: validation Test:  Loss: 0.5108,  AUC: 0.8083, Acc: 72.1727,  Precision: 0.8879 -- Elapsed: 0m17s
2023-01-22 21:30:15.668757 Starting testing the train set with 20 subgraphs!
2023-01-22 21:31:51.814590: training Test:  Loss: 0.5376,  AUC: 0.7836, Acc: 71.2439,  Precision: 0.8582 -- Elapsed: 1m36s
2023-01-22 21:31:53.181657: Epoch: 6, Batch: 51, Loss: 0.4662, Elapsed: 0m1s
2023-01-22 21:31:54.856138: Epoch: 6, Batch: 52, Loss: 0.4929, Elapsed: 0m1s
2023-01-22 21:31:56.127740: Epoch: 6, Batch: 53, Loss: 0.5289, Elapsed: 0m1s
2023-01-22 21:31:57.156185: Epoch: 6, Batch: 54, Loss: 0.3629, Elapsed: 0m1s
2023-01-22 21:31:59.522849: Epoch: 6, Batch: 55, Loss: 0.5845, Elapsed: 0m2s
2023-01-22 21:32:02.523560: Epoch: 6, Batch: 56, Loss: 0.5672, Elapsed: 0m2s
2023-01-22 21:32:05.199547: Epoch: 6, Batch: 57, Loss: 0.5708, Elapsed: 0m2s
2023-01-22 21:32:07.489029: Epoch: 6, Batch: 58, Loss: 0.5177, Elapsed: 0m2s
2023-01-22 21:32:10.520078: Epoch: 6, Batch: 59, Loss: 0.5480, Elapsed: 0m3s
2023-01-22 21:32:12.437942: Epoch: 6, Batch: 60, Loss: 0.5109, Elapsed: 0m1s
2023-01-22 21:32:15.303739: Epoch: 6, Batch: 61, Loss: 0.5779, Elapsed: 0m2s
2023-01-22 21:32:18.464685: Epoch: 6, Batch: 62, Loss: 0.5593, Elapsed: 0m3s
2023-01-22 21:32:20.083755: Epoch: 6, Batch: 63, Loss: 0.5095, Elapsed: 0m1s
2023-01-22 21:32:23.608936: Epoch: 6, Batch: 64, Loss: 0.5407, Elapsed: 0m3s
2023-01-22 21:32:25.627895: Epoch: 6, Batch: 65, Loss: 0.5157, Elapsed: 0m2s
2023-01-22 21:32:27.584389: Epoch: 6, Batch: 66, Loss: 0.5471, Elapsed: 0m1s
2023-01-22 21:32:29.721961: Epoch: 6, Batch: 67, Loss: 0.5480, Elapsed: 0m2s
2023-01-22 21:32:33.659635: Epoch: 6, Batch: 68, Loss: 0.6157, Elapsed: 0m3s
2023-01-22 21:32:37.732563: Epoch: 6, Batch: 69, Loss: 0.5910, Elapsed: 0m4s
2023-01-22 21:32:39.711500: Epoch: 6, Batch: 70, Loss: 0.5237, Elapsed: 0m1s
2023-01-22 21:32:41.160749: Epoch: 6, Batch: 71, Loss: 0.4797, Elapsed: 0m1s
2023-01-22 21:32:42.446893: Epoch: 6, Batch: 72, Loss: 0.4810, Elapsed: 0m1s
2023-01-22 21:32:43.325793: Epoch: 6, Batch: 73, Loss: 0.4499, Elapsed: 0m0s
2023-01-22 21:32:45.793854: Epoch: 6, Batch: 74, Loss: 0.5315, Elapsed: 0m2s
2023-01-22 21:32:48.987976: Epoch: 6, Batch: 75, Loss: 0.5074, Elapsed: 0m3s
2023-01-22 21:32:50.377141: Epoch: 6, Batch: 76, Loss: 0.4562, Elapsed: 0m1s
2023-01-22 21:32:52.555990: Epoch: 6, Batch: 77, Loss: 0.5341, Elapsed: 0m2s
2023-01-22 21:32:54.570664: Epoch: 6, Batch: 78, Loss: 0.5501, Elapsed: 0m2s
2023-01-22 21:32:56.673323: Epoch: 6, Batch: 79, Loss: 0.5527, Elapsed: 0m2s
2023-01-22 21:33:00.801409: Epoch: 6, Batch: 80, Loss: 0.5733, Elapsed: 0m4s
2023-01-22 21:33:03.410478: Epoch: 7, Batch: 1, Loss: 0.5492, Elapsed: 0m2s
2023-01-22 21:33:04.989526: Epoch: 7, Batch: 2, Loss: 0.4667, Elapsed: 0m1s
2023-01-22 21:33:06.908657: Epoch: 7, Batch: 3, Loss: 0.5121, Elapsed: 0m1s
2023-01-22 21:33:09.888860: Epoch: 7, Batch: 4, Loss: 0.5370, Elapsed: 0m2s
2023-01-22 21:33:11.710760: Epoch: 7, Batch: 5, Loss: 0.5223, Elapsed: 0m1s
2023-01-22 21:33:17.139137: Epoch: 7, Batch: 6, Loss: 0.5472, Elapsed: 0m5s
2023-01-22 21:33:20.033587: Epoch: 7, Batch: 7, Loss: 0.5565, Elapsed: 0m2s
2023-01-22 21:33:22.165607: Epoch: 7, Batch: 8, Loss: 0.5495, Elapsed: 0m2s
2023-01-22 21:33:24.629654: Epoch: 7, Batch: 9, Loss: 0.5378, Elapsed: 0m2s
2023-01-22 21:33:26.653785: Epoch: 7, Batch: 10, Loss: 0.5249, Elapsed: 0m2s
2023-01-22 21:33:29.455148: Epoch: 7, Batch: 11, Loss: 0.5593, Elapsed: 0m2s
2023-01-22 21:33:33.296393: Epoch: 7, Batch: 12, Loss: 0.5749, Elapsed: 0m3s
2023-01-22 21:33:36.494332: Epoch: 7, Batch: 13, Loss: 0.5676, Elapsed: 0m3s
2023-01-22 21:33:40.446143: Epoch: 7, Batch: 14, Loss: 0.5893, Elapsed: 0m3s
2023-01-22 21:33:43.459927: Epoch: 7, Batch: 15, Loss: 0.5154, Elapsed: 0m3s
2023-01-22 21:33:48.165962: Epoch: 7, Batch: 16, Loss: 0.5567, Elapsed: 0m4s
2023-01-22 21:33:52.223171: Epoch: 7, Batch: 17, Loss: 0.5560, Elapsed: 0m4s
2023-01-22 21:33:54.393691: Epoch: 7, Batch: 18, Loss: 0.5281, Elapsed: 0m2s
2023-01-22 21:33:55.898791: Epoch: 7, Batch: 19, Loss: 0.4766, Elapsed: 0m1s
2023-01-22 21:33:58.276823: Epoch: 7, Batch: 20, Loss: 0.5707, Elapsed: 0m2s
2023-01-22 21:34:01.153975: Epoch: 7, Batch: 21, Loss: 0.5825, Elapsed: 0m2s
2023-01-22 21:34:02.428496: Epoch: 7, Batch: 22, Loss: 0.5167, Elapsed: 0m1s
2023-01-22 21:34:05.718580: Epoch: 7, Batch: 23, Loss: 0.5431, Elapsed: 0m3s
2023-01-22 21:34:09.549087: Epoch: 7, Batch: 24, Loss: 0.5582, Elapsed: 0m3s
2023-01-22 21:34:11.332522: Epoch: 7, Batch: 25, Loss: 0.5276, Elapsed: 0m1s
2023-01-22 21:34:14.551638: Epoch: 7, Batch: 26, Loss: 0.5091, Elapsed: 0m3s
2023-01-22 21:34:16.050923: Epoch: 7, Batch: 27, Loss: 0.5143, Elapsed: 0m1s
2023-01-22 21:34:18.842381: Epoch: 7, Batch: 28, Loss: 0.5528, Elapsed: 0m2s
2023-01-22 21:34:21.695734: Epoch: 7, Batch: 29, Loss: 0.5381, Elapsed: 0m2s
2023-01-22 21:34:24.253631: Epoch: 7, Batch: 30, Loss: 0.5068, Elapsed: 0m2s
2023-01-22 21:34:25.816774: Epoch: 7, Batch: 31, Loss: 0.4718, Elapsed: 0m1s
2023-01-22 21:34:26.928643: Epoch: 7, Batch: 32, Loss: 0.4385, Elapsed: 0m1s
2023-01-22 21:34:29.738117: Epoch: 7, Batch: 33, Loss: 0.5462, Elapsed: 0m2s
2023-01-22 21:34:32.638499: Epoch: 7, Batch: 34, Loss: 0.5305, Elapsed: 0m2s
2023-01-22 21:34:34.022938: Epoch: 7, Batch: 35, Loss: 0.4626, Elapsed: 0m1s
2023-01-22 21:34:36.317161: Epoch: 7, Batch: 36, Loss: 0.5054, Elapsed: 0m2s
2023-01-22 21:34:38.384546: Epoch: 7, Batch: 37, Loss: 0.4649, Elapsed: 0m2s
2023-01-22 21:34:41.063078: Epoch: 7, Batch: 38, Loss: 0.5146, Elapsed: 0m2s
2023-01-22 21:34:42.002293: Epoch: 7, Batch: 39, Loss: 0.4385, Elapsed: 0m0s
2023-01-22 21:34:44.558752: Epoch: 7, Batch: 40, Loss: 0.5248, Elapsed: 0m2s
2023-01-22 21:34:46.001521: Epoch: 7, Batch: 41, Loss: 0.5017, Elapsed: 0m1s
2023-01-22 21:34:47.596329: Epoch: 7, Batch: 42, Loss: 0.5072, Elapsed: 0m1s
2023-01-22 21:34:49.273445: Epoch: 7, Batch: 43, Loss: 0.4838, Elapsed: 0m1s
2023-01-22 21:34:50.554638: Epoch: 7, Batch: 44, Loss: 0.4553, Elapsed: 0m1s
2023-01-22 21:34:53.011984: Epoch: 7, Batch: 45, Loss: 0.5073, Elapsed: 0m2s
2023-01-22 21:34:56.536389: Epoch: 7, Batch: 46, Loss: 0.5385, Elapsed: 0m3s
2023-01-22 21:34:57.397855: Epoch: 7, Batch: 47, Loss: 0.3810, Elapsed: 0m0s
2023-01-22 21:34:59.495024: Epoch: 7, Batch: 48, Loss: 0.4951, Elapsed: 0m2s
2023-01-22 21:35:02.559659: Epoch: 7, Batch: 49, Loss: 0.5342, Elapsed: 0m3s
2023-01-22 21:35:05.529887: Epoch: 7, Batch: 50, Loss: 0.5621, Elapsed: 0m2s
2023-01-22 21:35:05.541243 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:35:22.940034: validation Test:  Loss: 0.5057,  AUC: 0.8104, Acc: 72.3626,  Precision: 0.8852 -- Elapsed: 0m17s
2023-01-22 21:35:22.940178 Starting testing the train set with 20 subgraphs!
2023-01-22 21:37:00.119494: training Test:  Loss: 0.5318,  AUC: 0.7860, Acc: 71.4452,  Precision: 0.8563 -- Elapsed: 1m37s
2023-01-22 21:37:01.182084: Epoch: 7, Batch: 51, Loss: 0.3680, Elapsed: 0m1s
2023-01-22 21:37:03.552606: Epoch: 7, Batch: 52, Loss: 0.5540, Elapsed: 0m2s
2023-01-22 21:37:08.685359: Epoch: 7, Batch: 53, Loss: 0.5629, Elapsed: 0m5s
2023-01-22 21:37:10.192642: Epoch: 7, Batch: 54, Loss: 0.4639, Elapsed: 0m1s
2023-01-22 21:37:12.028465: Epoch: 7, Batch: 55, Loss: 0.5218, Elapsed: 0m1s
2023-01-22 21:37:14.128220: Epoch: 7, Batch: 56, Loss: 0.5427, Elapsed: 0m2s
2023-01-22 21:37:16.602251: Epoch: 7, Batch: 57, Loss: 0.5208, Elapsed: 0m2s
2023-01-22 21:37:20.469259: Epoch: 7, Batch: 58, Loss: 0.5343, Elapsed: 0m3s
2023-01-22 21:37:21.744782: Epoch: 7, Batch: 59, Loss: 0.4797, Elapsed: 0m1s
2023-01-22 21:37:23.121974: Epoch: 7, Batch: 60, Loss: 0.4963, Elapsed: 0m1s
2023-01-22 21:37:27.720564: Epoch: 7, Batch: 61, Loss: 0.5338, Elapsed: 0m4s
2023-01-22 21:37:30.692453: Epoch: 7, Batch: 62, Loss: 0.5805, Elapsed: 0m2s
2023-01-22 21:37:34.815234: Epoch: 7, Batch: 63, Loss: 0.5699, Elapsed: 0m4s
2023-01-22 21:37:36.083373: Epoch: 7, Batch: 64, Loss: 0.5229, Elapsed: 0m1s
2023-01-22 21:37:37.527904: Epoch: 7, Batch: 65, Loss: 0.4739, Elapsed: 0m1s
2023-01-22 21:37:39.474262: Epoch: 7, Batch: 66, Loss: 0.5390, Elapsed: 0m1s
2023-01-22 21:37:40.802170: Epoch: 7, Batch: 67, Loss: 0.4606, Elapsed: 0m1s
2023-01-22 21:37:42.239478: Epoch: 7, Batch: 68, Loss: 0.5513, Elapsed: 0m1s
2023-01-22 21:37:45.268223: Epoch: 7, Batch: 69, Loss: 0.5480, Elapsed: 0m3s
2023-01-22 21:37:47.574134: Epoch: 7, Batch: 70, Loss: 0.5177, Elapsed: 0m2s
2023-01-22 21:37:51.083616: Epoch: 7, Batch: 71, Loss: 0.5542, Elapsed: 0m3s
2023-01-22 21:37:53.750931: Epoch: 7, Batch: 72, Loss: 0.5372, Elapsed: 0m2s
2023-01-22 21:37:58.388268: Epoch: 7, Batch: 73, Loss: 0.5397, Elapsed: 0m4s
2023-01-22 21:38:01.232074: Epoch: 7, Batch: 74, Loss: 0.5036, Elapsed: 0m2s
2023-01-22 21:38:03.447728: Epoch: 7, Batch: 75, Loss: 0.5212, Elapsed: 0m2s
2023-01-22 21:38:06.246106: Epoch: 7, Batch: 76, Loss: 0.5723, Elapsed: 0m2s
2023-01-22 21:38:08.252311: Epoch: 7, Batch: 77, Loss: 0.5201, Elapsed: 0m1s
2023-01-22 21:38:10.750543: Epoch: 7, Batch: 78, Loss: 0.5089, Elapsed: 0m2s
2023-01-22 21:38:13.375272: Epoch: 7, Batch: 79, Loss: 0.5309, Elapsed: 0m2s
2023-01-22 21:38:16.327938: Epoch: 7, Batch: 80, Loss: 0.5515, Elapsed: 0m2s
2023-01-22 21:38:17.649691: Epoch: 8, Batch: 1, Loss: 0.4569, Elapsed: 0m1s
2023-01-22 21:38:19.102401: Epoch: 8, Batch: 2, Loss: 0.4741, Elapsed: 0m1s
2023-01-22 21:38:21.571868: Epoch: 8, Batch: 3, Loss: 0.5125, Elapsed: 0m2s
2023-01-22 21:38:22.689120: Epoch: 8, Batch: 4, Loss: 0.4274, Elapsed: 0m1s
2023-01-22 21:38:24.138041: Epoch: 8, Batch: 5, Loss: 0.5009, Elapsed: 0m1s
2023-01-22 21:38:26.150865: Epoch: 8, Batch: 6, Loss: 0.5238, Elapsed: 0m2s
2023-01-22 21:38:28.987293: Epoch: 8, Batch: 7, Loss: 0.5407, Elapsed: 0m2s
2023-01-22 21:38:31.894215: Epoch: 8, Batch: 8, Loss: 0.5288, Elapsed: 0m2s
2023-01-22 21:38:34.523165: Epoch: 8, Batch: 9, Loss: 0.5313, Elapsed: 0m2s
2023-01-22 21:38:36.824882: Epoch: 8, Batch: 10, Loss: 0.5109, Elapsed: 0m2s
2023-01-22 21:38:39.173311: Epoch: 8, Batch: 11, Loss: 0.5657, Elapsed: 0m2s
2023-01-22 21:38:40.580475: Epoch: 8, Batch: 12, Loss: 0.4505, Elapsed: 0m1s
2023-01-22 21:38:44.721886: Epoch: 8, Batch: 13, Loss: 0.5710, Elapsed: 0m4s
2023-01-22 21:38:50.154905: Epoch: 8, Batch: 14, Loss: 0.5381, Elapsed: 0m5s
2023-01-22 21:38:53.724627: Epoch: 8, Batch: 15, Loss: 0.5344, Elapsed: 0m3s
2023-01-22 21:38:55.161291: Epoch: 8, Batch: 16, Loss: 0.5456, Elapsed: 0m1s
2023-01-22 21:38:56.052910: Epoch: 8, Batch: 17, Loss: 0.4335, Elapsed: 0m0s
2023-01-22 21:38:58.522813: Epoch: 8, Batch: 18, Loss: 0.5424, Elapsed: 0m2s
2023-01-22 21:39:00.610498: Epoch: 8, Batch: 19, Loss: 0.4647, Elapsed: 0m2s
2023-01-22 21:39:03.401955: Epoch: 8, Batch: 20, Loss: 0.5102, Elapsed: 0m2s
2023-01-22 21:39:05.598859: Epoch: 8, Batch: 21, Loss: 0.5079, Elapsed: 0m2s
2023-01-22 21:39:07.842676: Epoch: 8, Batch: 22, Loss: 0.5364, Elapsed: 0m2s
2023-01-22 21:39:09.140875: Epoch: 8, Batch: 23, Loss: 0.4516, Elapsed: 0m1s
2023-01-22 21:39:13.016670: Epoch: 8, Batch: 24, Loss: 0.5383, Elapsed: 0m3s
2023-01-22 21:39:16.196296: Epoch: 8, Batch: 25, Loss: 0.5682, Elapsed: 0m3s
2023-01-22 21:39:17.797242: Epoch: 8, Batch: 26, Loss: 0.5115, Elapsed: 0m1s
2023-01-22 21:39:20.363733: Epoch: 8, Batch: 27, Loss: 0.5103, Elapsed: 0m2s
2023-01-22 21:39:21.857267: Epoch: 8, Batch: 28, Loss: 0.5043, Elapsed: 0m1s
2023-01-22 21:39:23.346442: Epoch: 8, Batch: 29, Loss: 0.4654, Elapsed: 0m1s
2023-01-22 21:39:26.074901: Epoch: 8, Batch: 30, Loss: 0.5127, Elapsed: 0m2s
2023-01-22 21:39:29.550829: Epoch: 8, Batch: 31, Loss: 0.5578, Elapsed: 0m3s
2023-01-22 21:39:32.198548: Epoch: 8, Batch: 32, Loss: 0.5367, Elapsed: 0m2s
2023-01-22 21:39:35.221012: Epoch: 8, Batch: 33, Loss: 0.5339, Elapsed: 0m3s
2023-01-22 21:39:37.337607: Epoch: 8, Batch: 34, Loss: 0.5539, Elapsed: 0m2s
2023-01-22 21:39:40.325211: Epoch: 8, Batch: 35, Loss: 0.5714, Elapsed: 0m2s
2023-01-22 21:39:42.704205: Epoch: 8, Batch: 36, Loss: 0.5498, Elapsed: 0m2s
2023-01-22 21:39:44.987204: Epoch: 8, Batch: 37, Loss: 0.5046, Elapsed: 0m2s
2023-01-22 21:39:46.283788: Epoch: 8, Batch: 38, Loss: 0.4786, Elapsed: 0m1s
2023-01-22 21:39:49.281407: Epoch: 8, Batch: 39, Loss: 0.5616, Elapsed: 0m2s
2023-01-22 21:39:53.854887: Epoch: 8, Batch: 40, Loss: 0.5353, Elapsed: 0m4s
2023-01-22 21:39:55.938468: Epoch: 8, Batch: 41, Loss: 0.4838, Elapsed: 0m2s
2023-01-22 21:39:58.415557: Epoch: 8, Batch: 42, Loss: 0.5203, Elapsed: 0m2s
2023-01-22 21:39:59.774893: Epoch: 8, Batch: 43, Loss: 0.4923, Elapsed: 0m1s
2023-01-22 21:40:01.615673: Epoch: 8, Batch: 44, Loss: 0.5115, Elapsed: 0m1s
2023-01-22 21:40:06.934307: Epoch: 8, Batch: 45, Loss: 0.5624, Elapsed: 0m5s
2023-01-22 21:40:08.671211: Epoch: 8, Batch: 46, Loss: 0.4476, Elapsed: 0m1s
2023-01-22 21:40:10.722522: Epoch: 8, Batch: 47, Loss: 0.5119, Elapsed: 0m2s
2023-01-22 21:40:13.694813: Epoch: 8, Batch: 48, Loss: 0.5264, Elapsed: 0m2s
2023-01-22 21:40:16.748046: Epoch: 8, Batch: 49, Loss: 0.5288, Elapsed: 0m3s
2023-01-22 21:40:18.756196: Epoch: 8, Batch: 50, Loss: 0.5081, Elapsed: 0m1s
2023-01-22 21:40:18.768974 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:40:36.158655: validation Test:  Loss: 0.5011,  AUC: 0.8121, Acc: 73.1798,  Precision: 0.8414 -- Elapsed: 0m17s
2023-01-22 21:40:36.158760 Starting testing the train set with 20 subgraphs!
2023-01-22 21:42:12.274470: training Test:  Loss: 0.5280,  AUC: 0.7893, Acc: 72.0182,  Precision: 0.8169 -- Elapsed: 1m36s
2023-01-22 21:42:14.777288: Epoch: 8, Batch: 51, Loss: 0.5334, Elapsed: 0m2s
2023-01-22 21:42:17.850014: Epoch: 8, Batch: 52, Loss: 0.5753, Elapsed: 0m3s
2023-01-22 21:42:21.497339: Epoch: 8, Batch: 53, Loss: 0.5088, Elapsed: 0m3s
2023-01-22 21:42:22.937384: Epoch: 8, Batch: 54, Loss: 0.4962, Elapsed: 0m1s
2023-01-22 21:42:25.385844: Epoch: 8, Batch: 55, Loss: 0.5207, Elapsed: 0m2s
2023-01-22 21:42:27.234893: Epoch: 8, Batch: 56, Loss: 0.5179, Elapsed: 0m1s
2023-01-22 21:42:30.041607: Epoch: 8, Batch: 57, Loss: 0.5546, Elapsed: 0m2s
2023-01-22 21:42:31.983019: Epoch: 8, Batch: 58, Loss: 0.5401, Elapsed: 0m1s
2023-01-22 21:42:34.654713: Epoch: 8, Batch: 59, Loss: 0.5612, Elapsed: 0m2s
2023-01-22 21:42:37.306936: Epoch: 8, Batch: 60, Loss: 0.5112, Elapsed: 0m2s
2023-01-22 21:42:41.135599: Epoch: 8, Batch: 61, Loss: 0.5674, Elapsed: 0m3s
2023-01-22 21:42:42.401311: Epoch: 8, Batch: 62, Loss: 0.5184, Elapsed: 0m1s
2023-01-22 21:42:45.273002: Epoch: 8, Batch: 63, Loss: 0.5495, Elapsed: 0m2s
2023-01-22 21:42:47.290936: Epoch: 8, Batch: 64, Loss: 0.5098, Elapsed: 0m2s
2023-01-22 21:42:49.773285: Epoch: 8, Batch: 65, Loss: 0.5095, Elapsed: 0m2s
2023-01-22 21:42:54.509646: Epoch: 8, Batch: 66, Loss: 0.5543, Elapsed: 0m4s
2023-01-22 21:42:56.188889: Epoch: 8, Batch: 67, Loss: 0.4888, Elapsed: 0m1s
2023-01-22 21:43:00.240659: Epoch: 8, Batch: 68, Loss: 0.5584, Elapsed: 0m4s
2023-01-22 21:43:04.028286: Epoch: 8, Batch: 69, Loss: 0.5389, Elapsed: 0m3s
2023-01-22 21:43:06.845020: Epoch: 8, Batch: 70, Loss: 0.5458, Elapsed: 0m2s
2023-01-22 21:43:09.136724: Epoch: 8, Batch: 71, Loss: 0.5102, Elapsed: 0m2s
2023-01-22 21:43:11.926958: Epoch: 8, Batch: 72, Loss: 0.5658, Elapsed: 0m2s
2023-01-22 21:43:16.345795: Epoch: 8, Batch: 73, Loss: 0.5285, Elapsed: 0m4s
2023-01-22 21:43:19.612977: Epoch: 8, Batch: 74, Loss: 0.5416, Elapsed: 0m3s
2023-01-22 21:43:20.641997: Epoch: 8, Batch: 75, Loss: 0.3912, Elapsed: 0m1s
2023-01-22 21:43:22.405774: Epoch: 8, Batch: 76, Loss: 0.4645, Elapsed: 0m1s
2023-01-22 21:43:26.471602: Epoch: 8, Batch: 77, Loss: 0.5881, Elapsed: 0m4s
2023-01-22 21:43:28.069859: Epoch: 8, Batch: 78, Loss: 0.4679, Elapsed: 0m1s
2023-01-22 21:43:28.946196: Epoch: 8, Batch: 79, Loss: 0.3927, Elapsed: 0m0s
2023-01-22 21:43:31.899181: Epoch: 8, Batch: 80, Loss: 0.5570, Elapsed: 0m2s
2023-01-22 21:43:33.897094: Epoch: 9, Batch: 1, Loss: 0.5149, Elapsed: 0m1s
2023-01-22 21:43:37.152226: Epoch: 9, Batch: 2, Loss: 0.5447, Elapsed: 0m3s
2023-01-22 21:43:40.310350: Epoch: 9, Batch: 3, Loss: 0.5648, Elapsed: 0m3s
2023-01-22 21:43:43.354624: Epoch: 9, Batch: 4, Loss: 0.5402, Elapsed: 0m3s
2023-01-22 21:43:44.926786: Epoch: 9, Batch: 5, Loss: 0.4586, Elapsed: 0m1s
2023-01-22 21:43:46.304974: Epoch: 9, Batch: 6, Loss: 0.4920, Elapsed: 0m1s
2023-01-22 21:43:48.415955: Epoch: 9, Batch: 7, Loss: 0.5228, Elapsed: 0m2s
2023-01-22 21:43:50.055473: Epoch: 9, Batch: 8, Loss: 0.4642, Elapsed: 0m1s
2023-01-22 21:43:52.147701: Epoch: 9, Batch: 9, Loss: 0.4642, Elapsed: 0m2s
2023-01-22 21:43:53.989420: Epoch: 9, Batch: 10, Loss: 0.5172, Elapsed: 0m1s
2023-01-22 21:43:56.474937: Epoch: 9, Batch: 11, Loss: 0.5457, Elapsed: 0m2s
2023-01-22 21:43:58.766047: Epoch: 9, Batch: 12, Loss: 0.5073, Elapsed: 0m2s
2023-01-22 21:44:03.842346: Epoch: 9, Batch: 13, Loss: 0.5476, Elapsed: 0m5s
2023-01-22 21:44:06.333353: Epoch: 9, Batch: 14, Loss: 0.5122, Elapsed: 0m2s
2023-01-22 21:44:07.926153: Epoch: 9, Batch: 15, Loss: 0.5101, Elapsed: 0m1s
2023-01-22 21:44:10.593143: Epoch: 9, Batch: 16, Loss: 0.5423, Elapsed: 0m2s
2023-01-22 21:44:14.114103: Epoch: 9, Batch: 17, Loss: 0.5326, Elapsed: 0m3s
2023-01-22 21:44:14.985034: Epoch: 9, Batch: 18, Loss: 0.4268, Elapsed: 0m0s
2023-01-22 21:44:16.306948: Epoch: 9, Batch: 19, Loss: 0.4586, Elapsed: 0m1s
2023-01-22 21:44:17.803476: Epoch: 9, Batch: 20, Loss: 0.4436, Elapsed: 0m1s
2023-01-22 21:44:20.433153: Epoch: 9, Batch: 21, Loss: 0.5296, Elapsed: 0m2s
2023-01-22 21:44:22.814362: Epoch: 9, Batch: 22, Loss: 0.5454, Elapsed: 0m2s
2023-01-22 21:44:25.484190: Epoch: 9, Batch: 23, Loss: 0.5107, Elapsed: 0m2s
2023-01-22 21:44:26.783659: Epoch: 9, Batch: 24, Loss: 0.4325, Elapsed: 0m1s
2023-01-22 21:44:28.255188: Epoch: 9, Batch: 25, Loss: 0.5229, Elapsed: 0m1s
2023-01-22 21:44:29.982677: Epoch: 9, Batch: 26, Loss: 0.4756, Elapsed: 0m1s
2023-01-22 21:44:33.901525: Epoch: 9, Batch: 27, Loss: 0.5570, Elapsed: 0m3s
2023-01-22 21:44:36.364537: Epoch: 9, Batch: 28, Loss: 0.5181, Elapsed: 0m2s
2023-01-22 21:44:41.079229: Epoch: 9, Batch: 29, Loss: 0.5528, Elapsed: 0m4s
2023-01-22 21:44:43.958280: Epoch: 9, Batch: 30, Loss: 0.5471, Elapsed: 0m2s
2023-01-22 21:44:46.932549: Epoch: 9, Batch: 31, Loss: 0.5704, Elapsed: 0m2s
2023-01-22 21:44:48.365917: Epoch: 9, Batch: 32, Loss: 0.5452, Elapsed: 0m1s
2023-01-22 21:44:51.227394: Epoch: 9, Batch: 33, Loss: 0.5408, Elapsed: 0m2s
2023-01-22 21:44:54.392066: Epoch: 9, Batch: 34, Loss: 0.5336, Elapsed: 0m3s
2023-01-22 21:44:56.351088: Epoch: 9, Batch: 35, Loss: 0.5378, Elapsed: 0m1s
2023-01-22 21:44:59.302464: Epoch: 9, Batch: 36, Loss: 0.5477, Elapsed: 0m2s
2023-01-22 21:45:02.203972: Epoch: 9, Batch: 37, Loss: 0.5284, Elapsed: 0m2s
2023-01-22 21:45:03.484267: Epoch: 9, Batch: 38, Loss: 0.4658, Elapsed: 0m1s
2023-01-22 21:45:05.854820: Epoch: 9, Batch: 39, Loss: 0.5625, Elapsed: 0m2s
2023-01-22 21:45:07.124271: Epoch: 9, Batch: 40, Loss: 0.4660, Elapsed: 0m1s
2023-01-22 21:45:09.596146: Epoch: 9, Batch: 41, Loss: 0.4997, Elapsed: 0m2s
2023-01-22 21:45:12.422926: Epoch: 9, Batch: 42, Loss: 0.5524, Elapsed: 0m2s
2023-01-22 21:45:16.474816: Epoch: 9, Batch: 43, Loss: 0.5562, Elapsed: 0m4s
2023-01-22 21:45:19.694399: Epoch: 9, Batch: 44, Loss: 0.5120, Elapsed: 0m3s
2023-01-22 21:45:21.996568: Epoch: 9, Batch: 45, Loss: 0.5219, Elapsed: 0m2s
2023-01-22 21:45:24.788767: Epoch: 9, Batch: 46, Loss: 0.5702, Elapsed: 0m2s
2023-01-22 21:45:26.180689: Epoch: 9, Batch: 47, Loss: 0.4515, Elapsed: 0m1s
2023-01-22 21:45:30.313230: Epoch: 9, Batch: 48, Loss: 0.5693, Elapsed: 0m4s
2023-01-22 21:45:31.790601: Epoch: 9, Batch: 49, Loss: 0.5030, Elapsed: 0m1s
2023-01-22 21:45:33.724581: Epoch: 9, Batch: 50, Loss: 0.4919, Elapsed: 0m1s
2023-01-22 21:45:33.734662 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:45:51.287334: validation Test:  Loss: 0.5067,  AUC: 0.8087, Acc: 72.5111,  Precision: 0.8752 -- Elapsed: 0m17s
2023-01-22 21:45:51.287377 Starting testing the train set with 20 subgraphs!
2023-01-22 21:47:27.116572: training Test:  Loss: 0.5327,  AUC: 0.7848, Acc: 71.7052,  Precision: 0.8575 -- Elapsed: 1m35s
2023-01-22 21:47:28.594541: Epoch: 9, Batch: 51, Loss: 0.4750, Elapsed: 0m1s
2023-01-22 21:47:30.732069: Epoch: 9, Batch: 52, Loss: 0.5454, Elapsed: 0m2s
2023-01-22 21:47:33.190168: Epoch: 9, Batch: 53, Loss: 0.5241, Elapsed: 0m2s
2023-01-22 21:47:37.586859: Epoch: 9, Batch: 54, Loss: 0.5396, Elapsed: 0m4s
2023-01-22 21:47:40.441329: Epoch: 9, Batch: 55, Loss: 0.5134, Elapsed: 0m2s
2023-01-22 21:47:44.504935: Epoch: 9, Batch: 56, Loss: 0.5930, Elapsed: 0m4s
2023-01-22 21:47:48.635892: Epoch: 9, Batch: 57, Loss: 0.5409, Elapsed: 0m4s
2023-01-22 21:47:50.130803: Epoch: 9, Batch: 58, Loss: 0.5153, Elapsed: 0m1s
2023-01-22 21:47:51.015062: Epoch: 9, Batch: 59, Loss: 0.3912, Elapsed: 0m0s
2023-01-22 21:47:53.025497: Epoch: 9, Batch: 60, Loss: 0.5091, Elapsed: 0m2s
2023-01-22 21:47:55.501287: Epoch: 9, Batch: 61, Loss: 0.5091, Elapsed: 0m2s
2023-01-22 21:47:57.613890: Epoch: 9, Batch: 62, Loss: 0.4855, Elapsed: 0m2s
2023-01-22 21:48:00.478738: Epoch: 9, Batch: 63, Loss: 0.5780, Elapsed: 0m2s
2023-01-22 21:48:03.277630: Epoch: 9, Batch: 64, Loss: 0.5481, Elapsed: 0m2s
2023-01-22 21:48:05.448765: Epoch: 9, Batch: 65, Loss: 0.5282, Elapsed: 0m2s
2023-01-22 21:48:09.250076: Epoch: 9, Batch: 66, Loss: 0.5667, Elapsed: 0m3s
2023-01-22 21:48:12.754506: Epoch: 9, Batch: 67, Loss: 0.5498, Elapsed: 0m3s
2023-01-22 21:48:14.858366: Epoch: 9, Batch: 68, Loss: 0.5360, Elapsed: 0m2s
2023-01-22 21:48:17.813156: Epoch: 9, Batch: 69, Loss: 0.5280, Elapsed: 0m2s
2023-01-22 21:48:20.288426: Epoch: 9, Batch: 70, Loss: 0.5390, Elapsed: 0m2s
2023-01-22 21:48:24.851718: Epoch: 9, Batch: 71, Loss: 0.5327, Elapsed: 0m4s
2023-01-22 21:48:26.312112: Epoch: 9, Batch: 72, Loss: 0.5066, Elapsed: 0m1s
2023-01-22 21:48:29.310331: Epoch: 9, Batch: 73, Loss: 0.5635, Elapsed: 0m2s
2023-01-22 21:48:31.872047: Epoch: 9, Batch: 74, Loss: 0.5020, Elapsed: 0m2s
2023-01-22 21:48:33.644151: Epoch: 9, Batch: 75, Loss: 0.5257, Elapsed: 0m1s
2023-01-22 21:48:35.563188: Epoch: 9, Batch: 76, Loss: 0.5088, Elapsed: 0m1s
2023-01-22 21:48:38.239131: Epoch: 9, Batch: 77, Loss: 0.5598, Elapsed: 0m2s
2023-01-22 21:48:39.258011: Epoch: 9, Batch: 78, Loss: 0.3736, Elapsed: 0m1s
2023-01-22 21:48:41.088039: Epoch: 9, Batch: 79, Loss: 0.5183, Elapsed: 0m1s
2023-01-22 21:48:46.826405: Epoch: 9, Batch: 80, Loss: 0.5371, Elapsed: 0m5s
2023-01-22 21:48:51.081682: Epoch: 10, Batch: 1, Loss: 0.5265, Elapsed: 0m4s
2023-01-22 21:48:52.646456: Epoch: 10, Batch: 2, Loss: 0.4754, Elapsed: 0m1s
2023-01-22 21:48:55.656385: Epoch: 10, Batch: 3, Loss: 0.5612, Elapsed: 0m2s
2023-01-22 21:49:01.067990: Epoch: 10, Batch: 4, Loss: 0.5373, Elapsed: 0m5s
2023-01-22 21:49:02.367240: Epoch: 10, Batch: 5, Loss: 0.5225, Elapsed: 0m1s
2023-01-22 21:49:04.854407: Epoch: 10, Batch: 6, Loss: 0.5021, Elapsed: 0m2s
2023-01-22 21:49:08.183704: Epoch: 10, Batch: 7, Loss: 0.5410, Elapsed: 0m3s
2023-01-22 21:49:12.348221: Epoch: 10, Batch: 8, Loss: 0.5552, Elapsed: 0m4s
2023-01-22 21:49:14.852830: Epoch: 10, Batch: 9, Loss: 0.5073, Elapsed: 0m2s
2023-01-22 21:49:19.568542: Epoch: 10, Batch: 10, Loss: 0.5513, Elapsed: 0m4s
2023-01-22 21:49:21.882738: Epoch: 10, Batch: 11, Loss: 0.5150, Elapsed: 0m2s
2023-01-22 21:49:23.665788: Epoch: 10, Batch: 12, Loss: 0.5136, Elapsed: 0m1s
2023-01-22 21:49:28.093038: Epoch: 10, Batch: 13, Loss: 0.5281, Elapsed: 0m4s
2023-01-22 21:49:29.413984: Epoch: 10, Batch: 14, Loss: 0.4656, Elapsed: 0m1s
2023-01-22 21:49:32.635896: Epoch: 10, Batch: 15, Loss: 0.5017, Elapsed: 0m3s
2023-01-22 21:49:35.657268: Epoch: 10, Batch: 16, Loss: 0.5331, Elapsed: 0m3s
2023-01-22 21:49:37.054657: Epoch: 10, Batch: 17, Loss: 0.4470, Elapsed: 0m1s
2023-01-22 21:49:39.861993: Epoch: 10, Batch: 18, Loss: 0.5399, Elapsed: 0m2s
2023-01-22 21:49:41.224699: Epoch: 10, Batch: 19, Loss: 0.4906, Elapsed: 0m1s
2023-01-22 21:49:43.143424: Epoch: 10, Batch: 20, Loss: 0.5065, Elapsed: 0m1s
2023-01-22 21:49:44.174602: Epoch: 10, Batch: 21, Loss: 0.3690, Elapsed: 0m1s
2023-01-22 21:49:45.053647: Epoch: 10, Batch: 22, Loss: 0.3698, Elapsed: 0m0s
2023-01-22 21:49:48.897709: Epoch: 10, Batch: 23, Loss: 0.5552, Elapsed: 0m3s
2023-01-22 21:49:51.531655: Epoch: 10, Batch: 24, Loss: 0.5454, Elapsed: 0m2s
2023-01-22 21:49:55.408661: Epoch: 10, Batch: 25, Loss: 0.5503, Elapsed: 0m3s
2023-01-22 21:49:58.573039: Epoch: 10, Batch: 26, Loss: 0.5588, Elapsed: 0m3s
2023-01-22 21:49:59.869268: Epoch: 10, Batch: 27, Loss: 0.4787, Elapsed: 0m1s
2023-01-22 21:50:01.714346: Epoch: 10, Batch: 28, Loss: 0.5195, Elapsed: 0m1s
2023-01-22 21:50:03.739736: Epoch: 10, Batch: 29, Loss: 0.5294, Elapsed: 0m2s
2023-01-22 21:50:05.810524: Epoch: 10, Batch: 30, Loss: 0.4647, Elapsed: 0m2s
2023-01-22 21:50:07.299151: Epoch: 10, Batch: 31, Loss: 0.4515, Elapsed: 0m1s
2023-01-22 21:50:09.443869: Epoch: 10, Batch: 32, Loss: 0.5620, Elapsed: 0m2s
2023-01-22 21:50:12.395439: Epoch: 10, Batch: 33, Loss: 0.5470, Elapsed: 0m2s
2023-01-22 21:50:15.262801: Epoch: 10, Batch: 34, Loss: 0.5729, Elapsed: 0m2s
2023-01-22 21:50:17.803330: Epoch: 10, Batch: 35, Loss: 0.5035, Elapsed: 0m2s
2023-01-22 21:50:19.520996: Epoch: 10, Batch: 36, Loss: 0.4560, Elapsed: 0m1s
2023-01-22 21:50:21.989925: Epoch: 10, Batch: 37, Loss: 0.5428, Elapsed: 0m2s
2023-01-22 21:50:24.763541: Epoch: 10, Batch: 38, Loss: 0.5606, Elapsed: 0m2s
2023-01-22 21:50:25.914388: Epoch: 10, Batch: 39, Loss: 0.4235, Elapsed: 0m1s
2023-01-22 21:50:29.491904: Epoch: 10, Batch: 40, Loss: 0.5830, Elapsed: 0m3s
2023-01-22 21:50:30.946452: Epoch: 10, Batch: 41, Loss: 0.4722, Elapsed: 0m1s
2023-01-22 21:50:32.446774: Epoch: 10, Batch: 42, Loss: 0.4784, Elapsed: 0m1s
2023-01-22 21:50:35.122906: Epoch: 10, Batch: 43, Loss: 0.5568, Elapsed: 0m2s
2023-01-22 21:50:38.166428: Epoch: 10, Batch: 44, Loss: 0.5325, Elapsed: 0m3s
2023-01-22 21:50:40.817077: Epoch: 10, Batch: 45, Loss: 0.5396, Elapsed: 0m2s
2023-01-22 21:50:45.039010: Epoch: 10, Batch: 46, Loss: 0.5784, Elapsed: 0m4s
2023-01-22 21:50:47.213962: Epoch: 10, Batch: 47, Loss: 0.5460, Elapsed: 0m2s
2023-01-22 21:50:49.523113: Epoch: 10, Batch: 48, Loss: 0.5127, Elapsed: 0m2s
2023-01-22 21:50:51.700196: Epoch: 10, Batch: 49, Loss: 0.5074, Elapsed: 0m2s
2023-01-22 21:50:54.185410: Epoch: 10, Batch: 50, Loss: 0.5059, Elapsed: 0m2s
2023-01-22 21:50:54.195630 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:51:12.448738: validation Test:  Loss: 0.5010,  AUC: 0.8105, Acc: 72.7836,  Precision: 0.8775 -- Elapsed: 0m18s
2023-01-22 21:51:12.448842 Starting testing the train set with 20 subgraphs!
2023-01-22 21:52:48.426427: training Test:  Loss: 0.5273,  AUC: 0.7872, Acc: 71.7302,  Precision: 0.8610 -- Elapsed: 1m35s
2023-01-22 21:52:51.980057: Epoch: 10, Batch: 51, Loss: 0.5321, Elapsed: 0m3s
2023-01-22 21:52:54.962706: Epoch: 10, Batch: 52, Loss: 0.5302, Elapsed: 0m2s
2023-01-22 21:52:59.550376: Epoch: 10, Batch: 53, Loss: 0.5320, Elapsed: 0m4s
2023-01-22 21:53:02.019869: Epoch: 10, Batch: 54, Loss: 0.5264, Elapsed: 0m2s
2023-01-22 21:53:04.859338: Epoch: 10, Batch: 55, Loss: 0.5279, Elapsed: 0m2s
2023-01-22 21:53:08.106042: Epoch: 10, Batch: 56, Loss: 0.5463, Elapsed: 0m3s
2023-01-22 21:53:10.344896: Epoch: 10, Batch: 57, Loss: 0.5431, Elapsed: 0m2s
2023-01-22 21:53:12.424768: Epoch: 10, Batch: 58, Loss: 0.5081, Elapsed: 0m2s
2023-01-22 21:53:13.700888: Epoch: 10, Batch: 59, Loss: 0.4589, Elapsed: 0m1s
2023-01-22 21:53:17.549110: Epoch: 10, Batch: 60, Loss: 0.5338, Elapsed: 0m3s
2023-01-22 21:53:22.795332: Epoch: 10, Batch: 61, Loss: 0.5470, Elapsed: 0m5s
2023-01-22 21:53:25.471286: Epoch: 10, Batch: 62, Loss: 0.5094, Elapsed: 0m2s
2023-01-22 21:53:27.472369: Epoch: 10, Batch: 63, Loss: 0.5117, Elapsed: 0m1s
2023-01-22 21:53:29.938650: Epoch: 10, Batch: 64, Loss: 0.5187, Elapsed: 0m2s
2023-01-22 21:53:30.828863: Epoch: 10, Batch: 65, Loss: 0.4305, Elapsed: 0m0s
2023-01-22 21:53:32.285979: Epoch: 10, Batch: 66, Loss: 0.4968, Elapsed: 0m1s
2023-01-22 21:53:33.721909: Epoch: 10, Batch: 67, Loss: 0.5424, Elapsed: 0m1s
2023-01-22 21:53:36.090649: Epoch: 10, Batch: 68, Loss: 0.5682, Elapsed: 0m2s
2023-01-22 21:53:39.000393: Epoch: 10, Batch: 69, Loss: 0.5296, Elapsed: 0m2s
2023-01-22 21:53:41.459378: Epoch: 10, Batch: 70, Loss: 0.5360, Elapsed: 0m2s
2023-01-22 21:53:43.294912: Epoch: 10, Batch: 71, Loss: 0.5086, Elapsed: 0m1s
2023-01-22 21:53:44.976957: Epoch: 10, Batch: 72, Loss: 0.4789, Elapsed: 0m1s
2023-01-22 21:53:46.571908: Epoch: 10, Batch: 73, Loss: 0.5107, Elapsed: 0m1s
2023-01-22 21:53:49.299037: Epoch: 10, Batch: 74, Loss: 0.5140, Elapsed: 0m2s
2023-01-22 21:53:51.385503: Epoch: 10, Batch: 75, Loss: 0.4873, Elapsed: 0m2s
2023-01-22 21:53:54.204901: Epoch: 10, Batch: 76, Loss: 0.5531, Elapsed: 0m2s
2023-01-22 21:53:55.482738: Epoch: 10, Batch: 77, Loss: 0.5033, Elapsed: 0m1s
2023-01-22 21:53:56.980384: Epoch: 10, Batch: 78, Loss: 0.4989, Elapsed: 0m1s
2023-01-22 21:53:59.794534: Epoch: 10, Batch: 79, Loss: 0.5430, Elapsed: 0m2s
2023-01-22 21:54:02.769730: Epoch: 10, Batch: 80, Loss: 0.5799, Elapsed: 0m2s
2023-01-22 21:54:04.877006: Epoch: 11, Batch: 1, Loss: 0.4841, Elapsed: 0m2s
2023-01-22 21:54:06.239593: Epoch: 11, Batch: 2, Loss: 0.4908, Elapsed: 0m1s
2023-01-22 21:54:07.122874: Epoch: 11, Batch: 3, Loss: 0.3741, Elapsed: 0m0s
2023-01-22 21:54:11.672622: Epoch: 11, Batch: 4, Loss: 0.5685, Elapsed: 0m4s
2023-01-22 21:54:14.509699: Epoch: 11, Batch: 5, Loss: 0.5461, Elapsed: 0m2s
2023-01-22 21:54:16.098865: Epoch: 11, Batch: 6, Loss: 0.4615, Elapsed: 0m1s
2023-01-22 21:54:17.386728: Epoch: 11, Batch: 7, Loss: 0.4642, Elapsed: 0m1s
2023-01-22 21:54:18.502572: Epoch: 11, Batch: 8, Loss: 0.4268, Elapsed: 0m1s
2023-01-22 21:54:20.806379: Epoch: 11, Batch: 9, Loss: 0.5151, Elapsed: 0m2s
2023-01-22 21:54:22.910023: Epoch: 11, Batch: 10, Loss: 0.5334, Elapsed: 0m2s
2023-01-22 21:54:28.008576: Epoch: 11, Batch: 11, Loss: 0.5510, Elapsed: 0m5s
2023-01-22 21:54:29.304854: Epoch: 11, Batch: 12, Loss: 0.5037, Elapsed: 0m1s
2023-01-22 21:54:30.190304: Epoch: 11, Batch: 13, Loss: 0.4182, Elapsed: 0m0s
2023-01-22 21:54:33.110849: Epoch: 11, Batch: 14, Loss: 0.5256, Elapsed: 0m2s
2023-01-22 21:54:34.499770: Epoch: 11, Batch: 15, Loss: 0.4503, Elapsed: 0m1s
2023-01-22 21:54:37.610575: Epoch: 11, Batch: 16, Loss: 0.5625, Elapsed: 0m3s
2023-01-22 21:54:40.498915: Epoch: 11, Batch: 17, Loss: 0.5412, Elapsed: 0m2s
2023-01-22 21:54:43.715790: Epoch: 11, Batch: 18, Loss: 0.5033, Elapsed: 0m3s
2023-01-22 21:54:46.515796: Epoch: 11, Batch: 19, Loss: 0.5401, Elapsed: 0m2s
2023-01-22 21:54:49.175779: Epoch: 11, Batch: 20, Loss: 0.5062, Elapsed: 0m2s
2023-01-22 21:54:50.846954: Epoch: 11, Batch: 21, Loss: 0.4786, Elapsed: 0m1s
2023-01-22 21:54:56.273399: Epoch: 11, Batch: 22, Loss: 0.5341, Elapsed: 0m5s
2023-01-22 21:54:57.793016: Epoch: 11, Batch: 23, Loss: 0.5107, Elapsed: 0m1s
2023-01-22 21:55:00.663555: Epoch: 11, Batch: 24, Loss: 0.5462, Elapsed: 0m2s
2023-01-22 21:55:05.089179: Epoch: 11, Batch: 25, Loss: 0.5289, Elapsed: 0m4s
2023-01-22 21:55:09.144352: Epoch: 11, Batch: 26, Loss: 0.5563, Elapsed: 0m4s
2023-01-22 21:55:11.596496: Epoch: 11, Batch: 27, Loss: 0.5257, Elapsed: 0m2s
2023-01-22 21:55:13.120597: Epoch: 11, Batch: 28, Loss: 0.4699, Elapsed: 0m1s
2023-01-22 21:55:16.108227: Epoch: 11, Batch: 29, Loss: 0.5617, Elapsed: 0m2s
2023-01-22 21:55:17.538742: Epoch: 11, Batch: 30, Loss: 0.5433, Elapsed: 0m1s
2023-01-22 21:55:18.868725: Epoch: 11, Batch: 31, Loss: 0.4548, Elapsed: 0m1s
2023-01-22 21:55:23.612883: Epoch: 11, Batch: 32, Loss: 0.5494, Elapsed: 0m4s
2023-01-22 21:55:28.210122: Epoch: 11, Batch: 33, Loss: 0.5286, Elapsed: 0m4s
2023-01-22 21:55:30.873604: Epoch: 11, Batch: 34, Loss: 0.5331, Elapsed: 0m2s
2023-01-22 21:55:32.453123: Epoch: 11, Batch: 35, Loss: 0.4594, Elapsed: 0m1s
2023-01-22 21:55:34.282715: Epoch: 11, Batch: 36, Loss: 0.5063, Elapsed: 0m1s
2023-01-22 21:55:35.321541: Epoch: 11, Batch: 37, Loss: 0.3620, Elapsed: 0m1s
2023-01-22 21:55:38.128466: Epoch: 11, Batch: 38, Loss: 0.5524, Elapsed: 0m2s
2023-01-22 21:55:41.637552: Epoch: 11, Batch: 39, Loss: 0.5342, Elapsed: 0m3s
2023-01-22 21:55:43.127044: Epoch: 11, Batch: 40, Loss: 0.4448, Elapsed: 0m1s
2023-01-22 21:55:45.599339: Epoch: 11, Batch: 41, Loss: 0.5070, Elapsed: 0m2s
2023-01-22 21:55:47.614596: Epoch: 11, Batch: 42, Loss: 0.5164, Elapsed: 0m2s
2023-01-22 21:55:50.180198: Epoch: 11, Batch: 43, Loss: 0.5085, Elapsed: 0m2s
2023-01-22 21:55:52.233743: Epoch: 11, Batch: 44, Loss: 0.4584, Elapsed: 0m2s
2023-01-22 21:55:54.953539: Epoch: 11, Batch: 45, Loss: 0.5107, Elapsed: 0m2s
2023-01-22 21:55:58.780382: Epoch: 11, Batch: 46, Loss: 0.5608, Elapsed: 0m3s
2023-01-22 21:56:01.430093: Epoch: 11, Batch: 47, Loss: 0.5278, Elapsed: 0m2s
2023-01-22 21:56:04.442781: Epoch: 11, Batch: 48, Loss: 0.5375, Elapsed: 0m3s
2023-01-22 21:56:06.468871: Epoch: 11, Batch: 49, Loss: 0.5085, Elapsed: 0m2s
2023-01-22 21:56:09.154155: Epoch: 11, Batch: 50, Loss: 0.5570, Elapsed: 0m2s
2023-01-22 21:56:09.165464 Starting testing the valid set with 20 subgraphs!
2023-01-22 21:56:27.333358: validation Test:  Loss: 0.5000,  AUC: 0.8120, Acc: 73.1220,  Precision: 0.8570 -- Elapsed: 0m18s
2023-01-22 21:56:27.333400 Starting testing the train set with 20 subgraphs!
2023-01-22 21:58:03.218544: training Test:  Loss: 0.5260,  AUC: 0.7897, Acc: 71.9653,  Precision: 0.8384 -- Elapsed: 1m35s
2023-01-22 21:58:05.722167: Epoch: 11, Batch: 51, Loss: 0.5299, Elapsed: 0m2s
2023-01-22 21:58:09.201878: Epoch: 11, Batch: 52, Loss: 0.5494, Elapsed: 0m3s
2023-01-22 21:58:10.466865: Epoch: 11, Batch: 53, Loss: 0.5173, Elapsed: 0m1s
2023-01-22 21:58:11.939044: Epoch: 11, Batch: 54, Loss: 0.4946, Elapsed: 0m1s
2023-01-22 21:58:15.126072: Epoch: 11, Batch: 55, Loss: 0.5569, Elapsed: 0m3s
2023-01-22 21:58:18.979557: Epoch: 11, Batch: 56, Loss: 0.5355, Elapsed: 0m3s
2023-01-22 21:58:21.263706: Epoch: 11, Batch: 57, Loss: 0.5079, Elapsed: 0m2s
2023-01-22 21:58:24.247602: Epoch: 11, Batch: 58, Loss: 0.5734, Elapsed: 0m2s
2023-01-22 21:58:26.211032: Epoch: 11, Batch: 59, Loss: 0.5102, Elapsed: 0m1s
2023-01-22 21:58:30.281457: Epoch: 11, Batch: 60, Loss: 0.5847, Elapsed: 0m4s
2023-01-22 21:58:32.823086: Epoch: 11, Batch: 61, Loss: 0.5094, Elapsed: 0m2s
2023-01-22 21:58:36.595031: Epoch: 11, Batch: 62, Loss: 0.5268, Elapsed: 0m3s
2023-01-22 21:58:39.055592: Epoch: 11, Batch: 63, Loss: 0.5041, Elapsed: 0m2s
2023-01-22 21:58:41.427925: Epoch: 11, Batch: 64, Loss: 0.5463, Elapsed: 0m2s
2023-01-22 21:58:43.284165: Epoch: 11, Batch: 65, Loss: 0.5206, Elapsed: 0m1s
2023-01-22 21:58:45.749347: Epoch: 11, Batch: 66, Loss: 0.5177, Elapsed: 0m2s
2023-01-22 21:58:47.899841: Epoch: 11, Batch: 67, Loss: 0.5473, Elapsed: 0m2s
2023-01-22 21:58:49.179311: Epoch: 11, Batch: 68, Loss: 0.4688, Elapsed: 0m1s
2023-01-22 21:58:51.185621: Epoch: 11, Batch: 69, Loss: 0.5030, Elapsed: 0m1s
2023-01-22 21:58:54.454423: Epoch: 11, Batch: 70, Loss: 0.5383, Elapsed: 0m3s
2023-01-22 21:58:55.951903: Epoch: 11, Batch: 71, Loss: 0.4643, Elapsed: 0m1s
2023-01-22 21:58:57.899786: Epoch: 11, Batch: 72, Loss: 0.5470, Elapsed: 0m1s
2023-01-22 21:59:00.860996: Epoch: 11, Batch: 73, Loss: 0.5312, Elapsed: 0m2s
2023-01-22 21:59:03.025612: Epoch: 11, Batch: 74, Loss: 0.5053, Elapsed: 0m2s
2023-01-22 21:59:05.385599: Epoch: 11, Batch: 75, Loss: 0.5636, Elapsed: 0m2s
2023-01-22 21:59:06.992461: Epoch: 11, Batch: 76, Loss: 0.5065, Elapsed: 0m1s
2023-01-22 21:59:08.772451: Epoch: 11, Batch: 77, Loss: 0.5164, Elapsed: 0m1s
2023-01-22 21:59:11.733751: Epoch: 11, Batch: 78, Loss: 0.5481, Elapsed: 0m2s
2023-01-22 21:59:14.776620: Epoch: 11, Batch: 79, Loss: 0.5327, Elapsed: 0m3s
2023-01-22 21:59:17.647226: Epoch: 11, Batch: 80, Loss: 0.5740, Elapsed: 0m2s
2023-01-22 21:59:19.476232: Epoch: 12, Batch: 1, Loss: 0.5093, Elapsed: 0m1s
2023-01-22 21:59:21.387286: Epoch: 12, Batch: 2, Loss: 0.5023, Elapsed: 0m1s
2023-01-22 21:59:24.177981: Epoch: 12, Batch: 3, Loss: 0.5642, Elapsed: 0m2s
2023-01-22 21:59:26.836487: Epoch: 12, Batch: 4, Loss: 0.5391, Elapsed: 0m2s
2023-01-22 21:59:29.689425: Epoch: 12, Batch: 5, Loss: 0.5705, Elapsed: 0m2s
2023-01-22 21:59:31.286925: Epoch: 12, Batch: 6, Loss: 0.4547, Elapsed: 0m1s
2023-01-22 21:59:33.513926: Epoch: 12, Batch: 7, Loss: 0.5362, Elapsed: 0m2s
2023-01-22 21:59:36.958537: Epoch: 12, Batch: 8, Loss: 0.5463, Elapsed: 0m3s
2023-01-22 21:59:39.652606: Epoch: 12, Batch: 9, Loss: 0.5337, Elapsed: 0m2s
2023-01-22 21:59:41.706286: Epoch: 12, Batch: 10, Loss: 0.4628, Elapsed: 0m2s
2023-01-22 21:59:43.730895: Epoch: 12, Batch: 11, Loss: 0.5059, Elapsed: 0m2s
2023-01-22 21:59:45.415721: Epoch: 12, Batch: 12, Loss: 0.4748, Elapsed: 0m1s
2023-01-22 21:59:48.384111: Epoch: 12, Batch: 13, Loss: 0.5289, Elapsed: 0m2s
2023-01-22 21:59:50.854567: Epoch: 12, Batch: 14, Loss: 0.5274, Elapsed: 0m2s
2023-01-22 21:59:53.582354: Epoch: 12, Batch: 15, Loss: 0.5060, Elapsed: 0m2s
2023-01-22 21:59:58.000045: Epoch: 12, Batch: 16, Loss: 0.5342, Elapsed: 0m4s
2023-01-22 21:59:59.839546: Epoch: 12, Batch: 17, Loss: 0.5186, Elapsed: 0m1s
2023-01-22 22:00:02.824166: Epoch: 12, Batch: 18, Loss: 0.5753, Elapsed: 0m2s
2023-01-22 22:00:04.324762: Epoch: 12, Batch: 19, Loss: 0.5067, Elapsed: 0m1s
2023-01-22 22:00:08.448075: Epoch: 12, Batch: 20, Loss: 0.5699, Elapsed: 0m4s
2023-01-22 22:00:09.560815: Epoch: 12, Batch: 21, Loss: 0.4214, Elapsed: 0m1s
2023-01-22 22:00:12.427280: Epoch: 12, Batch: 22, Loss: 0.5466, Elapsed: 0m2s
2023-01-22 22:00:13.856867: Epoch: 12, Batch: 23, Loss: 0.5429, Elapsed: 0m1s
2023-01-22 22:00:16.914049: Epoch: 12, Batch: 24, Loss: 0.5315, Elapsed: 0m3s
2023-01-22 22:00:18.189162: Epoch: 12, Batch: 25, Loss: 0.4738, Elapsed: 0m1s
2023-01-22 22:00:19.769652: Epoch: 12, Batch: 26, Loss: 0.4938, Elapsed: 0m1s
2023-01-22 22:00:22.438240: Epoch: 12, Batch: 27, Loss: 0.5098, Elapsed: 0m2s
2023-01-22 22:00:27.521956: Epoch: 12, Batch: 28, Loss: 0.5507, Elapsed: 0m5s
2023-01-22 22:00:29.150106: Epoch: 12, Batch: 29, Loss: 0.5122, Elapsed: 0m1s
2023-01-22 22:00:33.897790: Epoch: 12, Batch: 30, Loss: 0.5539, Elapsed: 0m4s
2023-01-22 22:00:36.478603: Epoch: 12, Batch: 31, Loss: 0.4992, Elapsed: 0m2s
2023-01-22 22:00:39.296642: Epoch: 12, Batch: 32, Loss: 0.5084, Elapsed: 0m2s
2023-01-22 22:00:42.637233: Epoch: 12, Batch: 33, Loss: 0.5283, Elapsed: 0m3s
2023-01-22 22:00:43.526388: Epoch: 12, Batch: 34, Loss: 0.4139, Elapsed: 0m0s
2023-01-22 22:00:45.626657: Epoch: 12, Batch: 35, Loss: 0.4785, Elapsed: 0m2s
2023-01-22 22:00:49.510608: Epoch: 12, Batch: 36, Loss: 0.5411, Elapsed: 0m3s
2023-01-22 22:00:51.603523: Epoch: 12, Batch: 37, Loss: 0.5395, Elapsed: 0m2s
2023-01-22 22:00:54.818832: Epoch: 12, Batch: 38, Loss: 0.5625, Elapsed: 0m3s
2023-01-22 22:00:57.288038: Epoch: 12, Batch: 39, Loss: 0.5249, Elapsed: 0m2s
2023-01-22 22:01:00.552029: Epoch: 12, Batch: 40, Loss: 0.5433, Elapsed: 0m3s
2023-01-22 22:01:02.050976: Epoch: 12, Batch: 41, Loss: 0.4776, Elapsed: 0m1s
2023-01-22 22:01:04.432933: Epoch: 12, Batch: 42, Loss: 0.5493, Elapsed: 0m2s
2023-01-22 22:01:06.992456: Epoch: 12, Batch: 43, Loss: 0.5036, Elapsed: 0m2s
2023-01-22 22:01:11.570082: Epoch: 12, Batch: 44, Loss: 0.5325, Elapsed: 0m4s
2023-01-22 22:01:14.233716: Epoch: 12, Batch: 45, Loss: 0.5344, Elapsed: 0m2s
2023-01-22 22:01:19.652715: Epoch: 12, Batch: 46, Loss: 0.5375, Elapsed: 0m5s
2023-01-22 22:01:23.182885: Epoch: 12, Batch: 47, Loss: 0.5520, Elapsed: 0m3s
2023-01-22 22:01:24.738843: Epoch: 12, Batch: 48, Loss: 0.4590, Elapsed: 0m1s
2023-01-22 22:01:26.760558: Epoch: 12, Batch: 49, Loss: 0.5040, Elapsed: 0m2s
2023-01-22 22:01:29.765446: Epoch: 12, Batch: 50, Loss: 0.5595, Elapsed: 0m2s
2023-01-22 22:01:29.775291 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:01:48.082308: validation Test:  Loss: 0.5085,  AUC: 0.8101, Acc: 72.0406,  Precision: 0.8954 -- Elapsed: 0m18s
2023-01-22 22:01:48.082346 Starting testing the train set with 20 subgraphs!
2023-01-22 22:03:24.211703: training Test:  Loss: 0.5326,  AUC: 0.7860, Acc: 71.0911,  Precision: 0.8762 -- Elapsed: 1m36s
2023-01-22 22:03:26.030848: Epoch: 12, Batch: 51, Loss: 0.5294, Elapsed: 0m1s
2023-01-22 22:03:28.333570: Epoch: 12, Batch: 52, Loss: 0.5049, Elapsed: 0m2s
2023-01-22 22:03:30.502461: Epoch: 12, Batch: 53, Loss: 0.5245, Elapsed: 0m2s
2023-01-22 22:03:32.646388: Epoch: 12, Batch: 54, Loss: 0.5472, Elapsed: 0m2s
2023-01-22 22:03:33.689499: Epoch: 12, Batch: 55, Loss: 0.3682, Elapsed: 0m1s
2023-01-22 22:03:36.367927: Epoch: 12, Batch: 56, Loss: 0.5563, Elapsed: 0m2s
2023-01-22 22:03:37.723356: Epoch: 12, Batch: 57, Loss: 0.4883, Elapsed: 0m1s
2023-01-22 22:03:38.601101: Epoch: 12, Batch: 58, Loss: 0.3636, Elapsed: 0m0s
2023-01-22 22:03:40.100935: Epoch: 12, Batch: 59, Loss: 0.4454, Elapsed: 0m1s
2023-01-22 22:03:41.382150: Epoch: 12, Batch: 60, Loss: 0.5051, Elapsed: 0m1s
2023-01-22 22:03:42.705837: Epoch: 12, Batch: 61, Loss: 0.4514, Elapsed: 0m1s
2023-01-22 22:03:43.984184: Epoch: 12, Batch: 62, Loss: 0.4526, Elapsed: 0m1s
2023-01-22 22:03:45.993545: Epoch: 12, Batch: 63, Loss: 0.5306, Elapsed: 0m2s
2023-01-22 22:03:47.395668: Epoch: 12, Batch: 64, Loss: 0.4561, Elapsed: 0m1s
2023-01-22 22:03:50.253098: Epoch: 12, Batch: 65, Loss: 0.5584, Elapsed: 0m2s
2023-01-22 22:03:54.317859: Epoch: 12, Batch: 66, Loss: 0.5864, Elapsed: 0m4s
2023-01-22 22:03:56.851124: Epoch: 12, Batch: 67, Loss: 0.5111, Elapsed: 0m2s
2023-01-22 22:04:00.056065: Epoch: 12, Batch: 68, Loss: 0.5217, Elapsed: 0m3s
2023-01-22 22:04:01.312996: Epoch: 12, Batch: 69, Loss: 0.5087, Elapsed: 0m1s
2023-01-22 22:04:04.140541: Epoch: 12, Batch: 70, Loss: 0.5547, Elapsed: 0m2s
2023-01-22 22:04:07.079156: Epoch: 12, Batch: 71, Loss: 0.5512, Elapsed: 0m2s
2023-01-22 22:04:08.537201: Epoch: 12, Batch: 72, Loss: 0.4677, Elapsed: 0m1s
2023-01-22 22:04:10.844001: Epoch: 12, Batch: 73, Loss: 0.5135, Elapsed: 0m2s
2023-01-22 22:04:13.206946: Epoch: 12, Batch: 74, Loss: 0.5569, Elapsed: 0m2s
2023-01-22 22:04:16.723264: Epoch: 12, Batch: 75, Loss: 0.5294, Elapsed: 0m3s
2023-01-22 22:04:19.202873: Epoch: 12, Batch: 76, Loss: 0.5401, Elapsed: 0m2s
2023-01-22 22:04:22.996586: Epoch: 12, Batch: 77, Loss: 0.5766, Elapsed: 0m3s
2023-01-22 22:04:27.053515: Epoch: 12, Batch: 78, Loss: 0.5493, Elapsed: 0m4s
2023-01-22 22:04:30.914039: Epoch: 12, Batch: 79, Loss: 0.5566, Elapsed: 0m3s
2023-01-22 22:04:33.732106: Epoch: 12, Batch: 80, Loss: 0.5417, Elapsed: 0m2s
2023-01-22 22:04:36.195018: Epoch: 13, Batch: 1, Loss: 0.5468, Elapsed: 0m2s
2023-01-22 22:04:40.786540: Epoch: 13, Batch: 2, Loss: 0.5315, Elapsed: 0m4s
2023-01-22 22:04:42.156885: Epoch: 13, Batch: 3, Loss: 0.5003, Elapsed: 0m1s
2023-01-22 22:04:43.936331: Epoch: 13, Batch: 4, Loss: 0.5209, Elapsed: 0m1s
2023-01-22 22:04:48.695794: Epoch: 13, Batch: 5, Loss: 0.5509, Elapsed: 0m4s
2023-01-22 22:04:50.203306: Epoch: 13, Batch: 6, Loss: 0.4785, Elapsed: 0m1s
2023-01-22 22:04:53.267844: Epoch: 13, Batch: 7, Loss: 0.5321, Elapsed: 0m3s
2023-01-22 22:04:55.264034: Epoch: 13, Batch: 8, Loss: 0.5503, Elapsed: 0m1s
2023-01-22 22:04:56.992319: Epoch: 13, Batch: 9, Loss: 0.5119, Elapsed: 0m1s
2023-01-22 22:05:00.659295: Epoch: 13, Batch: 10, Loss: 0.5049, Elapsed: 0m3s
2023-01-22 22:05:02.184279: Epoch: 13, Batch: 11, Loss: 0.4615, Elapsed: 0m1s
2023-01-22 22:05:04.355321: Epoch: 13, Batch: 12, Loss: 0.5387, Elapsed: 0m2s
2023-01-22 22:05:07.225275: Epoch: 13, Batch: 13, Loss: 0.5769, Elapsed: 0m2s
2023-01-22 22:05:09.239256: Epoch: 13, Batch: 14, Loss: 0.5149, Elapsed: 0m2s
2023-01-22 22:05:12.192131: Epoch: 13, Batch: 15, Loss: 0.5532, Elapsed: 0m2s
2023-01-22 22:05:16.609704: Epoch: 13, Batch: 16, Loss: 0.5394, Elapsed: 0m4s
2023-01-22 22:05:19.073931: Epoch: 13, Batch: 17, Loss: 0.5309, Elapsed: 0m2s
2023-01-22 22:05:20.371975: Epoch: 13, Batch: 18, Loss: 0.4530, Elapsed: 0m1s
2023-01-22 22:05:25.782964: Epoch: 13, Batch: 19, Loss: 0.5384, Elapsed: 0m5s
2023-01-22 22:05:29.865590: Epoch: 13, Batch: 20, Loss: 0.5510, Elapsed: 0m4s
2023-01-22 22:05:31.142918: Epoch: 13, Batch: 21, Loss: 0.4560, Elapsed: 0m1s
2023-01-22 22:05:33.253660: Epoch: 13, Batch: 22, Loss: 0.4979, Elapsed: 0m2s
2023-01-22 22:05:35.319452: Epoch: 13, Batch: 23, Loss: 0.4705, Elapsed: 0m2s
2023-01-22 22:05:37.686193: Epoch: 13, Batch: 24, Loss: 0.5582, Elapsed: 0m2s
2023-01-22 22:05:40.348403: Epoch: 13, Batch: 25, Loss: 0.5345, Elapsed: 0m2s
2023-01-22 22:05:43.033511: Epoch: 13, Batch: 26, Loss: 0.5191, Elapsed: 0m2s
2023-01-22 22:05:44.421646: Epoch: 13, Batch: 27, Loss: 0.4634, Elapsed: 0m1s
2023-01-22 22:05:46.440267: Epoch: 13, Batch: 28, Loss: 0.5241, Elapsed: 0m2s
2023-01-22 22:05:47.322131: Epoch: 13, Batch: 29, Loss: 0.4293, Elapsed: 0m0s
2023-01-22 22:05:48.757610: Epoch: 13, Batch: 30, Loss: 0.5401, Elapsed: 0m1s
2023-01-22 22:05:49.878883: Epoch: 13, Batch: 31, Loss: 0.4277, Elapsed: 0m1s
2023-01-22 22:05:51.319870: Epoch: 13, Batch: 32, Loss: 0.4597, Elapsed: 0m1s
2023-01-22 22:05:54.286104: Epoch: 13, Batch: 33, Loss: 0.5274, Elapsed: 0m2s
2023-01-22 22:05:57.103831: Epoch: 13, Batch: 34, Loss: 0.5534, Elapsed: 0m2s
2023-01-22 22:05:59.819111: Epoch: 13, Batch: 35, Loss: 0.5094, Elapsed: 0m2s
2023-01-22 22:06:02.852900: Epoch: 13, Batch: 36, Loss: 0.5344, Elapsed: 0m3s
2023-01-22 22:06:06.274301: Epoch: 13, Batch: 37, Loss: 0.5637, Elapsed: 0m3s
2023-01-22 22:06:08.008344: Epoch: 13, Batch: 38, Loss: 0.4766, Elapsed: 0m1s
2023-01-22 22:06:09.575734: Epoch: 13, Batch: 39, Loss: 0.4632, Elapsed: 0m1s
2023-01-22 22:06:13.171415: Epoch: 13, Batch: 40, Loss: 0.5853, Elapsed: 0m3s
2023-01-22 22:06:18.262955: Epoch: 13, Batch: 41, Loss: 0.5582, Elapsed: 0m5s
2023-01-22 22:06:20.592248: Epoch: 13, Batch: 42, Loss: 0.4996, Elapsed: 0m2s
2023-01-22 22:06:23.065093: Epoch: 13, Batch: 43, Loss: 0.5146, Elapsed: 0m2s
2023-01-22 22:06:25.423841: Epoch: 13, Batch: 44, Loss: 0.5500, Elapsed: 0m2s
2023-01-22 22:06:26.469585: Epoch: 13, Batch: 45, Loss: 0.3574, Elapsed: 0m1s
2023-01-22 22:06:28.068249: Epoch: 13, Batch: 46, Loss: 0.5098, Elapsed: 0m1s
2023-01-22 22:06:29.349806: Epoch: 13, Batch: 47, Loss: 0.5027, Elapsed: 0m1s
2023-01-22 22:06:32.372948: Epoch: 13, Batch: 48, Loss: 0.5301, Elapsed: 0m3s
2023-01-22 22:06:36.503737: Epoch: 13, Batch: 49, Loss: 0.5662, Elapsed: 0m4s
2023-01-22 22:06:39.065606: Epoch: 13, Batch: 50, Loss: 0.5053, Elapsed: 0m2s
2023-01-22 22:06:39.074770 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:06:56.491108: validation Test:  Loss: 0.5012,  AUC: 0.8130, Acc: 73.5017,  Precision: 0.8278 -- Elapsed: 0m17s
2023-01-22 22:06:56.491146 Starting testing the train set with 20 subgraphs!
2023-01-22 22:08:32.637608: training Test:  Loss: 0.5273,  AUC: 0.7905, Acc: 72.2474,  Precision: 0.8073 -- Elapsed: 1m36s
2023-01-22 22:08:34.978400: Epoch: 13, Batch: 51, Loss: 0.5060, Elapsed: 0m2s
2023-01-22 22:08:38.480767: Epoch: 13, Batch: 52, Loss: 0.5508, Elapsed: 0m3s
2023-01-22 22:08:41.348162: Epoch: 13, Batch: 53, Loss: 0.5469, Elapsed: 0m2s
2023-01-22 22:08:43.798210: Epoch: 13, Batch: 54, Loss: 0.5177, Elapsed: 0m2s
2023-01-22 22:08:45.643608: Epoch: 13, Batch: 55, Loss: 0.5180, Elapsed: 0m1s
2023-01-22 22:08:48.569630: Epoch: 13, Batch: 56, Loss: 0.5264, Elapsed: 0m2s
2023-01-22 22:08:50.569615: Epoch: 13, Batch: 57, Loss: 0.5086, Elapsed: 0m1s
2023-01-22 22:08:51.461753: Epoch: 13, Batch: 58, Loss: 0.3651, Elapsed: 0m0s
2023-01-22 22:08:54.293822: Epoch: 13, Batch: 59, Loss: 0.5395, Elapsed: 0m2s
2023-01-22 22:08:56.737445: Epoch: 13, Batch: 60, Loss: 0.5201, Elapsed: 0m2s
2023-01-22 22:08:59.419211: Epoch: 13, Batch: 61, Loss: 0.5582, Elapsed: 0m2s
2023-01-22 22:09:00.683754: Epoch: 13, Batch: 62, Loss: 0.5158, Elapsed: 0m1s
2023-01-22 22:09:02.599380: Epoch: 13, Batch: 63, Loss: 0.5032, Elapsed: 0m1s
2023-01-22 22:09:05.080359: Epoch: 13, Batch: 64, Loss: 0.5031, Elapsed: 0m2s
2023-01-22 22:09:08.944210: Epoch: 13, Batch: 65, Loss: 0.5411, Elapsed: 0m3s
2023-01-22 22:09:12.198939: Epoch: 13, Batch: 66, Loss: 0.5352, Elapsed: 0m3s
2023-01-22 22:09:14.502877: Epoch: 13, Batch: 67, Loss: 0.5464, Elapsed: 0m2s
2023-01-22 22:09:17.329852: Epoch: 13, Batch: 68, Loss: 0.5085, Elapsed: 0m2s
2023-01-22 22:09:19.091097: Epoch: 13, Batch: 69, Loss: 0.4476, Elapsed: 0m1s
2023-01-22 22:09:22.932382: Epoch: 13, Batch: 70, Loss: 0.5602, Elapsed: 0m3s
2023-01-22 22:09:24.420294: Epoch: 13, Batch: 71, Loss: 0.4448, Elapsed: 0m1s
2023-01-22 22:09:26.595638: Epoch: 13, Batch: 72, Loss: 0.5102, Elapsed: 0m2s
2023-01-22 22:09:29.762693: Epoch: 13, Batch: 73, Loss: 0.5604, Elapsed: 0m3s
2023-01-22 22:09:32.719146: Epoch: 13, Batch: 74, Loss: 0.5710, Elapsed: 0m2s
2023-01-22 22:09:35.601906: Epoch: 13, Batch: 75, Loss: 0.5383, Elapsed: 0m2s
2023-01-22 22:09:37.164023: Epoch: 13, Batch: 76, Loss: 0.4932, Elapsed: 0m1s
2023-01-22 22:09:40.786176: Epoch: 13, Batch: 77, Loss: 0.5281, Elapsed: 0m3s
2023-01-22 22:09:43.583976: Epoch: 13, Batch: 78, Loss: 0.5608, Elapsed: 0m2s
2023-01-22 22:09:47.392054: Epoch: 13, Batch: 79, Loss: 0.5293, Elapsed: 0m3s
2023-01-22 22:09:49.216459: Epoch: 13, Batch: 80, Loss: 0.5144, Elapsed: 0m1s
2023-01-22 22:09:51.885980: Epoch: 14, Batch: 1, Loss: 0.5408, Elapsed: 0m2s
2023-01-22 22:09:54.354360: Epoch: 14, Batch: 2, Loss: 0.5102, Elapsed: 0m2s
2023-01-22 22:09:56.527651: Epoch: 14, Batch: 3, Loss: 0.5176, Elapsed: 0m2s
2023-01-22 22:09:58.133471: Epoch: 14, Batch: 4, Loss: 0.5125, Elapsed: 0m1s
2023-01-22 22:10:01.051844: Epoch: 14, Batch: 5, Loss: 0.5274, Elapsed: 0m2s
2023-01-22 22:10:03.828071: Epoch: 14, Batch: 6, Loss: 0.5610, Elapsed: 0m2s
2023-01-22 22:10:05.318424: Epoch: 14, Batch: 7, Loss: 0.4416, Elapsed: 0m1s
2023-01-22 22:10:06.644371: Epoch: 14, Batch: 8, Loss: 0.4573, Elapsed: 0m1s
2023-01-22 22:10:08.140756: Epoch: 14, Batch: 9, Loss: 0.5116, Elapsed: 0m1s
2023-01-22 22:10:09.176667: Epoch: 14, Batch: 10, Loss: 0.3543, Elapsed: 0m1s
2023-01-22 22:10:12.027009: Epoch: 14, Batch: 11, Loss: 0.5459, Elapsed: 0m2s
2023-01-22 22:10:16.748402: Epoch: 14, Batch: 12, Loss: 0.5580, Elapsed: 0m4s
2023-01-22 22:10:19.201829: Epoch: 14, Batch: 13, Loss: 0.5133, Elapsed: 0m2s
2023-01-22 22:10:22.757435: Epoch: 14, Batch: 14, Loss: 0.5685, Elapsed: 0m3s
2023-01-22 22:10:25.147508: Epoch: 14, Batch: 15, Loss: 0.5537, Elapsed: 0m2s
2023-01-22 22:10:26.929068: Epoch: 14, Batch: 16, Loss: 0.5304, Elapsed: 0m1s
2023-01-22 22:10:29.749889: Epoch: 14, Batch: 17, Loss: 0.5457, Elapsed: 0m2s
2023-01-22 22:10:32.209706: Epoch: 14, Batch: 18, Loss: 0.5225, Elapsed: 0m2s
2023-01-22 22:10:36.333698: Epoch: 14, Batch: 19, Loss: 0.5764, Elapsed: 0m4s
2023-01-22 22:10:38.630830: Epoch: 14, Batch: 20, Loss: 0.5070, Elapsed: 0m2s
2023-01-22 22:10:41.586439: Epoch: 14, Batch: 21, Loss: 0.5775, Elapsed: 0m2s
2023-01-22 22:10:44.851751: Epoch: 14, Batch: 22, Loss: 0.5359, Elapsed: 0m3s
2023-01-22 22:10:47.321664: Epoch: 14, Batch: 23, Loss: 0.5300, Elapsed: 0m2s
2023-01-22 22:10:49.787466: Epoch: 14, Batch: 24, Loss: 0.5373, Elapsed: 0m2s
2023-01-22 22:10:51.242053: Epoch: 14, Batch: 25, Loss: 0.4911, Elapsed: 0m1s
2023-01-22 22:10:55.045390: Epoch: 14, Batch: 26, Loss: 0.5310, Elapsed: 0m3s
2023-01-22 22:10:57.920098: Epoch: 14, Batch: 27, Loss: 0.5760, Elapsed: 0m2s
2023-01-22 22:11:01.755719: Epoch: 14, Batch: 28, Loss: 0.5579, Elapsed: 0m3s
2023-01-22 22:11:04.779917: Epoch: 14, Batch: 29, Loss: 0.5386, Elapsed: 0m3s
2023-01-22 22:11:06.614748: Epoch: 14, Batch: 30, Loss: 0.5382, Elapsed: 0m1s
2023-01-22 22:11:09.302286: Epoch: 14, Batch: 31, Loss: 0.5620, Elapsed: 0m2s
2023-01-22 22:11:11.302523: Epoch: 14, Batch: 32, Loss: 0.5194, Elapsed: 0m1s
2023-01-22 22:11:13.146428: Epoch: 14, Batch: 33, Loss: 0.5196, Elapsed: 0m1s
2023-01-22 22:11:15.248500: Epoch: 14, Batch: 34, Loss: 0.5394, Elapsed: 0m2s
2023-01-22 22:11:16.531910: Epoch: 14, Batch: 35, Loss: 0.5117, Elapsed: 0m1s
2023-01-22 22:11:18.635965: Epoch: 14, Batch: 36, Loss: 0.4830, Elapsed: 0m2s
2023-01-22 22:11:21.453044: Epoch: 14, Batch: 37, Loss: 0.5508, Elapsed: 0m2s
2023-01-22 22:11:27.061639: Epoch: 14, Batch: 38, Loss: 0.5468, Elapsed: 0m5s
2023-01-22 22:11:29.652027: Epoch: 14, Batch: 39, Loss: 0.5685, Elapsed: 0m2s
2023-01-22 22:11:32.203491: Epoch: 14, Batch: 40, Loss: 0.5219, Elapsed: 0m2s
2023-01-22 22:11:33.085398: Epoch: 14, Batch: 41, Loss: 0.3714, Elapsed: 0m0s
2023-01-22 22:11:34.488044: Epoch: 14, Batch: 42, Loss: 0.4466, Elapsed: 0m1s
2023-01-22 22:11:36.622806: Epoch: 14, Batch: 43, Loss: 0.5479, Elapsed: 0m2s
2023-01-22 22:11:39.830141: Epoch: 14, Batch: 44, Loss: 0.5139, Elapsed: 0m3s
2023-01-22 22:11:42.569914: Epoch: 14, Batch: 45, Loss: 0.5168, Elapsed: 0m2s
2023-01-22 22:11:47.984490: Epoch: 14, Batch: 46, Loss: 0.5486, Elapsed: 0m5s
2023-01-22 22:11:52.496978: Epoch: 14, Batch: 47, Loss: 0.5404, Elapsed: 0m4s
2023-01-22 22:11:54.147195: Epoch: 14, Batch: 48, Loss: 0.4754, Elapsed: 0m1s
2023-01-22 22:11:57.360874: Epoch: 14, Batch: 49, Loss: 0.5574, Elapsed: 0m3s
2023-01-22 22:11:59.034964: Epoch: 14, Batch: 50, Loss: 0.4853, Elapsed: 0m1s
2023-01-22 22:11:59.047859 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:12:16.470554: validation Test:  Loss: 0.5046,  AUC: 0.8099, Acc: 72.2305,  Precision: 0.8911 -- Elapsed: 0m17s
2023-01-22 22:12:16.470663 Starting testing the train set with 20 subgraphs!
2023-01-22 22:13:52.803003: training Test:  Loss: 0.5297,  AUC: 0.7875, Acc: 71.3585,  Precision: 0.8759 -- Elapsed: 1m36s
2023-01-22 22:13:53.729609: Epoch: 14, Batch: 51, Loss: 0.4261, Elapsed: 0m0s
2023-01-22 22:13:55.010711: Epoch: 14, Batch: 52, Loss: 0.5126, Elapsed: 0m1s
2023-01-22 22:13:56.295427: Epoch: 14, Batch: 53, Loss: 0.4657, Elapsed: 0m1s
2023-01-22 22:14:00.880560: Epoch: 14, Batch: 54, Loss: 0.5353, Elapsed: 0m4s
2023-01-22 22:14:02.809811: Epoch: 14, Batch: 55, Loss: 0.5080, Elapsed: 0m1s
2023-01-22 22:14:05.628626: Epoch: 14, Batch: 56, Loss: 0.5368, Elapsed: 0m2s
2023-01-22 22:14:09.642773: Epoch: 14, Batch: 57, Loss: 0.5509, Elapsed: 0m4s
2023-01-22 22:14:10.935469: Epoch: 14, Batch: 58, Loss: 0.4580, Elapsed: 0m1s
2023-01-22 22:14:13.493588: Epoch: 14, Batch: 59, Loss: 0.5096, Elapsed: 0m2s
2023-01-22 22:14:15.456471: Epoch: 14, Batch: 60, Loss: 0.5514, Elapsed: 0m1s
2023-01-22 22:14:16.835316: Epoch: 14, Batch: 61, Loss: 0.4989, Elapsed: 0m1s
2023-01-22 22:14:18.277675: Epoch: 14, Batch: 62, Loss: 0.5529, Elapsed: 0m1s
2023-01-22 22:14:20.299708: Epoch: 14, Batch: 63, Loss: 0.5190, Elapsed: 0m2s
2023-01-22 22:14:23.793121: Epoch: 14, Batch: 64, Loss: 0.5533, Elapsed: 0m3s
2023-01-22 22:14:27.322670: Epoch: 14, Batch: 65, Loss: 0.5356, Elapsed: 0m3s
2023-01-22 22:14:28.818526: Epoch: 14, Batch: 66, Loss: 0.4926, Elapsed: 0m1s
2023-01-22 22:14:31.295740: Epoch: 14, Batch: 67, Loss: 0.5262, Elapsed: 0m2s
2023-01-22 22:14:34.352940: Epoch: 14, Batch: 68, Loss: 0.5313, Elapsed: 0m3s
2023-01-22 22:14:37.388181: Epoch: 14, Batch: 69, Loss: 0.5471, Elapsed: 0m3s
2023-01-22 22:14:39.207734: Epoch: 14, Batch: 70, Loss: 0.4576, Elapsed: 0m1s
2023-01-22 22:14:42.042270: Epoch: 14, Batch: 71, Loss: 0.5071, Elapsed: 0m2s
2023-01-22 22:14:44.148297: Epoch: 14, Batch: 72, Loss: 0.5106, Elapsed: 0m2s
2023-01-22 22:14:48.031497: Epoch: 14, Batch: 73, Loss: 0.5356, Elapsed: 0m3s
2023-01-22 22:14:50.700541: Epoch: 14, Batch: 74, Loss: 0.5121, Elapsed: 0m2s
2023-01-22 22:14:52.142342: Epoch: 14, Batch: 75, Loss: 0.4643, Elapsed: 0m1s
2023-01-22 22:14:55.106299: Epoch: 14, Batch: 76, Loss: 0.5272, Elapsed: 0m2s
2023-01-22 22:14:57.744143: Epoch: 14, Batch: 77, Loss: 0.5288, Elapsed: 0m2s
2023-01-22 22:14:58.855615: Epoch: 14, Batch: 78, Loss: 0.4288, Elapsed: 0m1s
2023-01-22 22:15:02.438183: Epoch: 14, Batch: 79, Loss: 0.5790, Elapsed: 0m3s
2023-01-22 22:15:04.497363: Epoch: 14, Batch: 80, Loss: 0.4625, Elapsed: 0m2s
2023-01-22 22:15:05.542067: Epoch: 15, Batch: 1, Loss: 0.3529, Elapsed: 0m1s
2023-01-22 22:15:06.658321: Epoch: 15, Batch: 2, Loss: 0.4176, Elapsed: 0m1s
2023-01-22 22:15:08.336867: Epoch: 15, Batch: 3, Loss: 0.4804, Elapsed: 0m1s
2023-01-22 22:15:09.901733: Epoch: 15, Batch: 4, Loss: 0.4690, Elapsed: 0m1s
2023-01-22 22:15:12.270928: Epoch: 15, Batch: 5, Loss: 0.5529, Elapsed: 0m2s
2023-01-22 22:15:14.294129: Epoch: 15, Batch: 6, Loss: 0.5164, Elapsed: 0m2s
2023-01-22 22:15:15.688672: Epoch: 15, Batch: 7, Loss: 0.4462, Elapsed: 0m1s
2023-01-22 22:15:18.704032: Epoch: 15, Batch: 8, Loss: 0.5402, Elapsed: 0m3s
2023-01-22 22:15:21.173425: Epoch: 15, Batch: 9, Loss: 0.5345, Elapsed: 0m2s
2023-01-22 22:15:22.940236: Epoch: 15, Batch: 10, Loss: 0.5098, Elapsed: 0m1s
2023-01-22 22:15:27.076180: Epoch: 15, Batch: 11, Loss: 0.5758, Elapsed: 0m4s
2023-01-22 22:15:30.938751: Epoch: 15, Batch: 12, Loss: 0.5444, Elapsed: 0m3s
2023-01-22 22:15:33.900446: Epoch: 15, Batch: 13, Loss: 0.5556, Elapsed: 0m2s
2023-01-22 22:15:35.909546: Epoch: 15, Batch: 14, Loss: 0.5136, Elapsed: 0m1s
2023-01-22 22:15:38.715411: Epoch: 15, Batch: 15, Loss: 0.5580, Elapsed: 0m2s
2023-01-22 22:15:40.837325: Epoch: 15, Batch: 16, Loss: 0.5433, Elapsed: 0m2s
2023-01-22 22:15:44.120629: Epoch: 15, Batch: 17, Loss: 0.5580, Elapsed: 0m3s
2023-01-22 22:15:47.115446: Epoch: 15, Batch: 18, Loss: 0.5602, Elapsed: 0m2s
2023-01-22 22:15:49.678131: Epoch: 15, Batch: 19, Loss: 0.5126, Elapsed: 0m2s
2023-01-22 22:15:52.153636: Epoch: 15, Batch: 20, Loss: 0.5173, Elapsed: 0m2s
2023-01-22 22:15:53.425273: Epoch: 15, Batch: 21, Loss: 0.5263, Elapsed: 0m1s
2023-01-22 22:15:54.709400: Epoch: 15, Batch: 22, Loss: 0.5051, Elapsed: 0m1s
2023-01-22 22:15:58.498303: Epoch: 15, Batch: 23, Loss: 0.5760, Elapsed: 0m3s
2023-01-22 22:16:00.451723: Epoch: 15, Batch: 24, Loss: 0.5414, Elapsed: 0m1s
2023-01-22 22:16:02.038167: Epoch: 15, Batch: 25, Loss: 0.4716, Elapsed: 0m1s
2023-01-22 22:16:02.919225: Epoch: 15, Batch: 26, Loss: 0.3756, Elapsed: 0m0s
2023-01-22 22:16:07.521951: Epoch: 15, Batch: 27, Loss: 0.5328, Elapsed: 0m4s
2023-01-22 22:16:09.616292: Epoch: 15, Batch: 28, Loss: 0.4894, Elapsed: 0m2s
2023-01-22 22:16:13.492649: Epoch: 15, Batch: 29, Loss: 0.5848, Elapsed: 0m3s
2023-01-22 22:16:15.671661: Epoch: 15, Batch: 30, Loss: 0.5093, Elapsed: 0m2s
2023-01-22 22:16:20.102778: Epoch: 15, Batch: 31, Loss: 0.5322, Elapsed: 0m4s
2023-01-22 22:16:22.569919: Epoch: 15, Batch: 32, Loss: 0.5437, Elapsed: 0m2s
2023-01-22 22:16:23.440387: Epoch: 15, Batch: 33, Loss: 0.4321, Elapsed: 0m0s
2023-01-22 22:16:26.090532: Epoch: 15, Batch: 34, Loss: 0.5307, Elapsed: 0m2s
2023-01-22 22:16:28.572231: Epoch: 15, Batch: 35, Loss: 0.5052, Elapsed: 0m2s
2023-01-22 22:16:31.555253: Epoch: 15, Batch: 36, Loss: 0.5335, Elapsed: 0m2s
2023-01-22 22:16:33.154075: Epoch: 15, Batch: 37, Loss: 0.5061, Elapsed: 0m1s
2023-01-22 22:16:38.569536: Epoch: 15, Batch: 38, Loss: 0.5397, Elapsed: 0m5s
2023-01-22 22:16:40.613623: Epoch: 15, Batch: 39, Loss: 0.5153, Elapsed: 0m2s
2023-01-22 22:16:42.439911: Epoch: 15, Batch: 40, Loss: 0.5034, Elapsed: 0m1s
2023-01-22 22:16:45.263578: Epoch: 15, Batch: 41, Loss: 0.5449, Elapsed: 0m2s
2023-01-22 22:16:49.089625: Epoch: 15, Batch: 42, Loss: 0.5898, Elapsed: 0m3s
2023-01-22 22:16:50.751513: Epoch: 15, Batch: 43, Loss: 0.5471, Elapsed: 0m1s
2023-01-22 22:16:53.155570: Epoch: 15, Batch: 44, Loss: 0.5408, Elapsed: 0m2s
2023-01-22 22:16:56.331431: Epoch: 15, Batch: 45, Loss: 0.5742, Elapsed: 0m3s
2023-01-22 22:16:58.638446: Epoch: 15, Batch: 46, Loss: 0.5155, Elapsed: 0m2s
2023-01-22 22:17:01.444592: Epoch: 15, Batch: 47, Loss: 0.5565, Elapsed: 0m2s
2023-01-22 22:17:02.892561: Epoch: 15, Batch: 48, Loss: 0.4764, Elapsed: 0m1s
2023-01-22 22:17:04.407434: Epoch: 15, Batch: 49, Loss: 0.4808, Elapsed: 0m1s
2023-01-22 22:17:05.894031: Epoch: 15, Batch: 50, Loss: 0.4497, Elapsed: 0m1s
2023-01-22 22:17:05.907902 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:17:23.446395: validation Test:  Loss: 0.5036,  AUC: 0.8111, Acc: 73.0972,  Precision: 0.8365 -- Elapsed: 0m17s
2023-01-22 22:17:23.446458 Starting testing the train set with 20 subgraphs!
2023-01-22 22:18:59.916870: training Test:  Loss: 0.5285,  AUC: 0.7893, Acc: 71.9741,  Precision: 0.8164 -- Elapsed: 1m36s
2023-01-22 22:19:02.334963: Epoch: 15, Batch: 51, Loss: 0.4601, Elapsed: 0m2s
2023-01-22 22:19:03.667372: Epoch: 15, Batch: 52, Loss: 0.4655, Elapsed: 0m1s
2023-01-22 22:19:06.154747: Epoch: 15, Batch: 53, Loss: 0.5057, Elapsed: 0m2s
2023-01-22 22:19:09.415184: Epoch: 15, Batch: 54, Loss: 0.5436, Elapsed: 0m3s
2023-01-22 22:19:10.773654: Epoch: 15, Batch: 55, Loss: 0.4970, Elapsed: 0m1s
2023-01-22 22:19:13.449648: Epoch: 15, Batch: 56, Loss: 0.5398, Elapsed: 0m2s
2023-01-22 22:19:15.912205: Epoch: 15, Batch: 57, Loss: 0.5158, Elapsed: 0m2s
2023-01-22 22:19:18.355210: Epoch: 15, Batch: 58, Loss: 0.5243, Elapsed: 0m2s
2023-01-22 22:19:21.547792: Epoch: 15, Batch: 59, Loss: 0.5103, Elapsed: 0m3s
2023-01-22 22:19:24.208444: Epoch: 15, Batch: 60, Loss: 0.5124, Elapsed: 0m2s
2023-01-22 22:19:25.491958: Epoch: 15, Batch: 61, Loss: 0.4673, Elapsed: 0m1s
2023-01-22 22:19:30.217902: Epoch: 15, Batch: 62, Loss: 0.5557, Elapsed: 0m4s
2023-01-22 22:19:33.273419: Epoch: 15, Batch: 63, Loss: 0.5377, Elapsed: 0m3s
2023-01-22 22:19:36.781317: Epoch: 15, Batch: 64, Loss: 0.5333, Elapsed: 0m3s
2023-01-22 22:19:38.696525: Epoch: 15, Batch: 65, Loss: 0.5062, Elapsed: 0m1s
2023-01-22 22:19:41.426600: Epoch: 15, Batch: 66, Loss: 0.5095, Elapsed: 0m2s
2023-01-22 22:19:46.514678: Epoch: 15, Batch: 67, Loss: 0.5488, Elapsed: 0m5s
2023-01-22 22:19:50.043847: Epoch: 15, Batch: 68, Loss: 0.5532, Elapsed: 0m3s
2023-01-22 22:19:51.540317: Epoch: 15, Batch: 69, Loss: 0.5135, Elapsed: 0m1s
2023-01-22 22:19:52.865633: Epoch: 15, Batch: 70, Loss: 0.4615, Elapsed: 0m1s
2023-01-22 22:19:54.322600: Epoch: 15, Batch: 71, Loss: 0.4984, Elapsed: 0m1s
2023-01-22 22:19:57.150835: Epoch: 15, Batch: 72, Loss: 0.5408, Elapsed: 0m2s
2023-01-22 22:20:00.063255: Epoch: 15, Batch: 73, Loss: 0.5226, Elapsed: 0m2s
2023-01-22 22:20:02.465972: Epoch: 15, Batch: 74, Loss: 0.5043, Elapsed: 0m2s
2023-01-22 22:20:04.319054: Epoch: 15, Batch: 75, Loss: 0.5208, Elapsed: 0m1s
2023-01-22 22:20:07.194709: Epoch: 15, Batch: 76, Loss: 0.5718, Elapsed: 0m2s
2023-01-22 22:20:10.049240: Epoch: 15, Batch: 77, Loss: 0.5474, Elapsed: 0m2s
2023-01-22 22:20:13.210493: Epoch: 15, Batch: 78, Loss: 0.5550, Elapsed: 0m3s
2023-01-22 22:20:17.233683: Epoch: 15, Batch: 79, Loss: 0.5513, Elapsed: 0m4s
2023-01-22 22:20:19.586352: Epoch: 15, Batch: 80, Loss: 0.5617, Elapsed: 0m2s
2023-01-22 22:20:23.065743: Epoch: 16, Batch: 1, Loss: 0.5502, Elapsed: 0m3s
2023-01-22 22:20:28.185018: Epoch: 16, Batch: 2, Loss: 0.5487, Elapsed: 0m5s
2023-01-22 22:20:31.097120: Epoch: 16, Batch: 3, Loss: 0.5631, Elapsed: 0m2s
2023-01-22 22:20:32.588675: Epoch: 16, Batch: 4, Loss: 0.4411, Elapsed: 0m1s
2023-01-22 22:20:34.951557: Epoch: 16, Batch: 5, Loss: 0.5495, Elapsed: 0m2s
2023-01-22 22:20:37.423334: Epoch: 16, Batch: 6, Loss: 0.5273, Elapsed: 0m2s
2023-01-22 22:20:39.015151: Epoch: 16, Batch: 7, Loss: 0.4555, Elapsed: 0m1s
2023-01-22 22:20:41.804845: Epoch: 16, Batch: 8, Loss: 0.5379, Elapsed: 0m2s
2023-01-22 22:20:44.321636: Epoch: 16, Batch: 9, Loss: 0.5079, Elapsed: 0m2s
2023-01-22 22:20:45.447319: Epoch: 16, Batch: 10, Loss: 0.4208, Elapsed: 0m1s
2023-01-22 22:20:47.459854: Epoch: 16, Batch: 11, Loss: 0.5208, Elapsed: 0m2s
2023-01-22 22:20:48.927186: Epoch: 16, Batch: 12, Loss: 0.4946, Elapsed: 0m1s
2023-01-22 22:20:50.210641: Epoch: 16, Batch: 13, Loss: 0.4606, Elapsed: 0m1s
2023-01-22 22:20:53.801711: Epoch: 16, Batch: 14, Loss: 0.5820, Elapsed: 0m3s
2023-01-22 22:20:57.864311: Epoch: 16, Batch: 15, Loss: 0.5507, Elapsed: 0m4s
2023-01-22 22:21:00.221534: Epoch: 16, Batch: 16, Loss: 0.5586, Elapsed: 0m2s
2023-01-22 22:21:02.243580: Epoch: 16, Batch: 17, Loss: 0.5058, Elapsed: 0m2s
2023-01-22 22:21:04.711272: Epoch: 16, Batch: 18, Loss: 0.5162, Elapsed: 0m2s
2023-01-22 22:21:07.844992: Epoch: 16, Batch: 19, Loss: 0.5739, Elapsed: 0m3s
2023-01-22 22:21:11.059918: Epoch: 16, Batch: 20, Loss: 0.5393, Elapsed: 0m3s
2023-01-22 22:21:13.555253: Epoch: 16, Batch: 21, Loss: 0.5051, Elapsed: 0m2s
2023-01-22 22:21:16.666946: Epoch: 16, Batch: 22, Loss: 0.5609, Elapsed: 0m3s
2023-01-22 22:21:19.570785: Epoch: 16, Batch: 23, Loss: 0.5235, Elapsed: 0m2s
2023-01-22 22:21:22.227111: Epoch: 16, Batch: 24, Loss: 0.5130, Elapsed: 0m2s
2023-01-22 22:21:24.314478: Epoch: 16, Batch: 25, Loss: 0.5347, Elapsed: 0m2s
2023-01-22 22:21:26.157374: Epoch: 16, Batch: 26, Loss: 0.5040, Elapsed: 0m1s
2023-01-22 22:21:30.739530: Epoch: 16, Batch: 27, Loss: 0.5290, Elapsed: 0m4s
2023-01-22 22:21:33.550260: Epoch: 16, Batch: 28, Loss: 0.5395, Elapsed: 0m2s
2023-01-22 22:21:35.052398: Epoch: 16, Batch: 29, Loss: 0.4649, Elapsed: 0m1s
2023-01-22 22:21:38.855046: Epoch: 16, Batch: 30, Loss: 0.5618, Elapsed: 0m3s
2023-01-22 22:21:39.723042: Epoch: 16, Batch: 31, Loss: 0.3595, Elapsed: 0m0s
2023-01-22 22:21:41.507489: Epoch: 16, Batch: 32, Loss: 0.5172, Elapsed: 0m1s
2023-01-22 22:21:42.782443: Epoch: 16, Batch: 33, Loss: 0.4838, Elapsed: 0m1s
2023-01-22 22:21:45.268910: Epoch: 16, Batch: 34, Loss: 0.5079, Elapsed: 0m2s
2023-01-22 22:21:46.563571: Epoch: 16, Batch: 35, Loss: 0.5303, Elapsed: 0m1s
2023-01-22 22:21:49.120275: Epoch: 16, Batch: 36, Loss: 0.4995, Elapsed: 0m2s
2023-01-22 22:21:51.209513: Epoch: 16, Batch: 37, Loss: 0.4798, Elapsed: 0m2s
2023-01-22 22:21:54.157374: Epoch: 16, Batch: 38, Loss: 0.5588, Elapsed: 0m2s
2023-01-22 22:21:56.619651: Epoch: 16, Batch: 39, Loss: 0.5407, Elapsed: 0m2s
2023-01-22 22:21:59.501509: Epoch: 16, Batch: 40, Loss: 0.5566, Elapsed: 0m2s
2023-01-22 22:22:03.332413: Epoch: 16, Batch: 41, Loss: 0.5733, Elapsed: 0m3s
2023-01-22 22:22:06.348893: Epoch: 16, Batch: 42, Loss: 0.5540, Elapsed: 0m3s
2023-01-22 22:22:09.151298: Epoch: 16, Batch: 43, Loss: 0.5558, Elapsed: 0m2s
2023-01-22 22:22:12.590501: Epoch: 16, Batch: 44, Loss: 0.5113, Elapsed: 0m3s
2023-01-22 22:22:17.573202: Epoch: 16, Batch: 45, Loss: 0.5333, Elapsed: 0m4s
2023-01-22 22:22:19.171056: Epoch: 16, Batch: 46, Loss: 0.5426, Elapsed: 0m1s
2023-01-22 22:22:22.438326: Epoch: 16, Batch: 47, Loss: 0.5452, Elapsed: 0m3s
2023-01-22 22:22:25.111978: Epoch: 16, Batch: 48, Loss: 0.5719, Elapsed: 0m2s
2023-01-22 22:22:26.607977: Epoch: 16, Batch: 49, Loss: 0.5057, Elapsed: 0m1s
2023-01-22 22:22:28.554408: Epoch: 16, Batch: 50, Loss: 0.5484, Elapsed: 0m1s
2023-01-22 22:22:28.566893 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:22:45.955756: validation Test:  Loss: 0.5009,  AUC: 0.8126, Acc: 73.1468,  Precision: 0.8453 -- Elapsed: 0m17s
2023-01-22 22:22:45.955797 Starting testing the train set with 20 subgraphs!
2023-01-22 22:24:22.529620: training Test:  Loss: 0.5244,  AUC: 0.7920, Acc: 72.0475,  Precision: 0.8256 -- Elapsed: 1m36s
2023-01-22 22:24:23.635513: Epoch: 16, Batch: 51, Loss: 0.4251, Elapsed: 0m1s
2023-01-22 22:24:25.314822: Epoch: 16, Batch: 52, Loss: 0.4643, Elapsed: 0m1s
2023-01-22 22:24:26.830843: Epoch: 16, Batch: 53, Loss: 0.4671, Elapsed: 0m1s
2023-01-22 22:24:28.102429: Epoch: 16, Batch: 54, Loss: 0.5106, Elapsed: 0m1s
2023-01-22 22:24:30.026332: Epoch: 16, Batch: 55, Loss: 0.5059, Elapsed: 0m1s
2023-01-22 22:24:32.893897: Epoch: 16, Batch: 56, Loss: 0.5714, Elapsed: 0m2s
2023-01-22 22:24:35.609716: Epoch: 16, Batch: 57, Loss: 0.5128, Elapsed: 0m2s
2023-01-22 22:24:39.472324: Epoch: 16, Batch: 58, Loss: 0.5401, Elapsed: 0m3s
2023-01-22 22:24:43.608110: Epoch: 16, Batch: 59, Loss: 0.5664, Elapsed: 0m4s
2023-01-22 22:24:45.441411: Epoch: 16, Batch: 60, Loss: 0.5247, Elapsed: 0m1s
2023-01-22 22:24:47.922177: Epoch: 16, Batch: 61, Loss: 0.5442, Elapsed: 0m2s
2023-01-22 22:24:49.321450: Epoch: 16, Batch: 62, Loss: 0.4418, Elapsed: 0m1s
2023-01-22 22:24:52.511583: Epoch: 16, Batch: 63, Loss: 0.5558, Elapsed: 0m3s
2023-01-22 22:24:54.811693: Epoch: 16, Batch: 64, Loss: 0.5127, Elapsed: 0m2s
2023-01-22 22:24:57.106933: Epoch: 16, Batch: 65, Loss: 0.5024, Elapsed: 0m2s
2023-01-22 22:24:58.679949: Epoch: 16, Batch: 66, Loss: 0.4579, Elapsed: 0m1s
2023-01-22 22:25:03.400584: Epoch: 16, Batch: 67, Loss: 0.5503, Elapsed: 0m4s
2023-01-22 22:25:06.051424: Epoch: 16, Batch: 68, Loss: 0.5267, Elapsed: 0m2s
2023-01-22 22:25:08.527219: Epoch: 16, Batch: 69, Loss: 0.5022, Elapsed: 0m2s
2023-01-22 22:25:13.975974: Epoch: 16, Batch: 70, Loss: 0.5359, Elapsed: 0m5s
2023-01-22 22:25:17.028306: Epoch: 16, Batch: 71, Loss: 0.5248, Elapsed: 0m3s
2023-01-22 22:25:18.396950: Epoch: 16, Batch: 72, Loss: 0.4920, Elapsed: 0m1s
2023-01-22 22:25:20.470549: Epoch: 16, Batch: 73, Loss: 0.4673, Elapsed: 0m2s
2023-01-22 22:25:21.504012: Epoch: 16, Batch: 74, Loss: 0.3495, Elapsed: 0m1s
2023-01-22 22:25:23.530294: Epoch: 16, Batch: 75, Loss: 0.5118, Elapsed: 0m2s
2023-01-22 22:25:25.297680: Epoch: 16, Batch: 76, Loss: 0.5058, Elapsed: 0m1s
2023-01-22 22:25:27.754291: Epoch: 16, Batch: 77, Loss: 0.5459, Elapsed: 0m2s
2023-01-22 22:25:31.101701: Epoch: 16, Batch: 78, Loss: 0.5266, Elapsed: 0m3s
2023-01-22 22:25:32.774284: Epoch: 16, Batch: 79, Loss: 0.4756, Elapsed: 0m1s
2023-01-22 22:25:36.297349: Epoch: 16, Batch: 80, Loss: 0.5256, Elapsed: 0m3s
2023-01-22 22:25:40.149839: Epoch: 17, Batch: 1, Loss: 0.5594, Elapsed: 0m3s
2023-01-22 22:25:41.656069: Epoch: 17, Batch: 2, Loss: 0.4457, Elapsed: 0m1s
2023-01-22 22:25:44.477928: Epoch: 17, Batch: 3, Loss: 0.5500, Elapsed: 0m2s
2023-01-22 22:25:47.204182: Epoch: 17, Batch: 4, Loss: 0.5098, Elapsed: 0m2s
2023-01-22 22:25:49.110400: Epoch: 17, Batch: 5, Loss: 0.5018, Elapsed: 0m1s
2023-01-22 22:25:50.692226: Epoch: 17, Batch: 6, Loss: 0.4625, Elapsed: 0m1s
2023-01-22 22:25:53.342068: Epoch: 17, Batch: 7, Loss: 0.5094, Elapsed: 0m2s
2023-01-22 22:25:55.819330: Epoch: 17, Batch: 8, Loss: 0.4992, Elapsed: 0m2s
2023-01-22 22:25:57.921602: Epoch: 17, Batch: 9, Loss: 0.4808, Elapsed: 0m2s
2023-01-22 22:26:00.477560: Epoch: 17, Batch: 10, Loss: 0.5004, Elapsed: 0m2s
2023-01-22 22:26:02.965252: Epoch: 17, Batch: 11, Loss: 0.5016, Elapsed: 0m2s
2023-01-22 22:26:04.904206: Epoch: 17, Batch: 12, Loss: 0.5374, Elapsed: 0m1s
2023-01-22 22:26:06.178897: Epoch: 17, Batch: 13, Loss: 0.5093, Elapsed: 0m1s
2023-01-22 22:26:07.462101: Epoch: 17, Batch: 14, Loss: 0.4558, Elapsed: 0m1s
2023-01-22 22:26:08.737930: Epoch: 17, Batch: 15, Loss: 0.4992, Elapsed: 0m1s
2023-01-22 22:26:11.995971: Epoch: 17, Batch: 16, Loss: 0.5354, Elapsed: 0m3s
2023-01-22 22:26:13.313070: Epoch: 17, Batch: 17, Loss: 0.4609, Elapsed: 0m1s
2023-01-22 22:26:14.717046: Epoch: 17, Batch: 18, Loss: 0.4412, Elapsed: 0m1s
2023-01-22 22:26:17.007214: Epoch: 17, Batch: 19, Loss: 0.5036, Elapsed: 0m2s
2023-01-22 22:26:18.046420: Epoch: 17, Batch: 20, Loss: 0.3381, Elapsed: 0m1s
2023-01-22 22:26:20.403097: Epoch: 17, Batch: 21, Loss: 0.5531, Elapsed: 0m2s
2023-01-22 22:26:21.782041: Epoch: 17, Batch: 22, Loss: 0.4922, Elapsed: 0m1s
2023-01-22 22:26:24.255382: Epoch: 17, Batch: 23, Loss: 0.5160, Elapsed: 0m2s
2023-01-22 22:26:28.453978: Epoch: 17, Batch: 24, Loss: 0.5714, Elapsed: 0m4s
2023-01-22 22:26:31.898261: Epoch: 17, Batch: 25, Loss: 0.5172, Elapsed: 0m3s
2023-01-22 22:26:37.001685: Epoch: 17, Batch: 26, Loss: 0.5576, Elapsed: 0m5s
2023-01-22 22:26:39.643778: Epoch: 17, Batch: 27, Loss: 0.5278, Elapsed: 0m2s
2023-01-22 22:26:41.779941: Epoch: 17, Batch: 28, Loss: 0.5422, Elapsed: 0m2s
2023-01-22 22:26:44.683632: Epoch: 17, Batch: 29, Loss: 0.5259, Elapsed: 0m2s
2023-01-22 22:26:47.541697: Epoch: 17, Batch: 30, Loss: 0.5690, Elapsed: 0m2s
2023-01-22 22:26:50.706677: Epoch: 17, Batch: 31, Loss: 0.5498, Elapsed: 0m3s
2023-01-22 22:26:53.718418: Epoch: 17, Batch: 32, Loss: 0.5343, Elapsed: 0m2s
2023-01-22 22:26:54.995442: Epoch: 17, Batch: 33, Loss: 0.4775, Elapsed: 0m1s
2023-01-22 22:26:57.790532: Epoch: 17, Batch: 34, Loss: 0.5380, Elapsed: 0m2s
2023-01-22 22:27:00.775524: Epoch: 17, Batch: 35, Loss: 0.5676, Elapsed: 0m2s
2023-01-22 22:27:03.091508: Epoch: 17, Batch: 36, Loss: 0.5096, Elapsed: 0m2s
2023-01-22 22:27:07.684195: Epoch: 17, Batch: 37, Loss: 0.5305, Elapsed: 0m4s
2023-01-22 22:27:09.181432: Epoch: 17, Batch: 38, Loss: 0.5091, Elapsed: 0m1s
2023-01-22 22:27:10.866648: Epoch: 17, Batch: 39, Loss: 0.4736, Elapsed: 0m1s
2023-01-22 22:27:12.471782: Epoch: 17, Batch: 40, Loss: 0.5090, Elapsed: 0m1s
2023-01-22 22:27:15.335034: Epoch: 17, Batch: 41, Loss: 0.5477, Elapsed: 0m2s
2023-01-22 22:27:16.847076: Epoch: 17, Batch: 42, Loss: 0.4582, Elapsed: 0m1s
2023-01-22 22:27:20.631894: Epoch: 17, Batch: 43, Loss: 0.5409, Elapsed: 0m3s
2023-01-22 22:27:24.623253: Epoch: 17, Batch: 44, Loss: 0.5358, Elapsed: 0m3s
2023-01-22 22:27:26.802713: Epoch: 17, Batch: 45, Loss: 0.4550, Elapsed: 0m2s
2023-01-22 22:27:28.822467: Epoch: 17, Batch: 46, Loss: 0.5152, Elapsed: 0m2s
2023-01-22 22:27:31.786093: Epoch: 17, Batch: 47, Loss: 0.5398, Elapsed: 0m2s
2023-01-22 22:27:33.969462: Epoch: 17, Batch: 48, Loss: 0.5176, Elapsed: 0m2s
2023-01-22 22:27:38.543383: Epoch: 17, Batch: 49, Loss: 0.5490, Elapsed: 0m4s
2023-01-22 22:27:44.004901: Epoch: 17, Batch: 50, Loss: 0.5351, Elapsed: 0m5s
2023-01-22 22:27:44.018414 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:28:01.452009: validation Test:  Loss: 0.4992,  AUC: 0.8130, Acc: 72.9074,  Precision: 0.8547 -- Elapsed: 0m17s
2023-01-22 22:28:01.452116 Starting testing the train set with 20 subgraphs!
2023-01-22 22:29:37.485635: training Test:  Loss: 0.5239,  AUC: 0.7914, Acc: 71.9271,  Precision: 0.8355 -- Elapsed: 1m36s
2023-01-22 22:29:41.028730: Epoch: 17, Batch: 51, Loss: 0.5470, Elapsed: 0m3s
2023-01-22 22:29:44.100807: Epoch: 17, Batch: 52, Loss: 0.5284, Elapsed: 0m3s
2023-01-22 22:29:49.447861: Epoch: 17, Batch: 53, Loss: 0.5478, Elapsed: 0m5s
2023-01-22 22:29:51.082349: Epoch: 17, Batch: 54, Loss: 0.5339, Elapsed: 0m1s
2023-01-22 22:29:54.677990: Epoch: 17, Batch: 55, Loss: 0.5814, Elapsed: 0m3s
2023-01-22 22:29:57.147177: Epoch: 17, Batch: 56, Loss: 0.5311, Elapsed: 0m2s
2023-01-22 22:30:00.660188: Epoch: 17, Batch: 57, Loss: 0.5223, Elapsed: 0m3s
2023-01-22 22:30:03.048392: Epoch: 17, Batch: 58, Loss: 0.5494, Elapsed: 0m2s
2023-01-22 22:30:07.473360: Epoch: 17, Batch: 59, Loss: 0.5235, Elapsed: 0m4s
2023-01-22 22:30:09.566604: Epoch: 17, Batch: 60, Loss: 0.5299, Elapsed: 0m2s
2023-01-22 22:30:12.565353: Epoch: 17, Batch: 61, Loss: 0.5259, Elapsed: 0m2s
2023-01-22 22:30:15.027612: Epoch: 17, Batch: 62, Loss: 0.5262, Elapsed: 0m2s
2023-01-22 22:30:17.499836: Epoch: 17, Batch: 63, Loss: 0.5326, Elapsed: 0m2s
2023-01-22 22:30:20.228439: Epoch: 17, Batch: 64, Loss: 0.5348, Elapsed: 0m2s
2023-01-22 22:30:21.410590: Epoch: 17, Batch: 65, Loss: 0.4254, Elapsed: 0m1s
2023-01-22 22:30:24.257888: Epoch: 17, Batch: 66, Loss: 0.5585, Elapsed: 0m2s
2023-01-22 22:30:26.256306: Epoch: 17, Batch: 67, Loss: 0.5009, Elapsed: 0m1s
2023-01-22 22:30:28.094172: Epoch: 17, Batch: 68, Loss: 0.5098, Elapsed: 0m1s
2023-01-22 22:30:31.118611: Epoch: 17, Batch: 69, Loss: 0.5587, Elapsed: 0m3s
2023-01-22 22:30:33.130255: Epoch: 17, Batch: 70, Loss: 0.5123, Elapsed: 0m1s
2023-01-22 22:30:36.171818: Epoch: 17, Batch: 71, Loss: 0.5506, Elapsed: 0m3s
2023-01-22 22:30:37.843902: Epoch: 17, Batch: 72, Loss: 0.4548, Elapsed: 0m1s
2023-01-22 22:30:39.850925: Epoch: 17, Batch: 73, Loss: 0.5224, Elapsed: 0m1s
2023-01-22 22:30:42.317670: Epoch: 17, Batch: 74, Loss: 0.5088, Elapsed: 0m2s
2023-01-22 22:30:45.008072: Epoch: 17, Batch: 75, Loss: 0.5588, Elapsed: 0m2s
2023-01-22 22:30:46.454092: Epoch: 17, Batch: 76, Loss: 0.4598, Elapsed: 0m1s
2023-01-22 22:30:47.332321: Epoch: 17, Batch: 77, Loss: 0.3499, Elapsed: 0m0s
2023-01-22 22:30:49.119233: Epoch: 17, Batch: 78, Loss: 0.5080, Elapsed: 0m1s
2023-01-22 22:30:50.127635: Epoch: 17, Batch: 79, Loss: 0.4202, Elapsed: 0m0s
2023-01-22 22:30:51.811300: Epoch: 17, Batch: 80, Loss: 0.4984, Elapsed: 0m1s
2023-01-22 22:30:54.259966: Epoch: 18, Batch: 1, Loss: 0.5445, Elapsed: 0m2s
2023-01-22 22:30:57.131988: Epoch: 18, Batch: 2, Loss: 0.5339, Elapsed: 0m2s
2023-01-22 22:30:58.623113: Epoch: 18, Batch: 3, Loss: 0.4586, Elapsed: 0m1s
2023-01-22 22:30:59.914608: Epoch: 18, Batch: 4, Loss: 0.4447, Elapsed: 0m1s
2023-01-22 22:31:01.420008: Epoch: 18, Batch: 5, Loss: 0.5028, Elapsed: 0m1s
2023-01-22 22:31:04.204510: Epoch: 18, Batch: 6, Loss: 0.5599, Elapsed: 0m2s
2023-01-22 22:31:06.760484: Epoch: 18, Batch: 7, Loss: 0.5008, Elapsed: 0m2s
2023-01-22 22:31:08.319561: Epoch: 18, Batch: 8, Loss: 0.4570, Elapsed: 0m1s
2023-01-22 22:31:11.842627: Epoch: 18, Batch: 9, Loss: 0.5292, Elapsed: 0m3s
2023-01-22 22:31:14.310828: Epoch: 18, Batch: 10, Loss: 0.5211, Elapsed: 0m2s
2023-01-22 22:31:16.150020: Epoch: 18, Batch: 11, Loss: 0.5273, Elapsed: 0m1s
2023-01-22 22:31:18.466208: Epoch: 18, Batch: 12, Loss: 0.5036, Elapsed: 0m2s
2023-01-22 22:31:21.372132: Epoch: 18, Batch: 13, Loss: 0.5230, Elapsed: 0m2s
2023-01-22 22:31:23.201306: Epoch: 18, Batch: 14, Loss: 0.5024, Elapsed: 0m1s
2023-01-22 22:31:25.229577: Epoch: 18, Batch: 15, Loss: 0.5060, Elapsed: 0m2s
2023-01-22 22:31:29.075153: Epoch: 18, Batch: 16, Loss: 0.5665, Elapsed: 0m3s
2023-01-22 22:31:31.085609: Epoch: 18, Batch: 17, Loss: 0.5043, Elapsed: 0m2s
2023-01-22 22:31:33.010645: Epoch: 18, Batch: 18, Loss: 0.5027, Elapsed: 0m1s
2023-01-22 22:31:35.483209: Epoch: 18, Batch: 19, Loss: 0.5330, Elapsed: 0m2s
2023-01-22 22:31:38.151023: Epoch: 18, Batch: 20, Loss: 0.5321, Elapsed: 0m2s
2023-01-22 22:31:40.772311: Epoch: 18, Batch: 21, Loss: 0.5278, Elapsed: 0m2s
2023-01-22 22:31:42.872994: Epoch: 18, Batch: 22, Loss: 0.5336, Elapsed: 0m2s
2023-01-22 22:31:44.380856: Epoch: 18, Batch: 23, Loss: 0.4410, Elapsed: 0m1s
2023-01-22 22:31:46.844980: Epoch: 18, Batch: 24, Loss: 0.5022, Elapsed: 0m2s
2023-01-22 22:31:51.419118: Epoch: 18, Batch: 25, Loss: 0.5277, Elapsed: 0m4s
2023-01-22 22:31:52.321587: Epoch: 18, Batch: 26, Loss: 0.3538, Elapsed: 0m0s
2023-01-22 22:31:54.815133: Epoch: 18, Batch: 27, Loss: 0.5571, Elapsed: 0m2s
2023-01-22 22:31:56.350935: Epoch: 18, Batch: 28, Loss: 0.4526, Elapsed: 0m1s
2023-01-22 22:31:59.125722: Epoch: 18, Batch: 29, Loss: 0.5172, Elapsed: 0m2s
2023-01-22 22:32:01.992487: Epoch: 18, Batch: 30, Loss: 0.5443, Elapsed: 0m2s
2023-01-22 22:32:04.474336: Epoch: 18, Batch: 31, Loss: 0.5158, Elapsed: 0m2s
2023-01-22 22:32:08.347562: Epoch: 18, Batch: 32, Loss: 0.5440, Elapsed: 0m3s
2023-01-22 22:32:11.336729: Epoch: 18, Batch: 33, Loss: 0.5596, Elapsed: 0m2s
2023-01-22 22:32:14.552995: Epoch: 18, Batch: 34, Loss: 0.5042, Elapsed: 0m3s
2023-01-22 22:32:15.832046: Epoch: 18, Batch: 35, Loss: 0.5044, Elapsed: 0m1s
2023-01-22 22:32:16.710125: Epoch: 18, Batch: 36, Loss: 0.4134, Elapsed: 0m0s
2023-01-22 22:32:18.384379: Epoch: 18, Batch: 37, Loss: 0.4884, Elapsed: 0m1s
2023-01-22 22:32:21.960167: Epoch: 18, Batch: 38, Loss: 0.5843, Elapsed: 0m3s
2023-01-22 22:32:24.783129: Epoch: 18, Batch: 39, Loss: 0.5470, Elapsed: 0m2s
2023-01-22 22:32:29.510229: Epoch: 18, Batch: 40, Loss: 0.5490, Elapsed: 0m4s
2023-01-22 22:32:32.568407: Epoch: 18, Batch: 41, Loss: 0.5238, Elapsed: 0m3s
2023-01-22 22:32:34.012364: Epoch: 18, Batch: 42, Loss: 0.5399, Elapsed: 0m1s
2023-01-22 22:32:37.272301: Epoch: 18, Batch: 43, Loss: 0.5328, Elapsed: 0m3s
2023-01-22 22:32:38.548312: Epoch: 18, Batch: 44, Loss: 0.5078, Elapsed: 0m1s
2023-01-22 22:32:41.282926: Epoch: 18, Batch: 45, Loss: 0.5098, Elapsed: 0m2s
2023-01-22 22:32:44.108548: Epoch: 18, Batch: 46, Loss: 0.5362, Elapsed: 0m2s
2023-01-22 22:32:45.506687: Epoch: 18, Batch: 47, Loss: 0.4424, Elapsed: 0m1s
2023-01-22 22:32:47.107297: Epoch: 18, Batch: 48, Loss: 0.5102, Elapsed: 0m1s
2023-01-22 22:32:49.466199: Epoch: 18, Batch: 49, Loss: 0.5602, Elapsed: 0m2s
2023-01-22 22:32:53.595259: Epoch: 18, Batch: 50, Loss: 0.5610, Elapsed: 0m4s
2023-01-22 22:32:53.605944 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:33:11.522089: validation Test:  Loss: 0.4995,  AUC: 0.8131, Acc: 73.1715,  Precision: 0.8367 -- Elapsed: 0m17s
2023-01-22 22:33:11.522193 Starting testing the train set with 20 subgraphs!
2023-01-22 22:34:47.624134: training Test:  Loss: 0.5232,  AUC: 0.7929, Acc: 72.2400,  Precision: 0.8185 -- Elapsed: 1m36s
2023-01-22 22:34:51.723977: Epoch: 18, Batch: 51, Loss: 0.5459, Elapsed: 0m4s
2023-01-22 22:34:52.747662: Epoch: 18, Batch: 52, Loss: 0.3429, Elapsed: 0m1s
2023-01-22 22:34:54.205292: Epoch: 18, Batch: 53, Loss: 0.4893, Elapsed: 0m1s
2023-01-22 22:34:59.646450: Epoch: 18, Batch: 54, Loss: 0.5355, Elapsed: 0m5s
2023-01-22 22:35:00.925592: Epoch: 18, Batch: 55, Loss: 0.4640, Elapsed: 0m1s
2023-01-22 22:35:02.995599: Epoch: 18, Batch: 56, Loss: 0.4601, Elapsed: 0m2s
2023-01-22 22:35:04.117308: Epoch: 18, Batch: 57, Loss: 0.4253, Elapsed: 0m1s
2023-01-22 22:35:06.302753: Epoch: 18, Batch: 58, Loss: 0.5055, Elapsed: 0m2s
2023-01-22 22:35:08.852062: Epoch: 18, Batch: 59, Loss: 0.4948, Elapsed: 0m2s
2023-01-22 22:35:13.874642: Epoch: 18, Batch: 60, Loss: 0.5314, Elapsed: 0m5s
2023-01-22 22:35:16.670871: Epoch: 18, Batch: 61, Loss: 0.5619, Elapsed: 0m2s
2023-01-22 22:35:18.109136: Epoch: 18, Batch: 62, Loss: 0.4567, Elapsed: 0m1s
2023-01-22 22:35:20.783824: Epoch: 18, Batch: 63, Loss: 0.5098, Elapsed: 0m2s
2023-01-22 22:35:23.254328: Epoch: 18, Batch: 64, Loss: 0.5331, Elapsed: 0m2s
2023-01-22 22:35:25.273501: Epoch: 18, Batch: 65, Loss: 0.5141, Elapsed: 0m2s
2023-01-22 22:35:28.295964: Epoch: 18, Batch: 66, Loss: 0.5290, Elapsed: 0m3s
2023-01-22 22:35:30.373939: Epoch: 18, Batch: 67, Loss: 0.4782, Elapsed: 0m2s
2023-01-22 22:35:33.345048: Epoch: 18, Batch: 68, Loss: 0.5711, Elapsed: 0m2s
2023-01-22 22:35:36.209313: Epoch: 18, Batch: 69, Loss: 0.5670, Elapsed: 0m2s
2023-01-22 22:35:37.995626: Epoch: 18, Batch: 70, Loss: 0.5175, Elapsed: 0m1s
2023-01-22 22:35:40.478749: Epoch: 18, Batch: 71, Loss: 0.5084, Elapsed: 0m2s
2023-01-22 22:35:41.839286: Epoch: 18, Batch: 72, Loss: 0.4926, Elapsed: 0m1s
2023-01-22 22:35:43.787590: Epoch: 18, Batch: 73, Loss: 0.5380, Elapsed: 0m1s
2023-01-22 22:35:46.977159: Epoch: 18, Batch: 74, Loss: 0.5532, Elapsed: 0m3s
2023-01-22 22:35:50.470425: Epoch: 18, Batch: 75, Loss: 0.5461, Elapsed: 0m3s
2023-01-22 22:35:54.236920: Epoch: 18, Batch: 76, Loss: 0.5385, Elapsed: 0m3s
2023-01-22 22:35:59.377394: Epoch: 18, Batch: 77, Loss: 0.5500, Elapsed: 0m5s
2023-01-22 22:36:00.955378: Epoch: 18, Batch: 78, Loss: 0.4558, Elapsed: 0m1s
2023-01-22 22:36:03.910134: Epoch: 18, Batch: 79, Loss: 0.5445, Elapsed: 0m2s
2023-01-22 22:36:06.882624: Epoch: 18, Batch: 80, Loss: 0.5249, Elapsed: 0m2s
2023-01-22 22:36:10.744893: Epoch: 19, Batch: 1, Loss: 0.5301, Elapsed: 0m3s
2023-01-22 22:36:13.073548: Epoch: 19, Batch: 2, Loss: 0.4745, Elapsed: 0m2s
2023-01-22 22:36:16.092149: Epoch: 19, Batch: 3, Loss: 0.5198, Elapsed: 0m3s
2023-01-22 22:36:17.769450: Epoch: 19, Batch: 4, Loss: 0.4892, Elapsed: 0m1s
2023-01-22 22:36:19.342927: Epoch: 19, Batch: 5, Loss: 0.4648, Elapsed: 0m1s
2023-01-22 22:36:21.808503: Epoch: 19, Batch: 6, Loss: 0.5118, Elapsed: 0m2s
2023-01-22 22:36:23.175290: Epoch: 19, Batch: 7, Loss: 0.4944, Elapsed: 0m1s
2023-01-22 22:36:25.546570: Epoch: 19, Batch: 8, Loss: 0.5518, Elapsed: 0m2s
2023-01-22 22:36:30.118954: Epoch: 19, Batch: 9, Loss: 0.5255, Elapsed: 0m4s
2023-01-22 22:36:32.435296: Epoch: 19, Batch: 10, Loss: 0.5067, Elapsed: 0m2s
2023-01-22 22:36:34.226902: Epoch: 19, Batch: 11, Loss: 0.5139, Elapsed: 0m1s
2023-01-22 22:36:38.002894: Epoch: 19, Batch: 12, Loss: 0.5291, Elapsed: 0m3s
2023-01-22 22:36:39.856677: Epoch: 19, Batch: 13, Loss: 0.5216, Elapsed: 0m1s
2023-01-22 22:36:42.348907: Epoch: 19, Batch: 14, Loss: 0.5053, Elapsed: 0m2s
2023-01-22 22:36:43.785924: Epoch: 19, Batch: 15, Loss: 0.5538, Elapsed: 0m1s
2023-01-22 22:36:46.262360: Epoch: 19, Batch: 16, Loss: 0.5337, Elapsed: 0m2s
2023-01-22 22:36:48.926747: Epoch: 19, Batch: 17, Loss: 0.5180, Elapsed: 0m2s
2023-01-22 22:36:50.993922: Epoch: 19, Batch: 18, Loss: 0.4613, Elapsed: 0m2s
2023-01-22 22:36:53.300260: Epoch: 19, Batch: 19, Loss: 0.5061, Elapsed: 0m2s
2023-01-22 22:36:54.801733: Epoch: 19, Batch: 20, Loss: 0.4847, Elapsed: 0m1s
2023-01-22 22:36:55.925235: Epoch: 19, Batch: 21, Loss: 0.4272, Elapsed: 0m1s
2023-01-22 22:36:58.958651: Epoch: 19, Batch: 22, Loss: 0.5491, Elapsed: 0m3s
2023-01-22 22:37:00.237971: Epoch: 19, Batch: 23, Loss: 0.4789, Elapsed: 0m1s
2023-01-22 22:37:03.220584: Epoch: 19, Batch: 24, Loss: 0.5293, Elapsed: 0m2s
2023-01-22 22:37:06.137693: Epoch: 19, Batch: 25, Loss: 0.5290, Elapsed: 0m2s
2023-01-22 22:37:07.958449: Epoch: 19, Batch: 26, Loss: 0.5255, Elapsed: 0m1s
2023-01-22 22:37:11.454534: Epoch: 19, Batch: 27, Loss: 0.5645, Elapsed: 0m3s
2023-01-22 22:37:12.730994: Epoch: 19, Batch: 28, Loss: 0.5176, Elapsed: 0m1s
2023-01-22 22:37:14.743898: Epoch: 19, Batch: 29, Loss: 0.5053, Elapsed: 0m2s
2023-01-22 22:37:17.910971: Epoch: 19, Batch: 30, Loss: 0.5357, Elapsed: 0m3s
2023-01-22 22:37:19.832685: Epoch: 19, Batch: 31, Loss: 0.5056, Elapsed: 0m1s
2023-01-22 22:37:24.247271: Epoch: 19, Batch: 32, Loss: 0.5629, Elapsed: 0m4s
2023-01-22 22:37:28.968940: Epoch: 19, Batch: 33, Loss: 0.5629, Elapsed: 0m4s
2023-01-22 22:37:30.282224: Epoch: 19, Batch: 34, Loss: 0.5172, Elapsed: 0m1s
2023-01-22 22:37:31.167167: Epoch: 19, Batch: 35, Loss: 0.4062, Elapsed: 0m0s
2023-01-22 22:37:35.588719: Epoch: 19, Batch: 36, Loss: 0.5428, Elapsed: 0m4s
2023-01-22 22:37:38.070874: Epoch: 19, Batch: 37, Loss: 0.5432, Elapsed: 0m2s
2023-01-22 22:37:41.202527: Epoch: 19, Batch: 38, Loss: 0.5761, Elapsed: 0m3s
2023-01-22 22:37:43.569579: Epoch: 19, Batch: 39, Loss: 0.5764, Elapsed: 0m2s
2023-01-22 22:37:47.408921: Epoch: 19, Batch: 40, Loss: 0.5616, Elapsed: 0m3s
2023-01-22 22:37:48.971232: Epoch: 19, Batch: 41, Loss: 0.4986, Elapsed: 0m1s
2023-01-22 22:37:52.149289: Epoch: 19, Batch: 42, Loss: 0.5637, Elapsed: 0m3s
2023-01-22 22:37:55.422669: Epoch: 19, Batch: 43, Loss: 0.5406, Elapsed: 0m3s
2023-01-22 22:37:56.922264: Epoch: 19, Batch: 44, Loss: 0.5086, Elapsed: 0m1s
2023-01-22 22:38:00.470063: Epoch: 19, Batch: 45, Loss: 0.5273, Elapsed: 0m3s
2023-01-22 22:38:03.038777: Epoch: 19, Batch: 46, Loss: 0.5145, Elapsed: 0m2s
2023-01-22 22:38:05.910334: Epoch: 19, Batch: 47, Loss: 0.5791, Elapsed: 0m2s
2023-01-22 22:38:08.650281: Epoch: 19, Batch: 48, Loss: 0.5318, Elapsed: 0m2s
2023-01-22 22:38:10.662831: Epoch: 19, Batch: 49, Loss: 0.5250, Elapsed: 0m2s
2023-01-22 22:38:13.489929: Epoch: 19, Batch: 50, Loss: 0.5614, Elapsed: 0m2s
2023-01-22 22:38:13.502459 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:38:31.718131: validation Test:  Loss: 0.5031,  AUC: 0.8124, Acc: 72.6762,  Precision: 0.8713 -- Elapsed: 0m18s
2023-01-22 22:38:31.718237 Starting testing the train set with 20 subgraphs!
2023-01-22 22:40:07.546696: training Test:  Loss: 0.5272,  AUC: 0.7893, Acc: 71.7904,  Precision: 0.8438 -- Elapsed: 1m35s
2023-01-22 22:40:13.019461: Epoch: 19, Batch: 51, Loss: 0.5353, Elapsed: 0m5s
2023-01-22 22:40:14.991267: Epoch: 19, Batch: 52, Loss: 0.5296, Elapsed: 0m1s
2023-01-22 22:40:16.487185: Epoch: 19, Batch: 53, Loss: 0.4618, Elapsed: 0m1s
2023-01-22 22:40:17.820715: Epoch: 19, Batch: 54, Loss: 0.4693, Elapsed: 0m1s
2023-01-22 22:40:20.607433: Epoch: 19, Batch: 55, Loss: 0.5563, Elapsed: 0m2s
2023-01-22 22:40:22.749235: Epoch: 19, Batch: 56, Loss: 0.5520, Elapsed: 0m2s
2023-01-22 22:40:23.785074: Epoch: 19, Batch: 57, Loss: 0.3464, Elapsed: 0m1s
2023-01-22 22:40:25.892827: Epoch: 19, Batch: 58, Loss: 0.5223, Elapsed: 0m2s
2023-01-22 22:40:27.174142: Epoch: 19, Batch: 59, Loss: 0.5062, Elapsed: 0m1s
2023-01-22 22:40:30.046047: Epoch: 19, Batch: 60, Loss: 0.5404, Elapsed: 0m2s
2023-01-22 22:40:32.972293: Epoch: 19, Batch: 61, Loss: 0.5122, Elapsed: 0m2s
2023-01-22 22:40:35.838971: Epoch: 19, Batch: 62, Loss: 0.5309, Elapsed: 0m2s
2023-01-22 22:40:38.531226: Epoch: 19, Batch: 63, Loss: 0.5723, Elapsed: 0m2s
2023-01-22 22:40:40.439455: Epoch: 19, Batch: 64, Loss: 0.5160, Elapsed: 0m1s
2023-01-22 22:40:44.571627: Epoch: 19, Batch: 65, Loss: 0.5657, Elapsed: 0m4s
2023-01-22 22:40:47.389529: Epoch: 19, Batch: 66, Loss: 0.5311, Elapsed: 0m2s
2023-01-22 22:40:50.354118: Epoch: 19, Batch: 67, Loss: 0.5692, Elapsed: 0m2s
2023-01-22 22:40:53.386321: Epoch: 19, Batch: 68, Loss: 0.5243, Elapsed: 0m3s
2023-01-22 22:40:58.515616: Epoch: 19, Batch: 69, Loss: 0.5491, Elapsed: 0m5s
2023-01-22 22:40:59.397462: Epoch: 19, Batch: 70, Loss: 0.4163, Elapsed: 0m0s
2023-01-22 22:41:01.891926: Epoch: 19, Batch: 71, Loss: 0.5040, Elapsed: 0m2s
2023-01-22 22:41:03.917973: Epoch: 19, Batch: 72, Loss: 0.5224, Elapsed: 0m2s
2023-01-22 22:41:07.502831: Epoch: 19, Batch: 73, Loss: 0.5860, Elapsed: 0m3s
2023-01-22 22:41:10.465800: Epoch: 19, Batch: 74, Loss: 0.5446, Elapsed: 0m2s
2023-01-22 22:41:12.951211: Epoch: 19, Batch: 75, Loss: 0.5318, Elapsed: 0m2s
2023-01-22 22:41:14.525935: Epoch: 19, Batch: 76, Loss: 0.4565, Elapsed: 0m1s
2023-01-22 22:41:17.724357: Epoch: 19, Batch: 77, Loss: 0.4986, Elapsed: 0m3s
2023-01-22 22:41:19.890989: Epoch: 19, Batch: 78, Loss: 0.5207, Elapsed: 0m2s
2023-01-22 22:41:21.505368: Epoch: 19, Batch: 79, Loss: 0.5178, Elapsed: 0m1s
2023-01-22 22:41:22.905727: Epoch: 19, Batch: 80, Loss: 0.4530, Elapsed: 0m1s
2023-01-22 22:41:25.940540: Epoch: 20, Batch: 1, Loss: 0.5292, Elapsed: 0m3s
2023-01-22 22:41:28.395538: Epoch: 20, Batch: 2, Loss: 0.5279, Elapsed: 0m2s
2023-01-22 22:41:32.822859: Epoch: 20, Batch: 3, Loss: 0.5252, Elapsed: 0m4s
2023-01-22 22:41:34.402421: Epoch: 20, Batch: 4, Loss: 0.4693, Elapsed: 0m1s
2023-01-22 22:41:37.685258: Epoch: 20, Batch: 5, Loss: 0.5753, Elapsed: 0m3s
2023-01-22 22:41:40.530007: Epoch: 20, Batch: 6, Loss: 0.5061, Elapsed: 0m2s
2023-01-22 22:41:42.008422: Epoch: 20, Batch: 7, Loss: 0.4597, Elapsed: 0m1s
2023-01-22 22:41:44.788950: Epoch: 20, Batch: 8, Loss: 0.5564, Elapsed: 0m2s
2023-01-22 22:41:46.885662: Epoch: 20, Batch: 9, Loss: 0.4762, Elapsed: 0m2s
2023-01-22 22:41:48.400898: Epoch: 20, Batch: 10, Loss: 0.5031, Elapsed: 0m1s
2023-01-22 22:41:50.460986: Epoch: 20, Batch: 11, Loss: 0.4639, Elapsed: 0m2s
2023-01-22 22:41:51.752083: Epoch: 20, Batch: 12, Loss: 0.4982, Elapsed: 0m1s
2023-01-22 22:41:55.561408: Epoch: 20, Batch: 13, Loss: 0.5838, Elapsed: 0m3s
2023-01-22 22:41:58.747139: Epoch: 20, Batch: 14, Loss: 0.5531, Elapsed: 0m3s
2023-01-22 22:42:01.223935: Epoch: 20, Batch: 15, Loss: 0.5047, Elapsed: 0m2s
2023-01-22 22:42:04.244420: Epoch: 20, Batch: 16, Loss: 0.5520, Elapsed: 0m3s
2023-01-22 22:42:08.381921: Epoch: 20, Batch: 17, Loss: 0.5611, Elapsed: 0m4s
2023-01-22 22:42:09.654201: Epoch: 20, Batch: 18, Loss: 0.5191, Elapsed: 0m1s
2023-01-22 22:42:10.942420: Epoch: 20, Batch: 19, Loss: 0.4571, Elapsed: 0m1s
2023-01-22 22:42:13.420882: Epoch: 20, Batch: 20, Loss: 0.5217, Elapsed: 0m2s
2023-01-22 22:42:14.866110: Epoch: 20, Batch: 21, Loss: 0.5328, Elapsed: 0m1s
2023-01-22 22:42:17.361845: Epoch: 20, Batch: 22, Loss: 0.5063, Elapsed: 0m2s
2023-01-22 22:42:19.848287: Epoch: 20, Batch: 23, Loss: 0.5315, Elapsed: 0m2s
2023-01-22 22:42:23.902454: Epoch: 20, Batch: 24, Loss: 0.5429, Elapsed: 0m4s
2023-01-22 22:42:29.007794: Epoch: 20, Batch: 25, Loss: 0.5408, Elapsed: 0m5s
2023-01-22 22:42:30.479227: Epoch: 20, Batch: 26, Loss: 0.4961, Elapsed: 0m1s
2023-01-22 22:42:31.363910: Epoch: 20, Batch: 27, Loss: 0.4237, Elapsed: 0m0s
2023-01-22 22:42:36.112855: Epoch: 20, Batch: 28, Loss: 0.5442, Elapsed: 0m4s
2023-01-22 22:42:38.494188: Epoch: 20, Batch: 29, Loss: 0.5650, Elapsed: 0m2s
2023-01-22 22:42:41.199303: Epoch: 20, Batch: 30, Loss: 0.5330, Elapsed: 0m2s
2023-01-22 22:42:44.423802: Epoch: 20, Batch: 31, Loss: 0.5299, Elapsed: 0m3s
2023-01-22 22:42:47.547198: Epoch: 20, Batch: 32, Loss: 0.5334, Elapsed: 0m3s
2023-01-22 22:42:50.418350: Epoch: 20, Batch: 33, Loss: 0.5704, Elapsed: 0m2s
2023-01-22 22:42:52.245301: Epoch: 20, Batch: 34, Loss: 0.5240, Elapsed: 0m1s
2023-01-22 22:42:54.935197: Epoch: 20, Batch: 35, Loss: 0.5293, Elapsed: 0m2s
2023-01-22 22:42:57.577859: Epoch: 20, Batch: 36, Loss: 0.5211, Elapsed: 0m2s
2023-01-22 22:43:01.453513: Epoch: 20, Batch: 37, Loss: 0.5307, Elapsed: 0m3s
2023-01-22 22:43:02.777854: Epoch: 20, Batch: 38, Loss: 0.4522, Elapsed: 0m1s
2023-01-22 22:43:03.808040: Epoch: 20, Batch: 39, Loss: 0.3528, Elapsed: 0m1s
2023-01-22 22:43:08.405367: Epoch: 20, Batch: 40, Loss: 0.5226, Elapsed: 0m4s
2023-01-22 22:43:11.623264: Epoch: 20, Batch: 41, Loss: 0.4950, Elapsed: 0m3s
2023-01-22 22:43:12.745796: Epoch: 20, Batch: 42, Loss: 0.4207, Elapsed: 0m1s
2023-01-22 22:43:14.424315: Epoch: 20, Batch: 43, Loss: 0.4732, Elapsed: 0m1s
2023-01-22 22:43:17.677322: Epoch: 20, Batch: 44, Loss: 0.5370, Elapsed: 0m3s
2023-01-22 22:43:19.634160: Epoch: 20, Batch: 45, Loss: 0.5375, Elapsed: 0m1s
2023-01-22 22:43:22.553839: Epoch: 20, Batch: 46, Loss: 0.5252, Elapsed: 0m2s
2023-01-22 22:43:25.539964: Epoch: 20, Batch: 47, Loss: 0.5219, Elapsed: 0m2s
2023-01-22 22:43:28.343537: Epoch: 20, Batch: 48, Loss: 0.5468, Elapsed: 0m2s
2023-01-22 22:43:30.373276: Epoch: 20, Batch: 49, Loss: 0.5082, Elapsed: 0m2s
2023-01-22 22:43:32.509678: Epoch: 20, Batch: 50, Loss: 0.5382, Elapsed: 0m2s
2023-01-22 22:43:32.522829 Starting testing the valid set with 20 subgraphs!
2023-01-22 22:43:50.830527: validation Test:  Loss: 0.4908,  AUC: 0.8191, Acc: 73.6503,  Precision: 0.8572 -- Elapsed: 0m18s
2023-01-22 22:43:50.830566 Starting testing the train set with 20 subgraphs!
2023-01-22 22:45:26.855811: training Test:  Loss: 0.5183,  AUC: 0.7973, Acc: 72.5045,  Precision: 0.8381 -- Elapsed: 1m36s
2023-01-22 22:45:30.739028: Epoch: 20, Batch: 51, Loss: 0.5541, Elapsed: 0m3s
2023-01-22 22:45:33.621822: Epoch: 20, Batch: 52, Loss: 0.5359, Elapsed: 0m2s
2023-01-22 22:45:35.403115: Epoch: 20, Batch: 53, Loss: 0.5028, Elapsed: 0m1s
2023-01-22 22:45:38.461164: Epoch: 20, Batch: 54, Loss: 0.5198, Elapsed: 0m3s
2023-01-22 22:45:40.766578: Epoch: 20, Batch: 55, Loss: 0.5163, Elapsed: 0m2s
2023-01-22 22:45:42.352574: Epoch: 20, Batch: 56, Loss: 0.4648, Elapsed: 0m1s
2023-01-22 22:45:44.370922: Epoch: 20, Batch: 57, Loss: 0.5088, Elapsed: 0m2s
2023-01-22 22:45:45.816144: Epoch: 20, Batch: 58, Loss: 0.4500, Elapsed: 0m1s
2023-01-22 22:45:48.557831: Epoch: 20, Batch: 59, Loss: 0.5161, Elapsed: 0m2s
2023-01-22 22:45:50.054575: Epoch: 20, Batch: 60, Loss: 0.4422, Elapsed: 0m1s
2023-01-22 22:45:53.655966: Epoch: 20, Batch: 61, Loss: 0.5731, Elapsed: 0m3s
2023-01-22 22:45:57.588939: Epoch: 20, Batch: 62, Loss: 0.5175, Elapsed: 0m3s
2023-01-22 22:45:59.234122: Epoch: 20, Batch: 63, Loss: 0.4471, Elapsed: 0m1s
2023-01-22 22:46:01.342311: Epoch: 20, Batch: 64, Loss: 0.5137, Elapsed: 0m2s
2023-01-22 22:46:03.916901: Epoch: 20, Batch: 65, Loss: 0.5013, Elapsed: 0m2s
2023-01-22 22:46:05.540554: Epoch: 20, Batch: 66, Loss: 0.5062, Elapsed: 0m1s
2023-01-22 22:46:07.567752: Epoch: 20, Batch: 67, Loss: 0.5056, Elapsed: 0m2s
2023-01-22 22:46:12.995090: Epoch: 20, Batch: 68, Loss: 0.5333, Elapsed: 0m5s
2023-01-22 22:46:15.414758: Epoch: 20, Batch: 69, Loss: 0.5383, Elapsed: 0m2s
2023-01-22 22:46:16.885587: Epoch: 20, Batch: 70, Loss: 0.4789, Elapsed: 0m1s
2023-01-22 22:46:19.930214: Epoch: 20, Batch: 71, Loss: 0.5316, Elapsed: 0m3s
2023-01-22 22:46:22.599528: Epoch: 20, Batch: 72, Loss: 0.5048, Elapsed: 0m2s
2023-01-22 22:46:24.888157: Epoch: 20, Batch: 73, Loss: 0.4940, Elapsed: 0m2s
2023-01-22 22:46:28.384968: Epoch: 20, Batch: 74, Loss: 0.5433, Elapsed: 0m3s
2023-01-22 22:46:30.563036: Epoch: 20, Batch: 75, Loss: 0.5059, Elapsed: 0m2s
2023-01-22 22:46:31.967666: Epoch: 20, Batch: 76, Loss: 0.4450, Elapsed: 0m1s
2023-01-22 22:46:33.795763: Epoch: 20, Batch: 77, Loss: 0.5003, Elapsed: 0m1s
2023-01-22 22:46:35.725512: Epoch: 20, Batch: 78, Loss: 0.4985, Elapsed: 0m1s
2023-01-22 22:46:38.522550: Epoch: 20, Batch: 79, Loss: 0.5489, Elapsed: 0m2s
2023-01-22 22:46:39.393796: Epoch: 20, Batch: 80, Loss: 0.3414, Elapsed: 0m0s
2023-01-22 22:46:39.403930: Training completed!
